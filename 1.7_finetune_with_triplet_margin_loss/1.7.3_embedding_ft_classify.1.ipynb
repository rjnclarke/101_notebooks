{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1WcBdNPaxZbov4_-FlF8HvPHgA0yOOFKG","timestamp":1729598546911},{"file_id":"1wov8AsUCwvkyGrNQGI0uZIpqvfIe-JAl","timestamp":1729156025307}],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNUIRWLs8RQ9mszyZl5CN8y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Text Classification - Embedding PAL FT - classify\n","\n","----"],"metadata":{"id":"7bm-QDtacSiP"}},{"cell_type":"markdown","source":["## $\\color{blue}{Sections:}$\n","* Preamble\n","* Admin - importing libraries\n","* Load - Loading our data from pandas\n","* Dataset - Create PyTorch Dataset\n","* Model - Create PyTorch Vanilla model\n","* Helper - Training helper functions\n","* Training - Training Loop\n"],"metadata":{"id":"NuCuZ_BudQGl"}},{"cell_type":"markdown","source":["## $\\color{blue}{Preamble:}$\n","\n","This note book will create and train a classification model based on the Embedding FT (hard batch triplet loss) embeddings."],"metadata":{"id":"xe8wi1XGdsYg"}},{"cell_type":"markdown","source":["## $\\color{blue}{Admin:}$"],"metadata":{"id":"i-a1PCObeWis"}},{"cell_type":"code","source":["from google.colab import drive"],"metadata":{"id":"J2nVP8EIeEQr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["drive.mount(\"/content/drive\")\n","%cd '/content/drive/MyDrive/'\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_q0vrFD0eZve","executionInfo":{"status":"ok","timestamp":1729606020076,"user_tz":-120,"elapsed":20764,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"bf589716-829f-4036-801d-65a7af37c546"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive\n"]}]},{"cell_type":"code","source":["%%capture\n","!pip install torch\n","!pip install dill"],"metadata":{"id":"ksCw2OLTeero"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G1LB8g7VfJ-Z","executionInfo":{"status":"ok","timestamp":1729606028767,"user_tz":-120,"elapsed":3165,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"0f8f120a-7bfe-4099-d5bb-d6f75ed3224d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"markdown","source":["## $\\color{blue}{Load:}$"],"metadata":{"id":"3ADkVfjYemwf"}},{"cell_type":"code","source":["import pandas as pd\n","path = \"class/datasets/\"\n","df_train = pd.read_pickle(path + \"df_train\")\n","df_dev = pd.read_pickle(path + \"df_dev\")\n","df_test = pd.read_pickle(path + \"df_test\")"],"metadata":{"id":"zqQLJhNvelrB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"id":"C77mixZverHU","executionInfo":{"status":"ok","timestamp":1729605483416,"user_tz":-120,"elapsed":6232,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"06f0c522-683e-4885-b6ad-ee1427c1d1fe"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   index     master  book_idx       book  chapter_idx  \\\n","0   8114  Dubliners         3  Dubliners           31   \n","1   4951    Ulysses         2     Nostos           15   \n","2   4629    Ulysses         2     Nostos           15   \n","3  11556    Dracula         4    Dracula           59   \n","4  12262   Republic         5   Republic           62   \n","\n","                                chapter       author  \\\n","0                                 GRACE        Joyce   \n","1                               Eumaeus        Joyce   \n","2                               Eumaeus        Joyce   \n","3  CHAPTER XXVII: MINA HARKER’S JOURNAL  Bram Stoker   \n","4                              Book III        Plato   \n","\n","                                             content  \\\n","0  “Is it John of Tuam?”   “Are you sure of that ...   \n","1  sibly there were several others. He personally...   \n","2  Stephen, who was trying his dead best to yawn ...   \n","3  Now to the historical, for as Madam Mina write...   \n","4  The harmonies which you mean are the mixed or ...   \n","\n","                                   vanilla_embedding  \\\n","0  [-0.012913608, -0.026916211, 0.0023321153, -0....   \n","1  [-0.019626686, -0.035692617, -0.034875672, 0.0...   \n","2  [0.015934143, -0.0034991587, 0.0035751674, 0.0...   \n","3  [-4.009433e-05, -0.0041142944, 0.026873538, -0...   \n","4  [0.0048890463, -0.0060007297, 0.0054147574, -0...   \n","\n","                                 vanilla_embedding.1  \\\n","0  [tensor(-0.0129), tensor(-0.0269), tensor(0.00...   \n","1  [tensor(-0.0196), tensor(-0.0357), tensor(-0.0...   \n","2  [tensor(0.0159), tensor(-0.0035), tensor(0.003...   \n","3  [tensor(-4.0125e-05), tensor(-0.0041), tensor(...   \n","4  [tensor(0.0049), tensor(-0.0060), tensor(0.005...   \n","\n","                                        ft_embedding  \n","0  [-0.03558476, -0.032069266, 0.016694317, -0.01...  \n","1  [-0.020282326, 0.019139778, -0.013562409, 0.03...  \n","2  [-0.018985722, 0.021503495, -0.012215637, 0.03...  \n","3  [0.005192333, -0.0079266345, 0.0034984224, -0....  \n","4  [0.06559832, 0.06647674, -0.011808932, -0.0327...  "],"text/html":["\n","  <div id=\"df-94e5f24c-05e1-46c5-ae9a-1079e2ae210f\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>master</th>\n","      <th>book_idx</th>\n","      <th>book</th>\n","      <th>chapter_idx</th>\n","      <th>chapter</th>\n","      <th>author</th>\n","      <th>content</th>\n","      <th>vanilla_embedding</th>\n","      <th>vanilla_embedding.1</th>\n","      <th>ft_embedding</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>8114</td>\n","      <td>Dubliners</td>\n","      <td>3</td>\n","      <td>Dubliners</td>\n","      <td>31</td>\n","      <td>GRACE</td>\n","      <td>Joyce</td>\n","      <td>“Is it John of Tuam?”   “Are you sure of that ...</td>\n","      <td>[-0.012913608, -0.026916211, 0.0023321153, -0....</td>\n","      <td>[tensor(-0.0129), tensor(-0.0269), tensor(0.00...</td>\n","      <td>[-0.03558476, -0.032069266, 0.016694317, -0.01...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4951</td>\n","      <td>Ulysses</td>\n","      <td>2</td>\n","      <td>Nostos</td>\n","      <td>15</td>\n","      <td>Eumaeus</td>\n","      <td>Joyce</td>\n","      <td>sibly there were several others. He personally...</td>\n","      <td>[-0.019626686, -0.035692617, -0.034875672, 0.0...</td>\n","      <td>[tensor(-0.0196), tensor(-0.0357), tensor(-0.0...</td>\n","      <td>[-0.020282326, 0.019139778, -0.013562409, 0.03...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4629</td>\n","      <td>Ulysses</td>\n","      <td>2</td>\n","      <td>Nostos</td>\n","      <td>15</td>\n","      <td>Eumaeus</td>\n","      <td>Joyce</td>\n","      <td>Stephen, who was trying his dead best to yawn ...</td>\n","      <td>[0.015934143, -0.0034991587, 0.0035751674, 0.0...</td>\n","      <td>[tensor(0.0159), tensor(-0.0035), tensor(0.003...</td>\n","      <td>[-0.018985722, 0.021503495, -0.012215637, 0.03...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>11556</td>\n","      <td>Dracula</td>\n","      <td>4</td>\n","      <td>Dracula</td>\n","      <td>59</td>\n","      <td>CHAPTER XXVII: MINA HARKER’S JOURNAL</td>\n","      <td>Bram Stoker</td>\n","      <td>Now to the historical, for as Madam Mina write...</td>\n","      <td>[-4.009433e-05, -0.0041142944, 0.026873538, -0...</td>\n","      <td>[tensor(-4.0125e-05), tensor(-0.0041), tensor(...</td>\n","      <td>[0.005192333, -0.0079266345, 0.0034984224, -0....</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>12262</td>\n","      <td>Republic</td>\n","      <td>5</td>\n","      <td>Republic</td>\n","      <td>62</td>\n","      <td>Book III</td>\n","      <td>Plato</td>\n","      <td>The harmonies which you mean are the mixed or ...</td>\n","      <td>[0.0048890463, -0.0060007297, 0.0054147574, -0...</td>\n","      <td>[tensor(0.0049), tensor(-0.0060), tensor(0.005...</td>\n","      <td>[0.06559832, 0.06647674, -0.011808932, -0.0327...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-94e5f24c-05e1-46c5-ae9a-1079e2ae210f')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-94e5f24c-05e1-46c5-ae9a-1079e2ae210f button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-94e5f24c-05e1-46c5-ae9a-1079e2ae210f');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-ad5db40f-f896-4590-b5ed-ddc7a3a169e5\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ad5db40f-f896-4590-b5ed-ddc7a3a169e5')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-ad5db40f-f896-4590-b5ed-ddc7a3a169e5 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_train","summary":"{\n  \"name\": \"df_train\",\n  \"rows\": 12000,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4034,\n        \"min\": 1,\n        \"max\": 13963,\n        \"num_unique_values\": 12000,\n        \"samples\": [\n          12943,\n          6855,\n          3959\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"master\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Ulysses\",\n          \"Republic\",\n          \"Dubliners\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"book_idx\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          3,\n          2,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"book\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Dubliners\",\n          \"Nostos\",\n          \"Telemachia\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chapter_idx\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 21,\n        \"min\": 0,\n        \"max\": 69,\n        \"num_unique_values\": 70,\n        \"samples\": [\n          11,\n          31,\n          68\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chapter\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 70,\n        \"samples\": [\n          \"Cyclops\",\n          \"GRACE\",\n          \"Book IX\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"author\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Joyce\",\n          \"Bram Stoker\",\n          \"Plato\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12000,\n        \"samples\": [\n          \"The sailors are quarrelling with one another about the steering\\u2014every one is of opinion that he has a right to steer, though he has never learned the art of navigation and cannot tell who taught him or when he learned, and will further assert that it cannot be taught, and they are ready to cut in pieces any one who says the contrary. They throng about the captain,\",\n          \"I often felt I wanted to kiss him all over also his lovely young cock there so\",\n          \"taim and Netaim begat Le Hirsch and Le Hirsch begat Jesurum and Jesurum begat Ma\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vanilla_embedding\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vanilla_embedding.1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12000,\n        \"samples\": [\n          \"tensor([-2.9863e-02, -2.4137e-02, -1.2980e-02,  7.9496e-03,  2.5395e-02,\\n         2.8110e-03,  4.6031e-02,  3.0127e-02, -1.3664e-02, -3.8568e-02,\\n        -3.8985e-02,  4.4425e-02, -5.2543e-02,  3.3829e-02,  1.2498e-02,\\n         8.8686e-02,  4.2462e-02,  2.3477e-02, -5.8387e-03, -5.1662e-03,\\n         1.9426e-02, -1.2606e-02,  9.2724e-03,  9.0744e-03,  4.6863e-02,\\n         9.1102e-03,  3.5423e-02,  5.5087e-03, -7.8943e-02,  5.4191e-03,\\n         4.4789e-02, -2.7046e-02, -2.3067e-02, -5.9169e-03, -3.7753e-02,\\n        -2.0195e-02, -1.8248e-02, -1.1951e-02,  2.4935e-02,  2.5796e-02,\\n        -3.9357e-02, -1.6716e-02, -5.8667e-05,  3.6154e-03, -3.8097e-02,\\n         1.2965e-02,  5.1190e-04,  1.9863e-02, -1.6463e-02, -5.0270e-02,\\n        -7.2113e-02,  3.5859e-02,  3.2563e-02,  2.7452e-02, -2.7480e-02,\\n         2.8004e-02, -1.6719e-02, -4.8378e-02, -8.2380e-03, -7.8102e-02,\\n         3.8732e-02,  4.3615e-02,  4.5682e-02, -3.5951e-03,  1.1467e-02,\\n         2.6137e-03,  3.3201e-03,  3.1726e-02, -6.0433e-02,  1.1828e-02,\\n        -2.2652e-02,  8.6694e-04,  1.8449e-02, -9.4057e-03,  1.9183e-02,\\n        -7.8142e-02,  2.6165e-02,  6.4350e-02,  2.4949e-02, -1.6273e-03,\\n         1.9832e-03, -1.2290e-02,  8.0915e-03,  7.4053e-02, -4.0986e-02,\\n        -4.9777e-02,  3.8292e-02,  2.9189e-03, -5.6897e-02,  3.6016e-02,\\n        -1.9681e-02, -4.6525e-02,  2.3347e-02,  3.5060e-02,  1.0205e-02,\\n         1.0588e-03,  6.9914e-02,  7.4516e-03,  2.0420e-02, -1.1732e-02,\\n        -8.3520e-03, -1.2817e-02,  3.5654e-02,  9.0776e-02, -7.1468e-02,\\n         8.6498e-03,  3.4361e-02,  1.3240e-02,  1.1418e-02,  1.2266e-02,\\n         1.5669e-02, -2.1138e-02, -1.7356e-02, -3.1643e-02, -5.5692e-02,\\n         5.6686e-02,  2.4702e-02, -1.9544e-02,  9.1201e-03, -3.1121e-02,\\n         3.8697e-02,  2.4540e-02,  4.9687e-03,  9.0912e-02, -1.5298e-03,\\n         3.9096e-02, -2.0340e-02,  7.0087e-02, -4.6208e-02, -8.4320e-02,\\n         1.8379e-02,  6.1340e-02, -5.4138e-02, -2.1309e-03, -3.9832e-02,\\n        -9.3246e-04, -2.2657e-04, -3.4827e-03, -1.3427e-02,  2.6477e-02,\\n        -3.0276e-02, -1.0194e-05,  1.8843e-02,  3.9524e-02,  3.5049e-02,\\n        -1.1611e-02,  7.0074e-03, -3.7855e-02, -2.5696e-02,  5.5981e-03,\\n         4.7483e-04, -2.0677e-02,  4.0041e-02, -3.3193e-02,  1.0369e-02,\\n         3.4938e-02,  8.2048e-03,  2.3415e-02, -5.7058e-02,  4.8873e-02,\\n         7.6957e-04, -2.5612e-03, -1.2115e-02, -1.4015e-02,  2.8809e-03,\\n        -7.3030e-03,  1.7589e-02,  1.3587e-02, -1.0081e-02,  1.7708e-02,\\n        -4.7791e-02, -1.9017e-03,  3.0289e-02, -1.6894e-02, -6.8109e-04,\\n         1.5728e-02,  7.7968e-02,  1.4674e-02,  4.2640e-02, -1.8292e-02,\\n        -1.0071e-01,  5.0221e-02,  5.2787e-03,  8.5143e-03,  4.3630e-03,\\n        -2.0364e-02,  2.5825e-02,  3.6757e-04,  1.1480e-02,  1.7190e-02,\\n        -5.4930e-02, -7.6031e-02,  9.1148e-03, -4.1332e-03,  6.0615e-02,\\n        -2.8586e-02, -2.6375e-02,  1.9739e-02,  1.7202e-02,  4.6524e-02,\\n        -3.7173e-02,  3.2619e-03,  7.2795e-02, -7.8540e-03, -4.5241e-02,\\n         7.8022e-02,  1.5629e-02, -8.5344e-03,  8.7421e-03,  3.7908e-02,\\n        -4.3589e-02, -2.9328e-02,  5.0121e-02,  7.2883e-03,  3.7568e-02,\\n        -6.0964e-02,  3.4033e-02, -6.0409e-02,  3.2882e-02, -2.3717e-02,\\n        -9.2783e-03,  1.3447e-02, -4.1766e-03,  3.4737e-02, -2.5385e-02,\\n         8.5897e-02,  8.7884e-02, -2.1897e-02, -3.8556e-02,  1.4516e-02,\\n        -5.5989e-03, -6.8120e-03,  6.9896e-03,  3.3590e-02,  1.2032e-03,\\n        -6.9860e-03,  2.2438e-02, -5.2962e-02,  4.9846e-02, -6.8445e-02,\\n        -5.8273e-03,  4.2903e-02,  2.3768e-02,  9.2723e-02,  3.3254e-02,\\n        -8.8730e-03,  2.7061e-02,  9.3088e-03, -2.3453e-03, -4.0875e-02,\\n         1.2700e-03, -3.7555e-02, -9.8984e-04, -3.7862e-02, -2.1162e-02,\\n        -1.0035e-01, -1.3747e-03, -1.1529e-02,  4.9897e-02,  3.5029e-02,\\n        -3.2510e-02,  3.5025e-02, -7.9630e-03, -1.2299e-02, -6.4601e-03,\\n        -8.6004e-02, -4.2667e-02,  2.1272e-02,  3.6887e-02,  1.8293e-02,\\n         3.8268e-02, -1.9020e-02,  2.3012e-02,  1.3083e-03, -2.8730e-02,\\n         1.4797e-02,  6.2837e-02, -1.8705e-02,  2.2138e-02, -2.6815e-02,\\n         1.1130e-02,  3.8768e-02, -2.2040e-02, -1.8263e-02,  2.9571e-02,\\n        -4.3131e-02,  2.1282e-02, -3.2188e-02, -4.3401e-02,  2.8807e-02,\\n         1.6530e-02,  4.3993e-02,  2.5416e-02,  2.2385e-02,  2.1376e-02,\\n         2.6256e-02,  1.4608e-03, -4.5930e-04,  1.5279e-02, -2.4773e-02,\\n         1.1253e-02,  4.0599e-02, -1.8160e-02, -1.4670e-02,  3.4483e-03,\\n         1.1305e-02,  4.9661e-03, -4.8446e-02, -2.1036e-01,  1.1912e-02,\\n         3.0943e-03, -2.7721e-02,  5.6620e-02, -6.3865e-03,  2.8574e-02,\\n        -5.3512e-02, -3.0129e-02,  2.7782e-02, -3.9856e-02,  2.8293e-02,\\n         3.1980e-02,  3.3337e-02,  5.2160e-02,  3.3916e-03, -1.2961e-02,\\n        -6.2043e-02,  1.5318e-02,  3.4520e-02, -1.5917e-02, -5.9162e-02,\\n        -2.6944e-02, -3.6265e-03,  1.5588e-02,  6.4518e-02, -2.7193e-02,\\n         7.1757e-05, -6.0703e-02, -2.8794e-02, -1.8277e-02, -1.2587e-02,\\n        -8.7987e-03, -3.3640e-02,  1.9510e-02, -6.0114e-02, -5.6944e-03,\\n        -2.1954e-02,  2.4841e-03, -3.9797e-02, -3.6614e-02, -6.0613e-02,\\n         1.0020e-02,  2.5751e-03,  5.9838e-02,  8.2750e-04, -1.9470e-02,\\n        -1.8626e-02,  1.1195e-02,  9.2473e-02, -1.4703e-03,  2.6109e-04,\\n         1.4313e-02,  1.3531e-02, -2.6636e-03,  1.3452e-03, -1.5079e-02,\\n         9.3440e-03, -6.9179e-02, -2.2251e-02,  1.9865e-02, -2.6030e-02,\\n        -5.4381e-02, -1.5965e-02, -1.4618e-02, -8.5487e-02, -2.4812e-03,\\n        -5.2660e-02,  5.6110e-02,  6.3610e-03, -1.5091e-02,  3.3730e-02,\\n        -3.9443e-02, -7.9906e-02,  9.6259e-03, -1.3070e-02, -1.4533e-02,\\n        -3.7235e-02,  1.8898e-02, -3.3195e-03, -6.2588e-02, -2.0315e-02,\\n         3.4936e-02,  6.0773e-03,  6.4859e-03, -3.4437e-02,  2.9843e-02,\\n        -1.6467e-02, -3.7340e-02, -2.2042e-02,  7.0662e-02,  2.7994e-02,\\n        -4.5311e-02, -2.1035e-02,  3.4413e-02,  7.5105e-03, -4.8450e-03,\\n        -4.7338e-05,  5.2897e-02, -9.0897e-03,  5.0395e-02, -7.6494e-02,\\n         2.4720e-02,  1.0721e-04, -2.4337e-03, -1.3737e-02, -4.2781e-02,\\n         3.7383e-02,  5.8610e-02,  7.3654e-03,  5.1579e-02, -7.6279e-03,\\n         4.9115e-02, -3.2968e-02, -1.4404e-02, -4.3081e-02,  3.5053e-02,\\n         6.2854e-02,  1.5349e-02,  3.1458e-02, -3.2882e-02,  4.1430e-02,\\n        -7.4620e-02, -2.6739e-02, -1.1351e-01,  2.3275e-02, -1.2260e-02,\\n        -2.4687e-02,  1.5752e-02,  2.9133e-03, -1.1193e-02, -3.1350e-03,\\n         4.4944e-02, -2.0699e-02,  2.2323e-02, -1.7723e-02, -5.3132e-02,\\n        -5.3242e-02,  6.0222e-03,  2.1197e-02,  1.4778e-02, -3.7157e-03,\\n        -2.9436e-03,  1.4513e-03,  3.4811e-02,  2.9415e-02, -3.6716e-02,\\n         2.2279e-02, -1.6084e-02,  2.7062e-02, -7.7919e-03,  7.1249e-03,\\n         3.7000e-02, -4.0412e-02,  1.5067e-02, -9.1175e-03,  3.4188e-03,\\n         2.1501e-02, -3.5556e-02, -2.1015e-02, -2.1198e-02, -3.5271e-02,\\n        -1.2982e-03, -1.3341e-02, -7.7036e-03,  7.2603e-02, -2.5343e-02,\\n        -1.6240e-02, -8.1827e-03,  1.2754e-02,  5.1120e-03, -3.5620e-02,\\n        -6.0731e-02,  2.5077e-02,  4.7340e-03,  2.9113e-02, -2.8349e-02,\\n        -1.3176e-02,  2.1623e-02, -1.3296e-02, -3.1832e-02,  1.6613e-02,\\n        -1.6553e-02, -1.0819e-02,  4.9536e-02, -3.9288e-02,  1.6248e-02,\\n        -7.6107e-02, -1.6216e-02,  6.2076e-05, -7.2438e-03,  3.0174e-02,\\n        -2.0034e-02,  1.1899e-02, -9.1987e-02, -5.6462e-02,  2.8377e-02,\\n         2.3043e-02,  2.5240e-02,  1.3916e-02,  1.7255e-02,  1.5307e-02,\\n        -1.3008e-02,  2.5145e-02,  1.0182e-02, -9.2633e-02,  4.8648e-02,\\n        -1.3750e-03, -3.0672e-02,  5.1711e-02, -7.3347e-02, -5.9863e-02,\\n        -2.9591e-02,  1.9455e-02,  2.7858e-02, -5.8052e-02, -4.3833e-03,\\n        -2.7406e-02,  1.9505e-02, -6.8249e-03, -1.6321e-03, -7.7714e-03,\\n        -5.4678e-02, -1.5759e-02, -4.6177e-02,  6.4159e-02, -3.4469e-02,\\n         9.9966e-03,  4.3263e-02, -2.1470e-02,  2.0343e-02, -3.6220e-02,\\n        -8.1282e-03,  5.9774e-02,  3.0713e-03,  7.4538e-04, -2.7106e-02,\\n        -1.8725e-02, -1.7035e-02,  5.6524e-02,  2.8417e-02,  1.8979e-02,\\n        -2.8612e-02, -2.3726e-02,  1.3383e-02,  2.8749e-02, -5.5099e-03,\\n        -1.0956e-02,  2.8481e-02,  3.2569e-02,  8.4480e-03, -2.7417e-02,\\n         1.8752e-04,  1.7030e-02,  4.0000e-02, -2.4175e-03, -3.0245e-02,\\n         1.4326e-02, -3.6192e-02,  2.8366e-02, -1.3916e-02, -3.8068e-03,\\n        -2.7187e-02, -1.8972e-02,  1.8578e-02,  1.6353e-02, -1.3282e-02,\\n        -5.7613e-03,  2.8506e-02, -1.2732e-03, -1.8450e-02, -9.3634e-02,\\n         3.0902e-02, -2.2701e-02,  8.8463e-03,  2.2184e-02, -1.6772e-02,\\n        -1.1447e-03,  3.0164e-02, -7.8465e-02, -4.8690e-02,  3.3422e-02,\\n        -4.9452e-02, -1.9951e-02,  1.2271e-02, -5.4492e-02,  2.6475e-02,\\n        -1.5425e-03, -6.5840e-02,  2.3555e-03,  1.1357e-02,  3.3587e-02,\\n         1.4985e-02, -4.5467e-03, -1.3470e-02, -2.9423e-02,  4.5198e-02,\\n         5.0525e-02, -3.2857e-03,  4.2876e-02, -8.7302e-02,  5.6024e-02,\\n         1.7709e-02, -1.5523e-02, -1.7379e-02,  2.9512e-02, -7.2198e-03,\\n        -5.9601e-02,  2.7620e-02,  3.3195e-02, -4.1064e-04, -7.2280e-02,\\n         4.8732e-02,  1.8132e-02, -4.9474e-02, -4.9900e-02,  2.2088e-02,\\n        -5.4815e-02, -1.5091e-02,  1.5560e-02,  1.9587e-02, -1.8394e-02,\\n         3.5408e-02,  1.7671e-02,  6.9367e-02,  4.4992e-02, -9.6492e-03,\\n        -1.1189e-02,  2.8355e-02,  9.6589e-02,  2.4372e-02,  3.3785e-02,\\n        -5.4007e-03,  7.2608e-02, -3.0947e-02, -4.7963e-02,  4.7805e-02,\\n         2.2063e-03,  2.0730e-02, -1.1726e-02,  3.7867e-03,  6.0092e-03,\\n         1.3531e-02,  4.6455e-02,  3.4423e-03, -6.0307e-03,  4.1172e-02,\\n         3.5473e-03,  2.8060e-02,  4.3816e-02,  2.6637e-02,  1.3840e-02,\\n         3.0573e-02, -8.3632e-02,  2.6332e-02,  3.9101e-03, -1.2674e-02,\\n         2.5557e-02, -3.0451e-02,  1.1706e-02, -8.4929e-03, -1.8894e-02,\\n         8.0283e-02,  1.2338e-02, -3.1975e-02,  4.1047e-03,  7.9437e-02,\\n        -3.2811e-04, -9.7614e-03,  3.0858e-02,  1.2160e-02,  2.3968e-02,\\n        -4.5059e-02, -1.4037e-02, -2.5902e-02,  1.4349e-02,  1.9149e-02,\\n         6.5323e-03,  1.1563e-02,  2.3068e-02,  2.6450e-02,  2.6589e-03,\\n        -4.1191e-02, -5.1647e-02, -4.1744e-02, -4.1313e-02, -1.5552e-02,\\n         1.6150e-02, -3.2451e-02, -2.8515e-02, -1.0222e-02, -3.9071e-02,\\n        -3.5194e-03,  1.7907e-02, -4.6404e-02, -4.9805e-02,  6.0830e-02,\\n         6.5889e-02,  2.8697e-02,  2.2425e-02,  5.2712e-02,  1.4629e-02,\\n        -3.8016e-02, -8.9093e-03,  5.5438e-03,  2.9319e-02, -1.0425e-02,\\n        -1.1059e-02, -1.1260e-01,  3.5242e-02,  1.2681e-02,  1.8486e-02,\\n        -9.3693e-02,  3.4758e-02, -4.8421e-02, -9.1410e-03,  7.3279e-02,\\n        -4.2658e-03,  7.0927e-03, -2.7025e-02, -4.6200e-02,  7.7008e-05,\\n        -3.8849e-02,  4.2036e-02, -7.7849e-02,  4.7517e-02, -1.3141e-02,\\n         1.4527e-02, -3.8119e-02,  4.5054e-02,  9.8307e-03, -8.1675e-03,\\n        -1.9488e-02, -1.4658e-02,  3.0034e-02, -1.2060e-01, -3.9692e-03,\\n        -1.5489e-02, -2.1422e-02, -5.0006e-02,  4.2380e-02, -1.0642e-02,\\n         2.4454e-02,  3.5327e-02,  9.7117e-03, -3.1390e-02, -4.3970e-02,\\n        -1.9657e-02, -3.1673e-03,  4.2955e-02,  3.1809e-02, -3.6673e-02,\\n        -9.3784e-03, -6.4675e-02,  1.1618e-03, -3.8935e-02,  8.9543e-04,\\n         1.2267e-02,  1.1006e-02,  1.8267e-03])\",\n          \"tensor([-3.6975e-02, -1.2474e-02,  7.1600e-04, -5.8543e-03,  2.4121e-02,\\n         1.5946e-02,  1.0461e-02,  2.6744e-02,  7.9212e-03, -1.6705e-02,\\n        -1.7883e-02, -2.0982e-02, -4.9010e-02,  1.8686e-02,  8.2015e-03,\\n         5.1007e-02,  3.9796e-02, -9.7566e-03,  1.7129e-02, -3.9193e-02,\\n         4.6385e-02,  7.0796e-03, -2.9421e-02,  4.4852e-02,  2.1693e-02,\\n        -2.3425e-02,  3.1312e-03, -3.5929e-02, -5.8223e-02, -3.6488e-02,\\n         2.7952e-02, -3.0520e-03, -1.5233e-02, -8.6148e-03, -3.1127e-02,\\n        -3.8032e-02,  7.4864e-03,  1.4978e-03, -3.6322e-02, -8.0330e-03,\\n        -3.0200e-02, -1.9461e-02,  1.1065e-02, -6.7383e-03, -9.3200e-03,\\n         7.1790e-03, -3.0926e-02,  5.9635e-02, -7.8852e-03, -3.5299e-02,\\n        -1.9356e-02,  4.9018e-02,  4.2036e-02, -1.3805e-02,  9.4507e-03,\\n         4.2177e-02, -2.9684e-02, -2.6431e-02, -7.3244e-03, -4.0327e-02,\\n         4.1203e-03, -1.6512e-03,  4.3058e-02,  1.0896e-02,  2.0419e-02,\\n         3.7431e-02, -1.3895e-02,  1.4659e-02, -6.6858e-02, -3.6731e-03,\\n        -3.0607e-02, -1.6788e-02,  9.5376e-03, -2.3516e-02,  7.0431e-03,\\n        -4.5368e-02,  5.9452e-03,  4.4239e-02, -1.7874e-02,  3.7110e-02,\\n        -3.5288e-02, -3.3972e-02, -2.4562e-02,  8.2143e-02,  6.1411e-03,\\n        -3.1255e-02,  9.8866e-03,  9.6595e-04, -3.9484e-02,  6.0871e-02,\\n        -4.4760e-02, -5.4522e-02,  4.1723e-02,  4.5581e-02,  1.0167e-02,\\n        -2.9095e-02,  4.1335e-02,  2.4116e-02, -5.1611e-03,  1.6459e-02,\\n        -1.9593e-02, -7.5063e-02, -1.0571e-02, -1.8325e-02, -1.0046e-01,\\n        -2.1994e-02, -1.0873e-02,  3.2881e-02,  3.6305e-02,  2.6543e-03,\\n        -2.0287e-02, -2.5426e-02, -1.7613e-02,  2.7796e-02, -4.4127e-02,\\n         8.1198e-02,  6.0275e-02, -3.9643e-03, -2.0760e-02, -3.8750e-02,\\n         5.1018e-02,  2.7232e-02,  4.9732e-02,  1.0047e-01,  2.2035e-02,\\n        -9.5026e-03,  1.6259e-03,  4.9989e-02,  1.0071e-03, -3.8901e-02,\\n         9.8802e-03,  9.4287e-02, -2.0707e-02, -1.5489e-02, -2.3383e-03,\\n        -1.0661e-02,  1.2098e-03, -1.9038e-02,  1.0468e-02, -2.2225e-02,\\n         1.2400e-02, -1.4488e-02,  1.6029e-02,  1.5534e-02,  2.6434e-02,\\n         2.2012e-02, -9.3774e-03, -2.2514e-02, -3.0558e-03,  4.0735e-02,\\n        -5.0450e-03,  3.4135e-03,  9.5908e-03, -5.3152e-02,  5.0901e-03,\\n         2.1864e-02, -4.0145e-02,  1.0698e-02, -6.6836e-03,  4.1285e-03,\\n         4.9150e-02, -1.7819e-03,  1.1567e-02, -9.9544e-04, -1.3923e-02,\\n        -1.2141e-03,  2.6684e-02, -6.9542e-02, -4.5050e-02,  4.8382e-02,\\n        -4.0704e-02,  2.5684e-02,  6.1361e-03, -8.7284e-03, -2.4146e-02,\\n         2.1287e-02,  9.3679e-02, -1.4011e-02, -1.5335e-03,  1.4765e-02,\\n        -8.1970e-02,  5.9190e-02,  8.6337e-03,  1.5566e-03, -1.6239e-03,\\n        -1.0325e-02,  5.5380e-02,  7.2568e-04,  1.8988e-02, -1.7337e-04,\\n        -8.0225e-02, -5.2026e-02, -1.6813e-02, -1.2201e-02,  3.5137e-02,\\n        -1.8481e-02, -1.0908e-02,  1.7704e-03,  2.9816e-02,  7.3233e-02,\\n        -3.7737e-02, -1.3995e-02,  2.7690e-02, -1.0828e-02, -6.8249e-02,\\n         1.2834e-02, -2.1746e-02,  3.2801e-03,  4.6605e-02,  3.8649e-02,\\n        -1.8488e-02,  2.6114e-02,  2.5211e-02, -4.2127e-02, -6.7848e-03,\\n        -1.0229e-02,  2.9025e-02, -2.3018e-02, -2.4768e-02, -4.3603e-02,\\n         4.8615e-02,  9.3231e-04, -2.8908e-02,  3.3895e-02,  1.7012e-02,\\n         5.7139e-02,  4.1373e-02, -7.6733e-02, -2.9972e-02,  9.5508e-03,\\n        -1.9220e-02, -1.9388e-02, -6.2584e-03,  1.8817e-02, -2.7700e-02,\\n         1.5385e-02,  3.5680e-02, -4.1785e-02,  3.4870e-02, -6.9296e-02,\\n        -4.3914e-03,  5.8352e-02,  2.6693e-02,  8.5666e-02, -3.3159e-02,\\n        -1.4889e-02,  2.6024e-02,  8.9438e-03, -4.0097e-02, -1.0101e-02,\\n        -2.4295e-02, -2.4425e-02, -2.9340e-02, -1.0662e-02, -4.6754e-03,\\n        -5.1674e-02,  6.7521e-03, -5.6098e-03,  3.8191e-02,  3.5094e-02,\\n         7.4597e-04, -2.5928e-02,  2.2237e-02, -1.7717e-02, -4.1082e-02,\\n        -5.6164e-02, -3.3580e-02, -1.1696e-02,  3.8102e-02, -4.1526e-03,\\n         3.4897e-02, -1.5218e-03,  2.4356e-02,  1.7932e-02,  3.2049e-03,\\n        -2.5297e-02,  4.6150e-02,  5.1635e-03,  3.0211e-02, -5.8007e-02,\\n         5.3795e-03,  7.8508e-02, -6.3470e-02, -5.0183e-02,  1.8526e-02,\\n        -5.5766e-02, -5.1868e-03, -9.2652e-03, -2.9823e-02, -3.4437e-03,\\n         3.8153e-02,  1.5684e-02,  1.1694e-02,  2.3175e-02,  5.7315e-03,\\n        -1.7816e-02,  1.0525e-02,  3.9852e-02, -1.1742e-02,  1.4325e-02,\\n         2.1771e-02,  3.6523e-02,  8.3707e-03, -1.1444e-02,  3.1449e-02,\\n        -6.9583e-03, -1.6624e-02, -4.5402e-02, -2.1878e-01,  5.2621e-03,\\n         1.6601e-02, -4.7282e-02,  2.7799e-02, -8.8790e-03,  1.7955e-02,\\n        -5.0596e-02, -3.7801e-02,  7.8983e-03, -2.4407e-02, -1.3047e-02,\\n         1.6384e-02,  3.6106e-02,  5.9348e-02,  1.1632e-02, -1.0144e-02,\\n        -5.7422e-02, -2.0587e-02,  5.2429e-02, -3.6303e-02, -5.6161e-02,\\n        -3.4542e-02, -6.1846e-03,  4.7945e-02,  6.4481e-02, -4.8659e-02,\\n         5.9455e-03, -5.8154e-02,  2.8625e-03, -4.1419e-02, -5.7732e-03,\\n         2.4122e-02,  3.0131e-02,  1.0596e-02,  9.2955e-03,  3.8426e-02,\\n        -5.7438e-02, -1.2456e-02,  2.6668e-03,  3.2530e-03, -5.3020e-02,\\n        -5.3779e-02, -4.6189e-03,  5.0089e-02, -4.1726e-02, -9.0277e-04,\\n        -2.2841e-02,  8.1492e-03,  5.8179e-02,  1.1285e-02,  1.3706e-02,\\n        -2.0564e-02, -1.6029e-02,  1.3535e-02, -7.3357e-03,  2.6879e-02,\\n         3.7505e-02, -6.5533e-02, -1.7058e-02, -1.9880e-02, -2.6784e-02,\\n        -4.7417e-02, -2.8413e-02,  1.8397e-02, -3.7614e-02, -2.6976e-04,\\n        -5.2264e-02,  4.3938e-02,  4.3596e-02, -2.7408e-03, -2.3940e-02,\\n         8.9851e-03, -6.8356e-02, -1.6124e-02, -1.8294e-02,  5.7246e-03,\\n        -3.2989e-02, -6.1763e-03,  6.1923e-03, -4.3653e-02,  2.3594e-02,\\n         1.4162e-02,  5.0259e-02, -2.2685e-02, -1.5038e-02,  5.1049e-03,\\n        -1.7746e-04, -2.8773e-02, -2.0429e-02,  8.2574e-02, -1.3241e-02,\\n        -1.6322e-02, -8.0756e-03,  4.3565e-02, -1.4483e-02, -3.6567e-02,\\n         2.7021e-02,  3.1791e-02, -2.7970e-02,  6.8145e-02, -6.6979e-02,\\n        -5.1436e-03,  3.0751e-04,  2.9940e-02, -8.5462e-03, -7.2687e-02,\\n        -1.1427e-02,  3.4570e-02,  2.6577e-02,  6.1222e-02,  1.1169e-02,\\n        -6.8678e-03, -4.9571e-02,  9.5997e-03, -4.3985e-02,  4.1020e-02,\\n         7.8699e-02,  1.8918e-02,  6.3988e-02, -3.9808e-02,  3.4053e-02,\\n        -6.6625e-02,  1.1416e-02, -8.6642e-02, -1.7917e-02, -1.0644e-02,\\n        -4.3728e-03, -3.8721e-02,  3.6788e-02,  1.0262e-02, -1.9267e-02,\\n         1.0289e-02, -4.8369e-02,  4.1396e-02,  1.4591e-02, -2.6531e-02,\\n        -8.5402e-02,  2.0877e-02, -2.6395e-03, -4.9370e-03,  4.7961e-02,\\n         3.5816e-03,  3.3825e-02,  3.4491e-02,  2.1462e-02, -3.6961e-02,\\n         1.7751e-02, -3.4929e-02,  4.2905e-02,  2.2491e-02, -1.3956e-02,\\n         2.3083e-02, -5.0724e-02, -1.7303e-02, -2.3714e-02,  1.5917e-02,\\n        -3.0142e-02,  7.3651e-03,  1.7899e-03, -1.0685e-02, -7.8583e-02,\\n        -4.8973e-03,  8.3046e-03,  4.7043e-03,  1.0396e-01, -4.3834e-02,\\n        -3.1780e-02, -1.6469e-02,  3.4521e-02,  1.9005e-02, -2.6451e-02,\\n        -4.2554e-02,  9.3652e-03, -1.0020e-02,  4.0113e-02, -4.0792e-02,\\n        -4.2903e-03, -1.0402e-02,  3.0235e-02, -8.2524e-03,  1.7649e-02,\\n         1.2773e-02, -4.6888e-03,  7.6012e-02, -1.8223e-02, -2.6015e-02,\\n        -4.5606e-02, -2.0520e-02,  5.3154e-02, -2.0563e-02,  3.4930e-02,\\n        -1.8294e-02,  3.9598e-02, -7.5466e-02, -8.3112e-02,  2.5211e-02,\\n        -5.2399e-03,  1.9151e-02,  3.1049e-03,  8.1267e-04, -5.4891e-03,\\n        -4.2015e-02,  1.6882e-02,  3.4076e-02, -1.0801e-01,  4.2181e-02,\\n         3.1410e-02, -1.8017e-02,  1.9811e-02, -2.0970e-02, -4.2829e-02,\\n        -1.8982e-02,  2.5851e-02,  3.1097e-02,  6.8753e-03,  2.5101e-02,\\n         9.4124e-03, -2.0872e-02, -1.9411e-02,  2.3391e-02, -3.9687e-02,\\n        -2.8768e-02,  4.2574e-03, -4.1233e-02,  1.6400e-02, -1.9069e-02,\\n        -3.0599e-02,  3.0764e-02, -3.9375e-02,  4.2435e-02, -5.4061e-02,\\n         3.5287e-02,  7.4228e-02, -1.1964e-02,  4.5781e-03, -7.0186e-03,\\n        -9.8654e-04,  1.4775e-03,  3.5482e-02,  5.6535e-02,  3.1159e-02,\\n        -4.6576e-02, -5.0963e-02, -5.4177e-03,  5.4017e-02,  1.7525e-02,\\n        -1.7890e-03, -8.2797e-03,  4.7723e-02, -7.6887e-04, -7.8831e-03,\\n         4.5719e-03,  5.3017e-03,  4.8033e-02, -1.2806e-04, -3.1091e-02,\\n        -9.7713e-03, -3.0943e-03,  1.7858e-02,  3.0145e-03,  4.9467e-02,\\n        -6.5012e-03,  1.6273e-02,  4.5879e-02,  2.7915e-02,  1.5289e-02,\\n         1.5500e-02,  6.0542e-03, -4.9925e-02, -2.6270e-02, -6.2388e-02,\\n         9.1509e-03, -1.3820e-02,  7.2403e-03, -1.0142e-02, -1.7263e-02,\\n        -1.1962e-02,  6.8927e-02, -1.1379e-01, -5.2304e-02,  1.8867e-02,\\n        -3.2494e-02, -2.6716e-02,  1.4146e-02, -5.5155e-02, -3.5791e-02,\\n        -4.6681e-02, -3.4323e-02,  1.3176e-02,  2.7691e-02,  3.6076e-02,\\n        -2.9883e-02,  2.4997e-02, -3.4112e-03, -1.9075e-02,  4.4763e-02,\\n         3.7715e-02, -2.0862e-02,  3.5406e-02, -6.7835e-02,  4.2638e-02,\\n         4.6656e-02,  1.5641e-02,  1.4062e-02,  1.5066e-02,  2.2695e-02,\\n        -5.8709e-02, -5.0876e-03,  8.2859e-03,  6.9292e-03, -5.4411e-02,\\n         3.0027e-02,  3.4948e-02, -5.6413e-02, -2.8182e-02,  7.3840e-02,\\n        -3.2099e-02, -2.8294e-02,  8.1908e-03,  8.4680e-02, -1.5743e-02,\\n         7.4656e-02,  8.6114e-03,  8.3578e-02,  5.1553e-02, -3.5007e-02,\\n         1.7429e-02,  1.5269e-02,  1.0948e-01,  4.3120e-02,  2.0492e-02,\\n        -1.6203e-02,  6.7195e-02, -1.1305e-02, -6.8512e-02,  4.5932e-02,\\n         2.3161e-02, -4.3385e-02,  3.1024e-02, -1.2651e-03,  3.8285e-02,\\n        -2.0366e-02,  4.6344e-02,  1.0412e-02,  1.7655e-02,  4.8274e-02,\\n        -2.2653e-02,  6.9032e-02,  9.4355e-03,  3.0381e-02,  3.9850e-02,\\n        -8.6390e-03, -5.9398e-02,  1.9885e-02,  2.7962e-02, -1.2831e-02,\\n         4.3659e-03, -4.5820e-02, -8.9758e-03, -2.3773e-02, -1.9217e-02,\\n         8.3715e-02,  4.8752e-04,  2.3020e-02,  3.7350e-02,  1.2812e-02,\\n        -1.1264e-02, -7.4055e-03, -1.3727e-02, -3.5358e-03,  2.5409e-03,\\n        -1.7893e-02, -1.8036e-02,  3.0282e-02,  7.9274e-03,  4.5093e-02,\\n        -6.5101e-02, -4.2129e-05,  2.3618e-02,  3.1066e-02, -7.4930e-03,\\n        -5.1721e-02, -4.0807e-02, -1.1190e-02, -1.3683e-02, -1.7314e-02,\\n         1.7551e-02, -4.6600e-02, -5.5107e-02,  4.8968e-02, -3.5812e-04,\\n         2.5981e-02, -1.2883e-02, -3.9860e-02, -6.9979e-02,  2.6230e-02,\\n         5.0949e-02,  5.7254e-03,  3.4607e-02,  7.3077e-02,  3.4286e-02,\\n        -5.0832e-02,  1.5012e-02, -2.7029e-02,  5.4852e-02, -1.2897e-02,\\n        -2.0997e-02, -9.9074e-02,  4.5091e-02, -7.4347e-03,  2.2673e-02,\\n        -7.9647e-02,  9.8810e-03, -2.4320e-02, -2.8984e-02,  4.1507e-02,\\n         1.9129e-02, -6.7304e-03, -2.1263e-02, -3.0691e-02,  2.9708e-02,\\n         1.3414e-02,  7.1618e-02, -5.6994e-02,  8.8919e-02,  3.7635e-02,\\n         3.2191e-02,  1.0182e-02,  5.8092e-02,  1.2257e-02, -3.0011e-02,\\n        -1.2816e-02, -2.8422e-02, -1.8172e-03, -7.5920e-02, -1.2733e-02,\\n         5.8530e-03,  1.9382e-03, -6.5604e-02,  5.0169e-03, -4.1583e-02,\\n        -1.7360e-02,  7.7576e-03, -2.4925e-02, -7.2088e-03, -4.0670e-02,\\n        -3.9465e-02, -7.7760e-02,  2.8829e-02,  3.9227e-02, -1.6145e-02,\\n         9.6006e-03, -3.8490e-02,  1.7102e-03, -2.9310e-03, -4.7092e-03,\\n        -1.4718e-02,  2.2584e-02,  1.5232e-02])\",\n          \"tensor([-2.1928e-02,  6.5600e-03, -2.4500e-02, -3.4080e-02,  5.2460e-02,\\n        -1.1276e-03,  4.7858e-02,  3.4278e-03, -2.0937e-02, -6.3145e-02,\\n        -9.7331e-03, -6.1181e-03, -4.7408e-02,  5.2725e-02,  3.7118e-03,\\n         6.9247e-02,  6.1816e-02, -2.8008e-02,  2.3940e-04, -9.7232e-03,\\n         1.2499e-02, -3.7705e-02,  1.6411e-02,  3.9191e-02,  2.7899e-02,\\n         6.1042e-03,  4.0753e-03, -9.0620e-03, -7.5522e-02, -1.8883e-02,\\n         3.2047e-02, -1.5679e-02, -2.7119e-03, -1.3414e-02, -3.6511e-02,\\n        -3.2858e-02, -3.1540e-03,  2.0572e-03,  4.5561e-03, -9.2326e-03,\\n        -1.8327e-02, -2.3632e-02, -1.8805e-02,  7.9053e-03, -4.3978e-02,\\n        -1.9073e-02, -1.3475e-02,  2.5734e-03, -4.2846e-02, -1.3337e-02,\\n        -3.2906e-02,  2.5224e-02,  3.4753e-02,  1.5611e-03,  5.7629e-03,\\n         4.0548e-02, -4.2903e-02, -3.2797e-02,  7.6742e-03, -3.6864e-02,\\n         4.4152e-02, -4.3657e-03,  8.5527e-02, -6.8442e-03,  2.9517e-03,\\n        -8.6434e-03, -2.0478e-04,  2.0037e-02, -6.2112e-02, -1.2210e-02,\\n        -5.8050e-03, -4.0877e-03, -1.9328e-02, -3.4065e-02,  2.9404e-02,\\n        -7.4538e-02,  1.9188e-03,  4.9618e-02,  1.7824e-03, -7.1077e-03,\\n        -7.8975e-03, -1.4662e-02,  3.3629e-02,  6.6817e-02, -2.9879e-03,\\n        -5.5562e-02,  2.1707e-03,  7.8910e-03, -5.7873e-02,  6.0931e-02,\\n        -1.2789e-02, -3.1642e-02,  2.6766e-02,  6.9033e-02,  1.4368e-02,\\n        -1.0537e-02,  9.5592e-02, -8.4620e-03,  3.5014e-02,  1.3741e-03,\\n         6.9551e-03, -6.3096e-03,  9.0944e-04,  2.0904e-02, -1.2329e-01,\\n        -2.7429e-02,  2.7898e-02,  1.6495e-02, -4.1217e-03, -1.7339e-02,\\n         4.7634e-02, -1.5387e-02, -8.0650e-04,  4.6702e-03, -5.7943e-02,\\n         7.0891e-02,  4.5644e-02, -1.2093e-02, -4.4295e-02,  2.6171e-02,\\n         5.8348e-02,  1.8299e-02,  1.4395e-02,  1.2028e-01,  3.4340e-02,\\n         1.0048e-02, -2.6960e-03,  8.3309e-02, -1.8492e-02, -5.1255e-02,\\n         2.4080e-02,  7.3497e-02,  1.2799e-02, -4.8563e-02, -2.3700e-02,\\n        -4.4806e-02,  1.3120e-02, -2.1091e-02,  2.5776e-02,  1.5277e-03,\\n        -5.1707e-02,  4.1095e-02,  1.4426e-02, -1.1615e-03,  2.5649e-02,\\n         1.5121e-02, -1.6312e-02, -2.4472e-02,  2.0511e-02,  3.2454e-02,\\n        -3.0022e-04,  2.0237e-02,  3.3984e-02, -4.9714e-02, -2.5514e-02,\\n         5.0895e-02, -4.3404e-03,  2.1375e-03, -5.0679e-02,  1.3788e-02,\\n         4.3010e-02, -1.1266e-03,  1.8763e-02,  1.1497e-02, -8.4228e-03,\\n         9.4657e-03,  5.3174e-02, -4.9031e-03, -3.0751e-02, -1.5510e-02,\\n        -1.9262e-02,  1.3816e-02,  5.2105e-02,  6.3508e-03, -5.5735e-02,\\n         1.9046e-02,  5.6615e-02,  1.1985e-02,  2.1999e-02, -2.7752e-02,\\n        -9.4502e-02,  9.0901e-02,  1.9156e-02, -1.3992e-03,  5.0227e-02,\\n        -9.5591e-03,  6.3517e-02, -2.9999e-02,  1.1239e-02,  3.1792e-03,\\n        -5.6795e-02, -5.5911e-02,  1.1048e-02,  1.7043e-02,  5.3434e-02,\\n        -1.4109e-02,  1.5795e-02,  5.1508e-02,  9.4767e-03,  2.8689e-02,\\n        -6.7631e-03,  9.6598e-03,  4.4901e-02, -1.6152e-02, -6.7893e-02,\\n         3.3677e-02, -8.1174e-03, -1.8596e-02,  4.0529e-02,  3.7106e-02,\\n         1.3710e-03, -1.0486e-02,  4.2427e-02,  1.0081e-02,  8.4150e-03,\\n        -2.5421e-02,  3.3876e-02, -5.4676e-02,  7.8310e-03, -1.9004e-02,\\n         2.6137e-02, -1.6399e-02, -3.0182e-02,  1.4990e-02, -2.1102e-02,\\n         1.1353e-01,  6.4062e-02, -6.3699e-02,  1.4747e-02,  3.5524e-02,\\n        -2.6713e-03, -1.0126e-02,  2.4580e-02,  9.5186e-03, -2.2252e-03,\\n         1.6596e-02,  2.5616e-02, -5.8781e-02,  4.8096e-02, -7.1265e-02,\\n        -7.5909e-04,  4.1423e-02,  1.4588e-02,  6.4257e-02,  2.5664e-02,\\n        -1.0378e-02,  1.5974e-02,  3.1701e-03,  5.4396e-03, -4.8455e-02,\\n         3.0078e-02, -4.3894e-02,  8.1884e-03, -2.8962e-02, -2.4031e-02,\\n        -8.7757e-02, -5.8814e-03,  1.8914e-02,  6.1757e-02,  4.2026e-02,\\n        -1.7018e-02,  3.4356e-02,  7.9402e-03, -2.2763e-02, -3.1901e-02,\\n        -8.2357e-02, -3.7594e-02,  1.3970e-02,  1.3249e-02,  3.6691e-02,\\n         6.4412e-02,  2.2760e-02,  5.2941e-03,  2.0064e-03, -1.8844e-02,\\n         2.7516e-03,  4.3713e-02,  1.6407e-03,  5.3359e-03, -2.5880e-02,\\n         1.9093e-02,  5.3124e-02, -3.6982e-02, -2.1451e-02,  1.4589e-02,\\n        -5.9327e-02, -6.2680e-03, -5.9029e-02,  9.5152e-03,  2.9536e-02,\\n         3.5096e-02,  5.6348e-02, -1.4066e-02, -7.0484e-04,  4.3265e-03,\\n        -3.3460e-02, -5.3387e-03,  4.3089e-02, -1.8815e-02, -7.9114e-03,\\n        -5.4571e-03, -3.2223e-03, -1.3621e-02,  1.4289e-02, -6.0873e-03,\\n        -1.0427e-02,  1.1678e-02, -2.6431e-02, -2.5640e-01, -3.1203e-02,\\n         1.9718e-02, -5.5951e-02,  3.0537e-02, -2.2296e-02,  3.6501e-02,\\n        -3.4877e-02, -2.5811e-02,  1.6219e-02, -5.6428e-02, -5.5153e-03,\\n        -1.0392e-02,  4.1452e-03,  5.5097e-02,  2.0125e-02, -2.3276e-02,\\n        -3.7445e-02,  1.3108e-02,  3.5781e-02, -5.4263e-04, -7.9570e-02,\\n        -2.4874e-02,  3.5737e-02,  3.1894e-02,  2.4589e-02, -1.5091e-02,\\n        -7.8704e-03, -5.5371e-02, -2.5975e-02, -3.2180e-02, -2.2966e-02,\\n         2.1993e-02, -1.2970e-02,  1.8290e-02, -1.9729e-02,  3.1813e-02,\\n        -3.6725e-02,  7.6036e-03, -2.2681e-02, -3.0079e-02, -6.8525e-02,\\n        -6.6445e-03, -1.6880e-02,  2.9794e-02, -2.0350e-02, -9.9735e-03,\\n        -2.7332e-02,  8.5700e-03,  5.8944e-02, -3.8790e-02,  2.5646e-02,\\n         9.6378e-03, -2.7412e-02, -3.8385e-02, -1.0908e-02, -4.8315e-02,\\n         1.7869e-02, -4.6493e-02, -2.0335e-02,  3.6163e-02, -5.0461e-02,\\n        -4.5253e-02, -3.0576e-02,  1.3858e-02, -6.7891e-02, -1.4571e-02,\\n        -2.9860e-02,  3.9438e-02,  9.0766e-03, -9.6346e-03, -1.0666e-02,\\n         2.6293e-03, -6.7444e-02, -2.0607e-03, -3.1730e-02, -3.7841e-02,\\n        -5.6861e-03,  3.5051e-03, -3.0299e-03, -5.8933e-02, -4.5436e-02,\\n         3.6916e-02,  7.3352e-03,  1.0457e-02, -4.0970e-02, -1.3129e-02,\\n        -7.3541e-03, -2.8422e-02, -2.1573e-02,  8.4134e-02, -5.2869e-03,\\n        -9.8282e-03,  8.9300e-03,  7.4976e-02,  1.8568e-02,  2.2661e-02,\\n         1.0065e-02,  4.8030e-02, -8.4177e-03,  2.1699e-02, -3.8627e-02,\\n         3.9416e-02,  9.2683e-03,  4.5755e-02,  2.2631e-02, -3.6607e-02,\\n         2.5048e-02,  3.0829e-02, -2.1526e-02,  4.9178e-02, -8.5765e-03,\\n         2.8900e-02, -6.1230e-02,  1.3585e-03, -6.4353e-02,  2.2877e-02,\\n         6.3371e-02,  1.2001e-02,  3.4546e-02, -4.3650e-02,  4.5089e-02,\\n        -5.3161e-02, -1.4973e-02, -6.6368e-02, -6.9621e-03, -3.9704e-02,\\n        -1.8785e-02,  4.9383e-03,  2.3429e-02,  1.6272e-02, -1.8609e-02,\\n         1.0419e-02,  1.1034e-02,  4.0339e-02, -7.3182e-03, -6.2779e-02,\\n        -7.4614e-02, -1.1926e-03, -9.8413e-03, -1.4739e-02,  8.9374e-03,\\n         3.9708e-02,  2.1720e-03,  1.4393e-02,  9.5153e-03, -5.1457e-03,\\n        -3.8619e-02, -2.8833e-02,  4.2183e-02,  2.5164e-04, -2.1134e-02,\\n         4.1989e-02, -7.6382e-02,  5.2824e-03, -2.5911e-02,  1.9893e-02,\\n         1.2621e-02, -1.1931e-02, -5.0747e-02, -2.1616e-02, -4.6497e-02,\\n         1.7484e-02,  1.1552e-02, -4.4957e-02,  1.0455e-01, -9.5387e-03,\\n        -4.8640e-02, -1.6960e-02, -6.0803e-04,  2.0940e-02, -4.8816e-02,\\n        -6.3497e-02,  2.9089e-02, -2.9614e-02,  3.5632e-02, -6.5279e-03,\\n         2.3716e-02,  2.6739e-02,  2.6127e-02, -1.1788e-02, -1.7406e-02,\\n         4.4442e-03,  6.0584e-03,  6.2266e-02, -1.2867e-02,  3.1551e-03,\\n        -5.6867e-02, -3.1390e-02,  1.0359e-02,  2.3147e-02,  4.5906e-03,\\n        -2.3654e-02,  2.0642e-02, -7.2834e-02, -9.6127e-02,  2.4364e-02,\\n         3.7919e-02,  5.4880e-02,  1.8520e-02,  3.8019e-03, -1.4860e-02,\\n        -2.4301e-02,  1.0774e-02,  2.3396e-04, -9.4380e-02,  3.4963e-02,\\n         2.5945e-02, -3.2087e-02,  1.1995e-02, -3.5906e-02, -3.5981e-02,\\n        -2.6751e-02, -2.1957e-02,  1.8788e-02, -4.4527e-02,  1.7198e-02,\\n        -2.0562e-02, -3.8620e-03,  6.0255e-03,  3.9091e-02, -3.8384e-03,\\n        -2.3323e-02,  3.6709e-03, -5.2431e-02,  3.3798e-02, -1.9731e-03,\\n         3.0582e-03, -2.7425e-02, -2.2851e-02,  8.0982e-03, -1.5774e-02,\\n         7.1648e-03,  6.5160e-02,  9.8477e-04,  2.1085e-02,  3.5188e-02,\\n        -1.0669e-02,  5.0194e-03,  4.6848e-02,  3.6653e-02, -1.3743e-02,\\n        -1.4472e-02, -1.8755e-02,  1.2762e-02,  5.4382e-02, -8.4238e-03,\\n         4.6341e-04, -2.4748e-03,  4.4581e-02,  1.1395e-02,  1.2812e-02,\\n        -2.5737e-02, -3.9112e-02,  6.4366e-02, -2.4482e-02, -6.0625e-02,\\n         2.5574e-02, -1.5019e-02,  2.7486e-03,  3.0237e-02,  3.6482e-03,\\n        -2.7426e-02,  2.6307e-02,  4.3576e-02,  2.1949e-03,  1.5998e-02,\\n        -4.9528e-02, -1.4625e-03, -1.6907e-02, -2.3287e-02, -7.9361e-02,\\n        -2.9131e-03, -3.2619e-02,  1.2457e-02,  1.0701e-02, -3.8696e-02,\\n        -2.6316e-02, -7.2399e-03, -6.8891e-02,  3.2093e-03,  2.4575e-02,\\n         7.7047e-04, -1.3923e-02,  5.0036e-02, -5.2281e-02, -4.1716e-03,\\n         2.0453e-02, -4.7635e-02,  4.0393e-02, -1.7219e-02,  3.5616e-02,\\n        -1.0449e-02,  1.1632e-03,  1.6257e-02, -1.7669e-02,  4.3686e-02,\\n         7.3279e-02, -2.5775e-02, -8.9997e-03, -4.4205e-02,  3.1667e-02,\\n         7.0260e-02,  7.7828e-03, -2.9778e-02,  1.5825e-02,  4.1247e-03,\\n        -6.5194e-02,  7.0815e-03,  2.2261e-02,  7.3198e-03, -5.3821e-02,\\n         5.7243e-02,  3.7341e-02, -7.6252e-02, -9.9194e-03,  2.2127e-02,\\n        -4.9905e-02,  2.1277e-02, -5.3442e-03,  2.3530e-02, -2.0224e-02,\\n         4.7529e-02,  3.4331e-02,  4.9894e-02,  3.6361e-02, -5.0639e-02,\\n         1.5923e-02,  2.8572e-03,  8.0701e-02,  4.8258e-02,  3.5147e-02,\\n        -1.4341e-02,  7.7789e-02, -5.2222e-03, -7.5781e-02,  1.7460e-02,\\n         2.2679e-05,  2.7333e-02,  2.2578e-02,  1.6946e-02,  7.7076e-02,\\n        -2.6776e-02,  5.8001e-02,  1.5771e-02,  3.6350e-02,  1.9251e-02,\\n        -1.6830e-02,  4.0868e-02,  2.4454e-02,  2.5220e-02,  8.6912e-04,\\n        -3.1178e-02, -6.0578e-02,  5.6664e-02, -1.3602e-02, -7.8880e-03,\\n         2.4459e-02, -1.4336e-02,  5.1202e-02, -3.6983e-02, -7.7566e-03,\\n         4.4357e-02,  3.3375e-02, -6.6247e-03,  2.7926e-02,  5.0281e-02,\\n        -2.9467e-02,  3.5038e-03,  2.3356e-02, -8.5424e-03,  1.0081e-02,\\n        -2.5287e-02,  2.1750e-02,  5.0179e-02, -1.2998e-02,  2.2368e-02,\\n        -1.4750e-02,  2.6152e-02,  5.2953e-03,  5.4464e-02, -3.1630e-02,\\n        -2.9561e-02, -4.3989e-02, -3.1828e-02, -2.9462e-02, -4.6714e-03,\\n        -1.1067e-03, -5.5991e-02, -5.2649e-02, -1.8276e-02, -1.5878e-02,\\n         1.6809e-02, -1.8288e-02, -6.6484e-02, -4.5713e-02,  2.4463e-02,\\n         3.2192e-02, -1.1549e-02,  3.3708e-02,  6.5036e-02,  4.9748e-02,\\n        -2.3858e-02, -2.5402e-02,  2.3019e-03,  1.4843e-02, -1.4155e-02,\\n        -7.5424e-03, -9.2405e-02,  3.6516e-02,  4.1431e-02, -1.0577e-02,\\n        -7.1129e-02,  3.2171e-02, -4.1130e-02, -1.7546e-02,  3.9681e-02,\\n         1.3990e-02, -5.8012e-03, -2.7345e-02, -5.2461e-03, -2.3556e-03,\\n        -2.3815e-02,  4.8733e-02, -3.8094e-02,  6.5027e-02,  3.2472e-02,\\n         7.5574e-03, -3.6950e-02,  4.2318e-02,  2.8659e-02, -1.8233e-02,\\n        -3.6814e-02, -2.5849e-02, -4.0734e-02, -9.2826e-02,  2.1899e-03,\\n        -5.4394e-03, -5.5611e-03, -5.1915e-02,  1.8526e-02, -2.2685e-02,\\n        -6.8814e-03,  4.1945e-03, -3.5026e-03, -1.5490e-02, -3.4807e-02,\\n        -1.1287e-02, -6.0801e-02,  3.0967e-02,  3.1356e-02, -3.2649e-02,\\n        -1.1853e-02, -3.3774e-02,  1.4477e-02, -7.2830e-03,  2.4599e-05,\\n        -6.2805e-03,  3.5291e-02, -7.5512e-03])\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ft_embedding\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["## $\\color{blue}{Dataset:}$"],"metadata":{"id":"cRp9eBQ4f_Dq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hT3DSEhJaFzN"},"outputs":[],"source":["train_embeddings = [torch.tensor(array) for array in df_train['ft_embedding']]\n","train_x = torch.stack(train_embeddings).to(device)\n","\n","dev_embeddings = [torch.tensor(array) for array in df_dev['ft_embedding']]\n","dev_x = torch.stack(dev_embeddings).to(device)\n","\n","test_embeddings = [torch.tensor(array) for array in df_test['ft_embedding']]\n","test_x = torch.stack(test_embeddings).to(device)"]},{"cell_type":"code","source":["# train_y = torch.LongTensor(list(df_train['book_idx'])).to(device)\n","# dev_y = torch.LongTensor(list(df_dev['book_idx'])).to(device)\n","# test_y = torch.LongTensor(list(df_test['book_idx'])).to(device)\n","\n","train_y = torch.LongTensor(list(df_train['chapter_idx'])).to(device)\n","dev_y = torch.LongTensor(list(df_dev['chapter_idx'])).to(device)\n","test_y = torch.LongTensor(list(df_test['chapter_idx'])).to(device)"],"metadata":{"id":"Y6wLXwgZgNoz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","# assuming already tensors, allready on device\n","class VanillaDataset(Dataset):\n","  \"\"\"Dataset maker\"\"\"\n","\n","  def __init__(self, x, y):\n","    self.x = x\n","    self.y = y\n","\n","  def __getitem__(self,index):\n","    x = self.x[index]\n","    y = self.y[index]\n","\n","    return x, y\n","\n","  def __len__(self):\n","    return len(self.y)\n"],"metadata":{"id":"Tv26LKgEgPJY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = VanillaDataset(train_x, train_y)\n","dev_dataset = VanillaDataset(dev_x, dev_y)\n","test_dataset = VanillaDataset(test_x, test_y)"],"metadata":{"id":"fzMdq10amXSA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset[0][0].size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1acpLl58muUM","executionInfo":{"status":"ok","timestamp":1729606037178,"user_tz":-120,"elapsed":14,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"2c38965d-6a12-4898-a740-233a43f9eb72"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([768])"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["## $\\color{blue}{Model:}$"],"metadata":{"id":"V2dl9alJo_8E"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class DenseBlock(nn.Module):\n","    def __init__(self, input_size, output_size, dropout_rate):\n","        super(DenseBlock, self).__init__()\n","        self.linear = nn.Linear(input_size, output_size)\n","        self.batch_norm = nn.BatchNorm1d(output_size)\n","        self.activation = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, x):\n","        x = self.linear(x)\n","        x = self.batch_norm(x)\n","        x = self.activation(x)\n","        x = self.dropout(x)\n","        return x\n","\n","class VanillaModel(nn.Module):\n","    def __init__(self, dropout_rate):\n","        super(VanillaModel, self).__init__()\n","\n","        # Define the dense blocks\n","        self.block1 = DenseBlock(768, 400, dropout_rate)\n","        self.block2 = DenseBlock(400, 200, dropout_rate)\n","        self.final_layer = nn.Linear(200, 70)\n","\n","        self.initialize_weights()\n","\n","    def forward(self, x):\n","        x = self.block1(x)  # Bx768 -> Bx400\n","        x = self.block2(x)  # Bx400 -> Bx200\n","        x = self.final_layer(x)  # Bx200 -> Bx70\n","\n","        return x\n","\n","    def initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear):\n","                nn.init.xavier_uniform_(m.weight)\n","                if m.bias is not None:\n","                    nn.init.zeros_(m.bias)\n"],"metadata":{"id":"RhaSLUB1l6Bu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## $\\color{blue}{Helper:}$"],"metadata":{"id":"IehbMcagpoTr"}},{"cell_type":"code","source":["def accuracy(outputs, labels):\n","    # argmax to get predicted classes\n","    _, predicted = torch.max(outputs, 1)\n","\n","    # count correct\n","    correct = (predicted == labels).sum().item()\n","\n","    # get average\n","    acc = correct / labels.size(0)  # Total number of samples\n","    return acc"],"metadata":{"id":"ZL4yGRfdtIVA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","def train(model, train_loader, criterion, optimizer):\n","    model.train()\n","    epoch_train_losses = []\n","    epoch_train_accuracy = []\n","\n","    for batch_idx, (x, y) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","\n","        out = model(x)\n","        train_loss = criterion(out, y)\n","        train_accuracy = accuracy(out, y)\n","\n","        epoch_train_losses.append(train_loss.item())\n","        epoch_train_accuracy.append(train_accuracy)\n","\n","        # Backpropagation and optimization\n","        train_loss.backward()\n","        optimizer.step()\n","\n","    return np.mean(epoch_train_losses), np.mean(epoch_train_accuracy)"],"metadata":{"id":"0p4GswPcz6di"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def validate(model, dev_loader, criterion):\n","    model.eval()\n","    epoch_dev_losses = []\n","    epoch_dev_accuracy = []\n","\n","    with torch.no_grad():\n","        for batch_idx, (x, y) in enumerate(dev_loader):\n","            out = model(x)\n","\n","            dev_loss = criterion(out, y)\n","            dev_accuracy = accuracy(out, y)\n","\n","            epoch_dev_losses.append(dev_loss.item())\n","            epoch_dev_accuracy.append(dev_accuracy)\n","\n","    return np.mean(epoch_dev_losses), np.mean(epoch_dev_accuracy)"],"metadata":{"id":"ujWAOFw30Uh7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import namedtuple\n","Stats = namedtuple('Stats', [\n","    'train_loss',\n","    'train_accuracy',\n","    'dev_loss',\n","    'dev_accuracy',\n","    'epoch',\n","    'bs',\n","    'lr',\n","    'alpha',\n","    'max_accuracy'\n","])"],"metadata":{"id":"gBvkpcO7Ktyn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def gen_config(lr_low, lr_high, alpha_low, alpha_high, b_size, b_step):\n","  bs_list = [b_size - b_step, b_size, b_size + b_step]\n","  bs = int(2**np.random.choice(bs_list))\n","  lr = round(10**float(np.random.uniform(lr_low,lr_high)),6)\n","  alpha = round(10**float(np.random.uniform(alpha_low,alpha_high)),6)\n","  return lr, alpha, bs"],"metadata":{"id":"zLMDuEADZFav"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def gen_ranges( lr, lr_range, alpha, alpha_range, b_size, iteration):\n","\n","  lr_center = lr\n","  lr_low = lr_center - lr_range/2\n","  lr_high = lr_center + lr_range/2\n","  lr_diff = lr_high - lr_low\n","\n","  alpha_center = alpha\n","  alpha_low = alpha_center - alpha_range/2\n","  alpha_high = alpha_center + alpha_range/2\n","  alpha_diff = alpha_high - alpha_low\n","\n","  b_step = 2 - iteration\n","\n","  return (lr_low, lr_high, alpha_low, alpha_high, b_size, b_step)"],"metadata":{"id":"g8RmMZjYZFoS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def search_stats(results):\n","  best_stats = None\n","  max_dev_accuracy = 0\n","  for i in range(len(results)):\n","    acc = results[i].dev_accuracy\n","    if acc > max_dev_accuracy:\n","      best_stats = results[i]\n","      max_dev_accuracy = acc\n","  return best_stats"],"metadata":{"id":"NFYizCESbNC1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## $\\color{blue}{Training:}$"],"metadata":{"id":"c6l1T-Uip7FG"}},{"cell_type":"code","source":["def tv_run(epochs, model, bs, lr, alpha, max_accuracy, path, verbose = 0):\n","  \"\"\"\n","  Runs a training setup\n","  verbose == 1 - print model results\n","  verbose == 2 -> print epoch and model results\n","  \"\"\"\n","  if bs < 16:\n","    bs = 16\n","\n","  # Set up new model\n","  model = model.to(device)\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=alpha)\n","\n","  # Prepare data loaders\n","  train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n","  dev_loader = DataLoader(dev_dataset, batch_size=bs)\n","\n","  # Hold epoch stats\n","  train_losses = []\n","  train_accuracy = []\n","  dev_losses = []\n","  dev_accuracy = []\n","  epoch_holder = []\n","\n","  # Break if no improvement\n","  current_best = 0\n","  no_improvement = 0\n","\n","  # Run epochs\n","  for epoch in range(epochs):\n","\n","    # break out of epochs\n","    if no_improvement >= 4:\n","      break\n","\n","    # call training and validation functions\n","    train_loss, train_acc = train(model, train_loader, criterion, optimizer)\n","    dev_loss, dev_acc = validate(model, dev_loader, criterion)\n","\n","    # Store epoch stats\n","    train_losses.append(train_loss)\n","    train_accuracy.append(train_acc)\n","    dev_losses.append(dev_loss)\n","    dev_accuracy.append(dev_acc)\n","    epoch_holder.append(epoch + 1)\n","\n","    # check for improvement\n","    if dev_acc > current_best:\n","      current_best = dev_acc\n","      no_improvement = 0\n","    else:\n","      no_improvement += 1\n","\n","    # save best model\n","    if dev_acc > max_accuracy:\n","      torch.save(model.state_dict(), path)\n","      max_accuracy = dev_acc\n","\n","    # optionally print epoch results\n","    if verbose == 2:\n","      print(f'\\n --------- \\nEpoch: {epoch + 1}\\n')\n","      print(f'Epoch {epoch + 1} train loss: {train_loss:.4f}')\n","      print(f'Epoch {epoch + 1} train accuracy: {train_acc:.4f}')\n","      print(f'Epoch {epoch + 1} dev loss: {dev_loss:.4f}')\n","      print(f'Epoch {epoch + 1} dev accuracy: {dev_acc:.4f}')\n","\n","  # save best results\n","  max_ind = np.argmax(dev_accuracy)\n","\n","  stats = Stats(\n","      train_losses[max_ind],\n","      train_accuracy[max_ind],\n","      dev_losses[max_ind],\n","      dev_accuracy[max_ind],\n","      epoch_holder[max_ind],\n","      bs, lr, alpha,\n","      max_accuracy\n","  )\n","\n","  # optionally print model results\n","  if verbose in [1,2]:\n","    print('\\n ######## \\n')\n","    print(f'bs:{stats.bs}, lr:{stats.lr}, alpha:{stats.alpha} @ epoch {stats.epoch}.')\n","    print(f'TL:{stats.train_loss}, TA:{stats.train_accuracy}.')\n","    print(f'DL:{stats.dev_loss}, DA:{stats.dev_accuracy}')\n","\n","  return stats"],"metadata":{"collapsed":true,"id":"QolAzAGlrqKb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Main Admin\n","\"\"\"\n","epochs = 40\n","max_accuracy = 0\n","path = \"class/models/embedding_ft_classify.pt\"\n","results = []\n","\n","\"\"\"\n","init random search\n","lr [10^-5 - 10^-1]\n","alpha [10^-5 - 10^-1]\n","bs [8, 32, 128]\n","\"\"\"\n","lr_low = -5\n","lr_high = -1\n","lr_range = lr_high - lr_low\n","\n","alpha_low = -5\n","alpha_high = -1\n","alpha_range = alpha_high - alpha_low\n","\n","b_size = 5\n","b_step = 2\n","\n","count = 0\n","\n","\"\"\"\n","Hyperparameter Search\n","\"\"\"\n","\n","for i in range(3):\n","  # debug\n","  print(f'round: {i}')\n","  print(f'lr_low{lr_low}, lr_high{lr_high}, lr_range{lr_range}')\n","  print(f'alpha_low{alpha_low}, lr_high{alpha_high}, lr_range{alpha_range}')\n","  print(f'b_size{b_size}')\n","  print(f'b_step{b_step}')\n","  print('max', max_accuracy)\n","\n","  for j in range(27):\n","    count += 1\n","    print(count)\n","\n","    # get config\n","    lr, alpha, bs = gen_config(lr_low, lr_high, alpha_low, alpha_high, b_size, b_step)\n","\n","    # define model\n","    model = VanillaModel(.1) # model with dropout\n","    model = model.to(device)\n","\n","    # run training\n","    res = tv_run(epochs, model, bs, lr, alpha, max_accuracy, path, verbose = 0)\n","    max_accuracy = res.max_accuracy\n","    results.append(res)\n","\n","  # get best result of the round or even so far\n","  stats = search_stats(results)\n","\n","\n","  print(stats) # debug\n","\n","  # reconfigure the new hypers\n","  lr = np.log10(stats.lr)\n","  lr_range = lr_range / 3\n","\n","  alpha = np.log10(stats.alpha)\n","  alpha_range = alpha_range / 3\n","\n","  bs = np.log2(stats.bs)\n","\n","  config = gen_ranges(lr, lr_range, alpha, alpha_range, bs, i + 1)\n","  lr_low, lr_high, alpha_low, alpha_high, b_size, b_step = config\n","  lr_range = lr_high - lr_low\n","  alpha_range = alpha_high - alpha_low\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"4tRW09rVZOUu","outputId":"39ece187-c753-493d-fbe2-481efbc1edc3","executionInfo":{"status":"ok","timestamp":1729608066978,"user_tz":-120,"elapsed":1997970,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["round: 0\n","lr_low-5, lr_high-1, lr_range4\n","alpha_low-5, lr_high-1, lr_range4\n","b_size5\n","b_step2\n","max 0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","Stats(train_loss=1.9579376748402912, train_accuracy=0.3785, dev_loss=2.103779028673641, dev_accuracy=0.3780737704918033, epoch=25, bs=16, lr=5.3e-05, alpha=1.4e-05, max_accuracy=0.3780737704918033)\n","round: 1\n","lr_low-4.942390797065878, lr_high-3.6090574637325443, lr_range1.3333333333333335\n","alpha_low-5.520538630988429, lr_high-4.187205297655095, lr_range1.333333333333334\n","b_size4.0\n","b_step1\n","max 0.3780737704918033\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n","Stats(train_loss=1.9579376748402912, train_accuracy=0.3785, dev_loss=2.103779028673641, dev_accuracy=0.3780737704918033, epoch=25, bs=16, lr=5.3e-05, alpha=1.4e-05, max_accuracy=0.3780737704918033)\n","round: 2\n","lr_low-4.497946352621433, lr_high-4.0535019081769885, lr_range0.44444444444444464\n","alpha_low-5.076094186543984, lr_high-4.6316497420995395, lr_range0.44444444444444464\n","b_size4.0\n","b_step0\n","max 0.3780737704918033\n","55\n","56\n","57\n","58\n","59\n","60\n","61\n","62\n","63\n","64\n","65\n","66\n","67\n","68\n","69\n","70\n","71\n","72\n","73\n","74\n","75\n","76\n","77\n","78\n","79\n","80\n","81\n","Stats(train_loss=1.9404880266189575, train_accuracy=0.38316666666666666, dev_loss=2.1313831786640356, dev_accuracy=0.3790983606557377, epoch=29, bs=16, lr=6.5e-05, alpha=1.5e-05, max_accuracy=0.3790983606557377)\n"]}]},{"cell_type":"code","source":["import dill\n","def save_results_to_file(namedtuples, filename):\n","    \"\"\"Saves a list of namedtuples to a specified file using dill.\"\"\"\n","    with open(filename, 'wb') as f:\n","        dill.dump(namedtuples, f)\n","\n","def load_results_from_file(filename):\n","    \"\"\"Loads a list of namedtuples from a specified file using dill.\"\"\"\n","    with open(filename, 'rb') as f:\n","        return dill.load(f)"],"metadata":{"id":"0UVgjxKJ1v28"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = 'class/results/'\n","save_results_to_file(results, path + 'embedding_ft_classify.pk')"],"metadata":{"id":"VS3U8EZP2niP"},"execution_count":null,"outputs":[]}]}