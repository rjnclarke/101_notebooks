{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyPo+kByzWuIVWA2EWnxFt3+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Text Classification - Training a GNN\n"],"metadata":{"id":"FBjat8lrIEsf"}},{"cell_type":"markdown","source":["## $\\color{blue}{Sections:}$\n","\n","* Preamble\n","1.   Admin\n","2.   Dataset\n","3.   Model\n","4.   Train - Validate\n","5.   Training Loop"],"metadata":{"id":"O5wHwOnkIKvx"}},{"cell_type":"markdown","source":["## $\\color{blue}{Preamble:}$\n","\n","We now train a GNN in basic PyTorch. The model will look like a GCN. Inference willbe completed in another notebook as the whole graph must be uploaded at once."],"metadata":{"id":"2LYAHLRYIY_C"}},{"cell_type":"markdown","source":["## $\\color{blue}{Admin}$\n","* Install relevant Libraries\n","* Import relevant Libraries"],"metadata":{"id":"udtGvzIPItVT"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vfqeSzgDH285","executionInfo":{"status":"ok","timestamp":1734532898436,"user_tz":-60,"elapsed":4137,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"214b7ea2-c45d-415c-c35f-899621dbf268"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}],"source":["import torch\n","import pandas as pd\n","from google.colab import drive\n","import numpy as np\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","source":["drive.mount(\"/content/drive\")\n","%cd '/content/drive/MyDrive'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FSXQ2wAfPWoF","executionInfo":{"status":"ok","timestamp":1734532916412,"user_tz":-60,"elapsed":17981,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"34d6d8f6-bb39-42d9-91dc-58a4f63bf3ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive\n"]}]},{"cell_type":"markdown","source":["## $\\color{blue}{Data}$\n","\n","* Connect to Drive\n","* Load the data\n","* Load adjacency matrices"],"metadata":{"id":"q4Mn5bkiJDE8"}},{"cell_type":"code","source":["path = 'class/datasets/'\n","df_train = pd.read_pickle(path + 'df_train')\n","df_dev = pd.read_pickle(path + 'df_dev')\n","df_test = pd.read_pickle(path + 'df_test')"],"metadata":{"id":"gPwH-5O5JHeM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = 'class/tensors/adj_{}.pt'\n","\n","# train\n","train_people = torch.load(path.format('train_people'))\n","train_locations = torch.load(path.format('train_locations'))\n","train_entities = torch.load(path.format('train_entities'))\n","\n","# dev\n","dev_people = torch.load(path.format('dev_people'))\n","dev_locations = torch.load(path.format('dev_locations'))\n","dev_entities = torch.load(path.format('dev_entities'))\n","\n","# val (contains the adjacency matrix for both the training and the development set)\n","val_people = torch.load(path.format('val_people'))\n","val_locations = torch.load(path.format('val_locations'))\n","val_entities = torch.load(path.format('val_entities'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DXpjrdiiJSO0","executionInfo":{"status":"ok","timestamp":1734532975908,"user_tz":-60,"elapsed":45875,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"418d5875-b19c-40f4-c15d-9cc2d32d8305"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-4-fd49eb201090>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  train_people = torch.load(path.format('train_people'))\n","<ipython-input-4-fd49eb201090>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  train_locations = torch.load(path.format('train_locations'))\n","<ipython-input-4-fd49eb201090>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  train_entities = torch.load(path.format('train_entities'))\n","<ipython-input-4-fd49eb201090>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  dev_people = torch.load(path.format('dev_people'))\n","<ipython-input-4-fd49eb201090>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  dev_locations = torch.load(path.format('dev_locations'))\n","<ipython-input-4-fd49eb201090>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  dev_entities = torch.load(path.format('dev_entities'))\n","<ipython-input-4-fd49eb201090>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  val_people = torch.load(path.format('val_people'))\n","<ipython-input-4-fd49eb201090>:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  val_locations = torch.load(path.format('val_locations'))\n","<ipython-input-4-fd49eb201090>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  val_entities = torch.load(path.format('val_entities'))\n"]}]},{"cell_type":"markdown","source":["## $\\color{blue}{Dataset:}$"],"metadata":{"id":"Tml4hS4iKIGX"}},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","from copy import deepcopy\n","\n","def sample_neighborhood(A, inds, neighbor_max, branch_max, seed=None):\n","    # Set the random seed for deterministic responses\n","    if seed is not None:\n","        np.random.seed(seed)\n","\n","    np.random.shuffle(A)  # Shuffle the list of adjacency matrices in place\n","    sampled_indices = set(inds)  # Initialize the set of sampled indices\n","    # print(\"-----\\nSTART\\n\")\n","    # print(\"A size: \",A[0].size())\n","    # print(\"sampled inices: \", sampled_indices)\n","    # print(\"neighbor_max: \", neighbor_max)\n","    # print(\"batch max: \", branch_max)\n","    # print(\"------\\nLOOPING THROUGH INDS: \", inds)\n","    for ind in inds:  # Iterate through node in mini-batch\n","        # print(\"-----\\n ind: \", ind)\n","        break_to_outer = False\n","        neighbors = set()\n","\n","        for adj in A:  # Iterate through all adjacency matrices\n","            if break_to_outer:\n","              break\n","\n","            # Get the indices of all neighbors that idx links to\n","            disclude = set([ind]) | sampled_indices\n","            new_neighbors = [neighbor.item() for neighbor in (adj[ind] > 0).nonzero(as_tuple=True)[0] if neighbor.item() not in disclude]\n","            neighbors.update(new_neighbors)\n","            # print(\"-------\\nnew neighbors: \", new_neighbors)\n","            # print(\"Value of neighbors: \", neighbors)\n","\n","            if len(neighbors) >= neighbor_max:  # Check if we have too many neighbors\n","                # Take a random subset using np.random.choice\n","                # print(\"------\\nneighbors is bigger than max neighbors\")\n","                neighbors = set(np.random.choice(list(neighbors), neighbor_max, replace=False))\n","                # print(\"Value of neighbors: \", neighbors)\n","\n","\n","            copy_neighbors = deepcopy(neighbors)\n","            for idx in copy_neighbors:\n","                if break_to_outer:\n","                  break\n","                # print(\"-----\\nLooping through neighbors with ind \", idx)\n","\n","                neighbors_neighbors = set()\n","                for adj in A:\n","                    disclude = set([ind,idx]) | sampled_indices | neighbors\n","                    new_neighbors_neighbors = [neighbor.item() for neighbor in (adj[idx] > 0).nonzero(as_tuple=True)[0] if neighbor.item() not in disclude]\n","                    if len(new_neighbors_neighbors) > neighbor_max:\n","                      new_neighbors_neighbors = set(np.random.choice(list(new_neighbors_neighbors), neighbor_max, replace = False))\n","                    neighbors_neighbors.update(new_neighbors_neighbors)\n","                    # print(\"-------\\nnew neighbors_neighbors: \", new_neighbors_neighbors)\n","                    # print(\"Value of neighbors_neighbors: \", neighbors_neighbors)\n","\n","                    if len(neighbors) + len(neighbors_neighbors) >= branch_max:\n","                      # print(f\"------branch max exceeded for ind {ind} with len neighbors = {len(neighbors)} and len neighbors neighbors = {len(neighbors_neighbors)}\")\n","\n","                      neighbors_neighbors = set(np.random.choice(list(neighbors_neighbors), branch_max - len(neighbors), replace=False))\n","                      # print(\"Value of neighbors_neighbors: \", neighbors_neighbors)\n","\n","                      neighbors.update(neighbors_neighbors)\n","                      # print(\"New value of neighbors: \", neighbors)\n","\n","                      break_to_outer = True\n","                      break\n","\n","                    neighbors.update(neighbors_neighbors)\n","                    # print(\"New value of neighbors with new neighbors_neighbors: \", neighbors)\n","\n","        sampled_indices.update(neighbors)  # Add new neighbors\n","        # print(f\"____\\n END OF ind {ind}; sampled indeices is now {sampled_indices}\")\n","\n","    return list(sampled_indices)"],"metadata":{"id":"wvPKQX4B_k9J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#check the conditions"],"metadata":{"id":"gQKaLkU1jxOb"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","a = torch.Tensor([\n","    [0,1,0,0,0,0,0,1],\n","    [1,0,0,1,0,1,0,0],\n","    [0,1,0,1,0,0,1,0],\n","    [0,1,0,0,1,0,0,1],\n","    [1,1,0,0,1,0,0,0],\n","    [0,0,1,1,0,1,1,0],\n","    [0,0,0,1,0,0,0,1],\n","    [0,0,0,1,0,1,1,1]\n","])\n","A = []\n","A.append(a)\n","inds = [1,2]\n","nm = 2\n","bm = 4\n","seed=42"],"metadata":{"id":"B5qimRXRPy4k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","def neighbor_analysis(primary_inds: list[int], inds: list[int], adjacency_matrix: torch.Tensor):\n","    result = []\n","\n","    # Loop over each primary index\n","    for primary_idx in primary_inds:\n","        # Get neighbors for the primary index\n","        neighbors = []\n","        for ind in inds:\n","            # Check if ind is a neighbor of primary_idx\n","            if adjacency_matrix[primary_idx, ind] == 1:\n","                neighbors.append(ind)\n","\n","        # Find neighbors of neighbors (n + 1 neighbors)\n","        neighbors_of_neighbors = []\n","        for neighbor in neighbors:\n","            for ind in inds:\n","                # Check if ind is a neighbor of the current neighbor\n","                if adjacency_matrix[neighbor, ind] == 1 and ind != primary_idx:\n","                    neighbors_of_neighbors.append(ind)\n","\n","        # Remove duplicates for the neighbors of neighbors\n","        neighbors_of_neighbors = list(set(neighbors_of_neighbors))\n","\n","        # Add the tuple to results\n","        result.append((primary_idx, neighbors, neighbors_of_neighbors))\n","\n","    return result"],"metadata":{"id":"pj2hFslUIQu5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","inds = random.sample(list(range(12000)),6)\n","print(inds)\n","#inds = list(range(101,105))\n","sample = sample_neighborhood([train_entities], inds, 4, 16)\n","print(sample)\n","print(len(sample))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3_FRZ_VOZt0D","executionInfo":{"status":"ok","timestamp":1734532976830,"user_tz":-60,"elapsed":323,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"2c26bf91-33f1-4314-930e-2a9fe217cce9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[2144, 2327, 5933, 10288, 346, 5847]\n","[7431, 6920, 5010, 8468, 11540, 2327, 5149, 543, 8865, 6951, 11052, 5933, 1711, 10288, 432, 4146, 3632, 4786, 6198, 2879, 2374, 4679, 9802, 7883, 4945, 9042, 7638, 5847, 346, 9820, 2144, 11872, 6624, 7523, 6888, 1001, 1774, 8574]\n","38\n"]}]},{"cell_type":"code","source":["analysis = neighbor_analysis(inds, sample, train_entities)\n","for item in analysis:\n","  print(item)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"flg7EyIjIm9X","executionInfo":{"status":"ok","timestamp":1734532976830,"user_tz":-60,"elapsed":19,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"165dfc1b-f2e6-481b-db3e-fad1354c9221"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(2144, [6920, 5010, 8468, 8865, 1711, 6198, 2879, 2374, 9802, 9042, 11872, 8574], [11872, 8865, 6624, 2374, 6920, 9802, 1711, 432, 5010, 9042, 8468, 4146, 6198, 7638, 8574, 2879])\n","(2327, [], [])\n","(5933, [], [])\n","(10288, [7431, 11540, 5149, 543, 6951, 11052, 3632, 4786, 4679, 7883, 4945, 9820, 7523, 6888, 1001, 1774], [7523, 6951, 4679, 6888, 1001, 7883, 11052, 7431, 1774, 3632, 4945, 4786, 11540, 9820, 5149, 543])\n","(346, [], [])\n","(5847, [], [])\n"]}]},{"cell_type":"code","source":["# inds = random.sample(list(range(12000)),4)\n","inds = list(range(101,105))\n","sample = sample_neighborhood([train_entities], inds, 16, 64, seed=42)\n","print(sample)\n","print(len(sample))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qK4tP7_j-ZH4","executionInfo":{"status":"ok","timestamp":1734532976830,"user_tz":-60,"elapsed":13,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"a6a8e195-9b34-45cd-a794-3ccf04c1f51d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[9221, 2061, 3599, 10261, 9214, 6937, 4386, 2855, 8492, 4654, 10039, 4152, 2363, 11581, 317, 6210, 580, 6725, 9046, 599, 5721, 3673, 1883, 3679, 4707, 4964, 101, 102, 103, 104, 8551, 5480, 5485, 11121, 3953, 7285, 8824, 11904, 6274, 9607, 11660, 7317, 5271, 8856, 3235, 7594, 4524, 5550, 10927, 4023, 1722, 4030, 10430, 8641, 4305, 8916, 9688, 2009, 10203, 10207, 9440, 9185, 1250, 8680, 6640, 11763, 2804, 510]\n","68\n"]}]},{"cell_type":"code","source":["class GNNDataset(Dataset):\n","  def __init__(self, H, A, labels, meta_indices, neighbor_max=4, branch_max=16, seed=None):\n","    \"\"\"Custom dataset with neighborhood sampling\n","\n","    Args:\n","      H : torch.tensor\n","        input embeddings (n x d)\n","\n","      A : list[torch.tensor]\n","        list of (n x n)\n","\n","      labels : torch.LongTensor\n","        y\n","\n","      meta_indices : torch.LongTensor\n","        index of datapoint to filter validation score\n","\n","      neighbor_max : int\n","        max neighbors for each node in mini-batch\n","\n","      batch_max : int\n","        max size of batch\n","\n","    \"\"\"\n","    # All inits must be tensors\n","    self.H = H.to(device)\n","    self.A = [a.to(device) for a in A]\n","    self.labels = labels.to(device)\n","    self.meta_indices = meta_indices\n","    self.neighbor_max = neighbor_max\n","    self.branch_max = branch_max\n","    self.seed = seed\n","\n","  def __len__(self):\n","    return len(self.labels)\n","\n","  def __getitem__(self, inds):\n","    # print('\\n####################\\n')\n","    # print('GET ITEM CALLED', 'INDS:', inds)\n","    # Sample neighborhood\n","\n","    # get inds in list\n","    inds = inds.tolist() if torch.is_tensor(inds) else (inds if isinstance(inds,list) else [inds])\n","\n","    # return the required inds\n","    sampled_indices = sample_neighborhood(self.A, inds, self.neighbor_max, self.branch_max,seed=self.seed)\n","\n","    # get the input for the required inds\n","    H_batch = self.H[sampled_indices]\n","\n","    # get the adjacency matrix for the required inds\n","    A_batch = [self.A[k][sampled_indices][:, sampled_indices] for k in range(len(self.A))]\n","\n","    # get the labels for the required inds\n","    labels_batch = self.labels[sampled_indices]\n","\n","    # get meta indices\n","    index_batch = self.meta_indices[sampled_indices]\n","\n","    return H_batch, A_batch, labels_batch, index_batch"],"metadata":{"id":"9Rag8OCtKo1r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["H = torch.stack(list(df_train['vanilla_embedding.1']))\n","labels = torch.LongTensor(list(df_train['chapter_idx']))\n","A = []\n","A.append(train_entities)\n","meta_indices = torch.LongTensor(list(range(df_train.shape[0])))"],"metadata":{"id":"NWXfTAo_PxGe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = GNNDataset(H, A, labels, meta_indices, neighbor_max=4, branch_max=10)\n","\n","# Prevent dataloader from calling a single index at a time\n","custom_sampler = torch.utils.data.sampler.BatchSampler(\n","    torch.utils.data.sampler.RandomSampler(train_dataset),\n","    batch_size=6,\n","    drop_last=False)\n","\n","\n","train_loader = DataLoader(train_dataset, sampler = custom_sampler)\n"],"metadata":{"id":"AxzIHGzKRr2V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check batches\n","\n","# Number of batches to inspect\n","num_batches_to_check = 2\n","\n","for batch_idx, (inputs, adjacency, labels, indices) in enumerate(train_loader):\n","    print('\\n##########################\\n')\n","    print(f\"Batch {batch_idx + 1}/{num_batches_to_check}:\")\n","    print('-' * 10)\n","    print(\"Inputs:\")\n","    print(f\"  Type: {type(inputs)}\")\n","    print(f\"  Shape: {inputs.size()}\")\n","    print('-' * 10)\n","    print(\"Adjacency:\")\n","    print(f\"  Type: {type(adjacency)}\")\n","    print(f\"  Shape: {adjacency[0].size()}\")\n","    print('-' * 10)\n","    print(\"Indices:\")\n","    print(f\"  Type: {type(indices)}\")\n","    print(f\"  Shape: {indices.size()}\")\n","    print(indices)\n","    print('-' * 10)\n","    print(\"Labels:\")\n","    print(f\"  Type: {type(labels)}\")\n","    print(f\"  Shape: {labels.size()}\")\n","    print(labels)\n","\n","    # Stop after inspecting the desired number of batches\n","    if batch_idx + 1 >= num_batches_to_check:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2MW9c2A1WE08","executionInfo":{"status":"ok","timestamp":1734532977373,"user_tz":-60,"elapsed":258,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"08506550-7497-4edb-9338-ebe55be83113"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","##########################\n","\n","Batch 1/2:\n","----------\n","Inputs:\n","  Type: <class 'torch.Tensor'>\n","  Shape: torch.Size([1, 26, 768])\n","----------\n","Adjacency:\n","  Type: <class 'list'>\n","  Shape: torch.Size([1, 26, 26])\n","----------\n","Indices:\n","  Type: <class 'torch.Tensor'>\n","  Shape: torch.Size([1, 26])\n","tensor([[ 9152,  4865,  8832,  2305,  9542, 10381,  8910,   146, 10964,  2389,\n","          4501,  3799,  3220,  2711,  9438,   607,   865,  1448,  8298,  2540,\n","          4271,  2101,  5814,   696,  1145,  3899]])\n","----------\n","Labels:\n","  Type: <class 'torch.Tensor'>\n","  Shape: torch.Size([1, 26])\n","tensor([[53, 17, 42, 12, 16,  5,  8, 47, 48, 57, 45, 54,  9, 15, 64, 12, 59, 11,\n","          5, 56, 58, 15, 47,  5,  5, 67]], device='cuda:0')\n","\n","##########################\n","\n","Batch 2/2:\n","----------\n","Inputs:\n","  Type: <class 'torch.Tensor'>\n","  Shape: torch.Size([1, 16, 768])\n","----------\n","Adjacency:\n","  Type: <class 'list'>\n","  Shape: torch.Size([1, 16, 16])\n","----------\n","Indices:\n","  Type: <class 'torch.Tensor'>\n","  Shape: torch.Size([1, 16])\n","tensor([[ 4098,   328, 11402, 10284,  4493,  1133,  9069, 11312,  2482,  6867,\n","          1939, 10997,  4567,  8058,  7003,  8028]])\n","----------\n","Labels:\n","  Type: <class 'torch.Tensor'>\n","  Shape: torch.Size([1, 16])\n","tensor([[62, 12, 17, 12, 65, 12, 12, 12, 53, 12,  1, 12, 63, 12, 12, 12]],\n","       device='cuda:0')\n"]}]},{"cell_type":"code","source":["H = torch.stack(list(df_train['vanilla_embedding.1']))\n","labels = torch.LongTensor(list(df_train['chapter_idx']))\n","A = []\n","A.append(train_entities)\n","meta_indices = torch.LongTensor(list(range(df_train.shape[0])))"],"metadata":{"id":"k5YldoFcXCHa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset_fixed = GNNDataset(H, A, labels, meta_indices, neighbor_max=8, batch_max=32, seed=42)\n","\n","# Prevent dataloader from calling a single index at a time\n","custom_validation_sampler = torch.utils.data.sampler.BatchSampler(\n","    torch.utils.data.sampler.SequentialSampler(train_dataset_fixed),\n","    batch_size=8,\n","    drop_last=False)\n","\n","\n","train_loader_fixed = DataLoader(train_dataset_fixed, sampler = custom_validation_sampler)"],"metadata":{"id":"dL7DV1_T9yZz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check batches\n","\n","# Number of batches to inspect\n","num_batches_to_check = 2\n","\n","for batch_idx, (inputs, adjacency, labels, indices) in enumerate(train_loader_fixed):\n","    print('\\n##########################\\n')\n","    print(f\"Batch {batch_idx + 1}/{num_batches_to_check}:\")\n","    print('-' * 10)\n","    print(\"Inputs:\")\n","    print(f\"  Type: {type(inputs)}\")\n","    print(f\"  Shape: {inputs.size()}\")\n","    print('-' * 10)\n","    print(\"Adjacency:\")\n","    print(f\"  Type: {type(adjacency)}\")\n","    print(f\"  Shape: {adjacency[0].size()}\")\n","    print('-' * 10)\n","    print(\"Indices:\")\n","    print(f\"  Type: {type(indices)}\")\n","    print(f\"  Shape: {indices.size()}\")\n","    print(indices)\n","    print('-' * 10)\n","    print(\"Labels:\")\n","    print(f\"  Type: {type(labels)}\")\n","    print(f\"  Shape: {labels.size()}\")\n","    print(labels)\n","\n","    # Stop after inspecting the desired number of batches\n","    if batch_idx + 1 >= num_batches_to_check:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PTddMbIyWSej","executionInfo":{"status":"ok","timestamp":1734101152688,"user_tz":-60,"elapsed":11,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"d9b61d66-64b8-472f-bf74-5cfc2446537a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","##########################\n","\n","Batch 1/2:\n","----------\n","Inputs:\n","  Type: <class 'torch.Tensor'>\n","  Shape: torch.Size([1, 32, 768])\n","----------\n","Adjacency:\n","  Type: <class 'list'>\n","  Shape: torch.Size([1, 32, 32])\n","----------\n","Indices:\n","  Type: <class 'torch.Tensor'>\n","  Shape: torch.Size([1, 32])\n","tensor([[    0,     1,     2,     3,     4,     5,     6,     7,  5446,  9222,\n","          2250,   583,  9547,  7760,  6290,  8020,  8983,  5533,  6623,   993,\n","          8424,  8616,  9900,  1900,  9773,  2668,   817,  3572,  7989,  9849,\n","         10620,  8189]])\n","----------\n","Labels:\n","  Type: <class 'torch.Tensor'>\n","  Shape: torch.Size([1, 32])\n","tensor([[31, 15, 15, 59, 62, 63, 17, 15, 31, 15, 31, 16, 16, 31, 59, 56, 16, 31,\n","         50, 31, 31,  5, 14, 59, 59, 57, 31, 58,  7, 31,  9, 59]],\n","       device='cuda:0')\n","\n","##########################\n","\n","Batch 2/2:\n","----------\n","Inputs:\n","  Type: <class 'torch.Tensor'>\n","  Shape: torch.Size([1, 32, 768])\n","----------\n","Adjacency:\n","  Type: <class 'list'>\n","  Shape: torch.Size([1, 32, 32])\n","----------\n","Indices:\n","  Type: <class 'torch.Tensor'>\n","  Shape: torch.Size([1, 32])\n","tensor([[    8,     9,    10,    11,    12,    13,    14,    15,  2312,  1419,\n","          6414, 10634,  5140,  3349,  6934,  6039,  3864,  6425,  3353,  6935,\n","          7714,  1058,  1444,  9768, 10770,  6703,  4271,  4919,  7223,  7611,\n","           828, 11327]])\n","----------\n","Labels:\n","  Type: <class 'torch.Tensor'>\n","  Shape: torch.Size([1, 32])\n","tensor([[ 8,  7, 12, 46, 16, 10, 53, 14,  8, 56, 49, 48, 52, 10, 59, 10, 46, 17,\n","         44, 14,  7, 14, 56,  8,  6, 17, 58, 10,  4,  5,  9, 56]],\n","       device='cuda:0')\n"]}]},{"cell_type":"markdown","source":["The dataloader seems to be working correctly, the implementation of custom sampling on all indices at once leads to DataLoaders collate function inserting a new dimension that it will stach against. Because all indices are dealt with at once, there is no stacking.\n","\n","The simple solution will be to simply squeeze the tensors in the training loop. The validation loader eradicates randomness from the process."],"metadata":{"id":"6H8779aHR9wL"}},{"cell_type":"markdown","source":["## $\\color{blue}{Model:}$"],"metadata":{"id":"WrZ8xkIsSnvw"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class GNNLayer(nn.Module):\n","    def __init__(self, in_features, out_features, num_relations=1, dropout=0.3):\n","        super(GNNLayer, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.num_relations = num_relations\n","        self.dropout = dropout\n","\n","        self.T = nn.ParameterList([nn.Parameter(torch.Tensor(in_features, out_features)) for _ in range(num_relations)])\n","        self.E = nn.ParameterList([nn.Parameter(torch.Tensor(in_features, out_features)) for _ in range(num_relations)])\n","\n","        # Batch normalization\n","        self.batch_norm = nn.BatchNorm1d(out_features)\n","\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        for t in self.T:\n","            nn.init.xavier_uniform_(t)\n","        for e in self.E:\n","            nn.init.xavier_uniform_(e)\n","\n","    def forward(self, H, A):\n","        H_out = torch.zeros_like(H)\n","        for k in range(self.num_relations):\n","            messages_projection = A[k].T @ H @ self.E[k]\n","            degrees = A[k].sum(dim=1, keepdim=True)\n","            degrees[degrees == 0] = 1.0\n","            messages_projection /= degrees\n","\n","            self_projection = H @ self.T[k]\n","\n","            # Include skip connection\n","            H_out += F.leaky_relu(self_projection + messages_projection) + H\n","\n","        # Apply batch normalization\n","        H_out = self.batch_norm(H_out)\n","\n","        # Apply dropout\n","        H_out = F.dropout(H_out, p=self.dropout, training=self.training)\n","\n","        return H_out\n","\n","class GNNModel(nn.Module):\n","    def __init__(self, d, h, c, num_relations=1, num_layers=3, dropout=0.3):\n","        super(GNNModel, self).__init__()\n","        self.num_layers = num_layers\n","        self.gnn_layers = nn.ModuleList([GNNLayer(d, d, num_relations, dropout) for _ in range(num_layers)])\n","        self.fc1 = nn.Linear(d, h)\n","        self.batch_norm_fc1 = nn.BatchNorm1d(h)\n","        self.fc2 = nn.Linear(h, c)\n","        self.dropout = dropout\n","\n","    def forward(self, H, A):\n","        for layer in self.gnn_layers:\n","            H = layer(H, A)\n","\n","        H = F.relu(self.batch_norm_fc1(self.fc1(H)))\n","        H = F.dropout(H, p=self.dropout, training=self.training)\n","        Output = self.fc2(H)\n","        return Output\n","\n","    def forward_layer(self, H, A, layer_idx):\n","        \"\"\"Forward pass for a specific layer.\"\"\"\n","        H = self.gnn_layers[layer_idx](H, A)\n","        return H\n"],"metadata":{"id":"64sKwqPvPXmM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GNNModel(nn.Module):\n","   def __init__(self, d, h, c, num_relations=1, num_layers=2):\n","      super(GNNModel, self).__init__()\n","      self.num_layers = num_layers\n","      self.gnn_layers = nn.ModuleList([GNNLayer(d, d) for _ in range(num_layers)])\n","      self.fc1 = nn.Linear(d, h)\n","      self.fc2 = nn.Linear(h, c)\n","\n","   def forward(self, H, A):\n","      for layer in self.gnn_layers:\n","         H = layer(H, A)\n","      # Classification\n","      H = F.relu(self.fc1(H))\n","      Output = self.fc2(H)\n","      return Output"],"metadata":{"id":"yb0uyLqeVPc3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.optim as optim\n","\n","d = 768\n","h = 400   # hidden dimension of fully connected layer\n","c = 70   # number of classes\n","num_relations = 1   # number of relationship types\n","\n","# Model, Loss, Optimizer\n","model = GNNModel(d, h, c, num_relations)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n"],"metadata":{"id":"NP8Q4qpiVp-1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def count_parameters_per_module(model):\n","    print(\"Module and parameter counts:\")\n","\n","    for name, module in model.named_modules():\n","        # Skip the top-level module (the model itself)\n","        if not isinstance(module, nn.Module) or name == \"\":\n","            continue\n","\n","        param_count = sum(p.numel() for p in module.parameters() if p.requires_grad)\n","\n","        if param_count > 0:  # Only print modules that have parameters\n","            print(f\"{name}: {param_count} parameters\")"],"metadata":{"id":"2zm1UjLWZUBB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count_parameters_per_module(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5LxQpA1fYB5e","executionInfo":{"status":"ok","timestamp":1734533137190,"user_tz":-60,"elapsed":290,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"74e0e83e-967e-4fa8-dec2-139f209edb77"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Module and parameter counts:\n","gnn_layers: 2362368 parameters\n","gnn_layers.0: 1181184 parameters\n","gnn_layers.0.T: 589824 parameters\n","gnn_layers.0.E: 589824 parameters\n","gnn_layers.0.batch_norm: 1536 parameters\n","gnn_layers.1: 1181184 parameters\n","gnn_layers.1.T: 589824 parameters\n","gnn_layers.1.E: 589824 parameters\n","gnn_layers.1.batch_norm: 1536 parameters\n","fc1: 307600 parameters\n","fc2: 28070 parameters\n"]}]},{"cell_type":"markdown","source":["## $\\color{blue}{Train-Validate:}$"],"metadata":{"id":"5so7JQvHdAvF"}},{"cell_type":"code","source":["def accuracy(outputs, labels):\n","    # argmax to get predicted classes\n","    _, predicted = torch.max(outputs, 1)\n","\n","    # count correct\n","    correct = (predicted == labels).sum().item()\n","\n","    # get average\n","    acc = correct / labels.size(0)  # Total number of samples\n","    return acc"],"metadata":{"id":"EZhnvYLtWbqk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","def train(model, train_loader, criterion, optimizer):\n","    model.train()\n","    epoch_train_losses = []\n","    epoch_train_accuracy = []\n","\n","    for batch_idx, (H, A, y, indices) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","\n","        H = H.squeeze(0)\n","        A = [a.squeeze(0) for a in A]\n","        y = y.squeeze(0)\n","\n","        out = model(H,A)\n","        train_loss = criterion(out, y)\n","        train_accuracy = accuracy(out, y)\n","\n","\n","        epoch_train_losses.append(train_loss.item())\n","        epoch_train_accuracy.append(train_accuracy)\n","\n","        # Backpropagation and optimization\n","        train_loss.backward()\n","        optimizer.step()\n","\n","    return np.mean(epoch_train_losses), np.mean(epoch_train_accuracy)"],"metadata":{"id":"EftvJs1pdDlQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def validate(model, dev_loader, criterion, threshold=12000):\n","    model.eval()\n","    epoch_dev_losses = []\n","    epoch_dev_accuracy = []\n","    pred_holder = []\n","    real_holder = []\n","\n","    with torch.no_grad():\n","        for batch_idx, (H, A, y, indices) in enumerate(dev_loader):\n","            H = H.squeeze(0)\n","            A = [a.squeeze(0) for a in A]\n","            y = y.squeeze(0)\n","            indices = indices.squeeze(0)\n","\n","            out = model(H, A)\n","\n","            # Filter out training points\n","            mask = indices >= threshold\n","            filtered_out = out[mask]\n","            filtered_y = y[mask]\n","\n","            # Calculate loss and accuracy only on filtered outputs\n","            if filtered_out.size(0) > 0:  # Ensure there are samples to evaluate\n","                dev_loss = criterion(filtered_out, filtered_y)\n","                dev_accuracy = accuracy(filtered_out, filtered_y)\n","\n","                epoch_dev_losses.append(dev_loss.item())\n","                epoch_dev_accuracy.append(dev_accuracy)\n","\n","    # Avoid division by zero if no validation points were processed\n","    return np.mean(epoch_dev_losses), np.mean(epoch_dev_accuracy)"],"metadata":{"id":"FEfSsAmimuEU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## $\\color{blue}{Training:}$"],"metadata":{"id":"dyzhkkmcR29K"}},{"cell_type":"code","source":["from collections import namedtuple\n","Stats = namedtuple('Stats', [\n","    'train_loss',\n","    'train_accuracy',\n","    'dev_loss',\n","    'dev_accuracy',\n","    'epoch',\n","    'lr',\n","    'alpha',\n","    'max_accuracy'\n","])"],"metadata":{"id":"8gM37WLCNRJ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def gen_config(lr_low, lr_high, alpha_low, alpha_high):\n","  np.random.seed()\n","  lr = round(10**float(np.random.uniform(lr_low,lr_high)),6)\n","  alpha = round(10**float(np.random.uniform(alpha_low,alpha_high)),6)\n","  return lr, alpha"],"metadata":{"id":"E_DaEjRlNXl7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def gen_ranges( lr, lr_range, alpha, alpha_range):\n","\n","  lr_center = lr\n","  lr_low = lr_center - lr_range/2\n","  lr_high = lr_center + lr_range/2\n","  lr_diff = lr_high - lr_low\n","\n","  alpha_center = alpha\n","  alpha_low = alpha_center - alpha_range/2\n","  alpha_high = alpha_center + alpha_range/2\n","  alpha_diff = alpha_high - alpha_low\n","\n","  return (lr_low, lr_high, alpha_low, alpha_high)"],"metadata":{"id":"4V9JC3PUNbpf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def search_stats(results):\n","  best_stats = None\n","  max_dev_accuracy = 0\n","  for i in range(len(results)):\n","    acc = results[i].dev_accuracy\n","    if acc > max_dev_accuracy:\n","      best_stats = results[i]\n","      max_dev_accuracy = acc\n","  return best_stats"],"metadata":{"id":"-sNDLKonNj_Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tv_run(epochs, model, lr, alpha, max_accuracy, path, verbose = 0):\n","  \"\"\"\n","  Runs a training setup\n","  verbose == 1 - print model results\n","  verbose == 2 -> print epoch and model results\n","  \"\"\"\n","  model = model.to(device)\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=alpha)\n","\n","  # Prepare data loaders\n","  train_loader = DataLoader(train_dataset, sampler = custom_train_sampler)\n","  dev_loader = DataLoader(validation_dataset, sampler = custom_validation_sampler)\n","\n","  # Hold epoch stats\n","  train_losses = []\n","  train_accuracy = []\n","  dev_losses = []\n","  dev_accuracy = []\n","  epoch_holder = []\n","\n","  # Break if no improvement\n","  current_best = 0\n","  no_improvement = 0\n","\n","\n","  # Run epochs\n","  for epoch in range(epochs):\n","\n","    # break out of epochs\n","    if no_improvement >= 6:\n","      break\n","\n","    # call training and validation functions\n","    train_loss, train_acc = train(model, train_loader, criterion, optimizer)\n","    dev_loss, dev_acc = validate(model, dev_loader, criterion)\n","\n","    # Store epoch stats\n","    train_losses.append(train_loss)\n","    train_accuracy.append(train_acc)\n","    dev_losses.append(dev_loss)\n","    dev_accuracy.append(dev_acc)\n","    epoch_holder.append(epoch + 1)\n","\n","    # check for improvement\n","    if dev_acc > current_best:\n","      current_best = dev_acc\n","      no_improvement = 0\n","    else:\n","      no_improvement += 1\n","\n","    # save best model\n","    if dev_acc > max_accuracy:\n","      torch.save(model.state_dict(), path)\n","      max_accuracy = dev_acc\n","\n","\n","    # optionally print epoch results\n","    if verbose == 2:\n","      print(f'\\n --------- \\nEpoch: {epoch + 1}\\n')\n","      print(f'Epoch {epoch + 1} train loss: {train_loss:.4f}')\n","      print(f'Epoch {epoch + 1} train accuracy: {train_acc:.4f}')\n","      print(f'Epoch {epoch + 1} dev loss: {dev_loss:.4f}')\n","      print(f'Epoch {epoch + 1} dev accuracy: {dev_acc:.4f}')\n","\n","      # save best results\n","  max_ind = np.argmax(dev_accuracy)\n","\n","  stats = Stats(\n","      train_losses[max_ind],\n","      train_accuracy[max_ind],\n","      dev_losses[max_ind],\n","      dev_accuracy[max_ind],\n","      epoch_holder[max_ind],\n","      lr, alpha,\n","      max_accuracy\n","  )\n","\n","  # optionally print model results\n","  if verbose in [1,2]:\n","    print('\\n ######## \\n')\n","    print(f'lr:{stats.lr}, alpha:{stats.alpha} @ epoch {stats.epoch}.')\n","    print(f'TL:{stats.train_loss}, TA:{stats.train_accuracy}.')\n","    print(f'DL:{stats.dev_loss}, DA:{stats.dev_accuracy}')\n","\n","  return stats"],"metadata":{"id":"J3PQBpDIYnib"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### $\\color{red}{Sanity-check:}$"],"metadata":{"id":"1C-wVpsPUasU"}},{"cell_type":"code","source":["# model\n","model = GNNModel(d, h, c, num_relations)"],"metadata":{"id":"h9dVcM1VaN6F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# training loader\n","H_train = torch.stack(list(df_train['vanilla_embedding.1']))\n","labels_train = torch.LongTensor(list(df_train['chapter_idx']))\n","A_train = []\n","A_train.append(train_entities)\n","train_indices = torch.LongTensor(list(range(df_train.shape[0])))\n","\n","train_dataset = GNNDataset(H_train, A_train, labels_train, train_indices, neighbor_max=4, branch_max=16)\n","\n","# Prevent dataloader from calling a single index at a time\n","custom_train_sampler = torch.utils.data.sampler.BatchSampler(\n","    torch.utils.data.sampler.RandomSampler(train_dataset),\n","    batch_size=8,\n","    drop_last=False)\n","\n","\n","train_loader = DataLoader(train_dataset, sampler = custom_train_sampler)\n","\n","# training loader\n","df1 = df_train[['vanilla_embedding.1', 'chapter_idx']]\n","df2 = df_dev[['vanilla_embedding.1', 'chapter_idx']]\n","df_val = pd.concat([df1, df2])\n","H_val = torch.stack(list(df_val['vanilla_embedding.1']))\n","labels_val = torch.LongTensor(list(df_val['chapter_idx']))\n","A_val = []\n","A_val.append(val_entities)\n","val_indices = torch.LongTensor(list(range(df_val.shape[0])))\n","\n","\n","\n","validation_dataset = GNNDataset(H_val, A_val, labels_val, val_indices, neighbor_max=4, branch_max=16, seed=42)\n","\n","# Prevent dataloader from calling a single index at a time\n","custom_validation_sampler = torch.utils.data.sampler.BatchSampler(\n","    torch.utils.data.sampler.SequentialSampler(validation_dataset),\n","    batch_size=8,\n","    drop_last=False)\n","\n","\n","dev_loader = DataLoader(validation_dataset, sampler = custom_validation_sampler)\n","\n"],"metadata":{"id":"9pxw5ELFaN8t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 2\n","lr = 0.0005\n","alpha = 0.0001\n","path = \"class/models/GNN.2.pt\"\n","max_accuracy = 0"],"metadata":{"id":"Qf0cxrsteYzM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tv_run(epochs, model, lr, alpha, max_accuracy, path, verbose = 2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RPYCNxFAUZPu","executionInfo":{"status":"ok","timestamp":1734534441361,"user_tz":-60,"elapsed":481660,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"3c1f2382-fb5d-49f6-efc5-e80a272bc0f7","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","8\n","16\n","24\n","32\n","40\n","48\n","56\n","64\n","72\n","80\n","88\n","96\n","104\n","112\n","120\n","128\n","136\n","144\n","152\n","160\n","168\n","176\n","184\n","192\n","200\n","208\n","216\n","224\n","232\n","240\n","248\n","256\n","264\n","272\n","280\n","288\n","296\n","304\n","312\n","320\n","328\n","336\n","344\n","352\n","360\n","368\n","376\n","384\n","392\n","400\n","408\n","416\n","424\n","432\n","440\n","448\n","456\n","464\n","472\n","480\n","488\n","496\n","504\n","512\n","520\n","528\n","536\n","544\n","552\n","560\n","568\n","576\n","584\n","592\n","600\n","608\n","616\n","624\n","632\n","640\n","648\n","656\n","664\n","672\n","680\n","688\n","696\n","704\n","712\n","720\n","728\n","736\n","744\n","752\n","760\n","768\n","776\n","784\n","792\n","800\n","808\n","816\n","824\n","832\n","840\n","848\n","856\n","864\n","872\n","880\n","888\n","896\n","904\n","912\n","920\n","928\n","936\n","944\n","952\n","960\n","968\n","976\n","984\n","992\n","1000\n","1008\n","1016\n","1024\n","1032\n","1040\n","1048\n","1056\n","1064\n","1072\n","1080\n","1088\n","1096\n","1104\n","1112\n","1120\n","1128\n","1136\n","1144\n","1152\n","1160\n","1168\n","1176\n","1184\n","1192\n","1200\n","1208\n","1216\n","1224\n","1232\n","1240\n","1248\n","1256\n","1264\n","1272\n","1280\n","1288\n","1296\n","1304\n","1312\n","1320\n","1328\n","1336\n","1344\n","1352\n","1360\n","1368\n","1376\n","1384\n","1392\n","1400\n","1408\n","1416\n","1424\n","1432\n","1440\n","1448\n","1456\n","1464\n","1472\n","1480\n","1488\n","1496\n","1504\n","1512\n","1520\n","1528\n","1536\n","1544\n","1552\n","1560\n","1568\n","1576\n","1584\n","1592\n","1600\n","1608\n","1616\n","1624\n","1632\n","1640\n","1648\n","1656\n","1664\n","1672\n","1680\n","1688\n","1696\n","1704\n","1712\n","1720\n","1728\n","1736\n","1744\n","1752\n","1760\n","1768\n","1776\n","1784\n","1792\n","1800\n","1808\n","1816\n","1824\n","1832\n","1840\n","1848\n","1856\n","1864\n","1872\n","1880\n","1888\n","1896\n","1904\n","1912\n","1920\n","1928\n","1936\n","1944\n","1952\n","1960\n","1968\n","1976\n","1984\n","1992\n","2000\n","2008\n","2016\n","2024\n","2032\n","2040\n","2048\n","2056\n","2064\n","2072\n","2080\n","2088\n","2096\n","2104\n","2112\n","2120\n","2128\n","2136\n","2144\n","2152\n","2160\n","2168\n","2176\n","2184\n","2192\n","2200\n","2208\n","2216\n","2224\n","2232\n","2240\n","2248\n","2256\n","2264\n","2272\n","2280\n","2288\n","2296\n","2304\n","2312\n","2320\n","2328\n","2336\n","2344\n","2352\n","2360\n","2368\n","2376\n","2384\n","2392\n","2400\n","2408\n","2416\n","2424\n","2432\n","2440\n","2448\n","2456\n","2464\n","2472\n","2480\n","2488\n","2496\n","2504\n","2512\n","2520\n","2528\n","2536\n","2544\n","2552\n","2560\n","2568\n","2576\n","2584\n","2592\n","2600\n","2608\n","2616\n","2624\n","2632\n","2640\n","2648\n","2656\n","2664\n","2672\n","2680\n","2688\n","2696\n","2704\n","2712\n","2720\n","2728\n","2736\n","2744\n","2752\n","2760\n","2768\n","2776\n","2784\n","2792\n","2800\n","2808\n","2816\n","2824\n","2832\n","2840\n","2848\n","2856\n","2864\n","2872\n","2880\n","2888\n","2896\n","2904\n","2912\n","2920\n","2928\n","2936\n","2944\n","2952\n","2960\n","2968\n","2976\n","2984\n","2992\n","3000\n","3008\n","3016\n","3024\n","3032\n","3040\n","3048\n","3056\n","3064\n","3072\n","3080\n","3088\n","3096\n","3104\n","3112\n","3120\n","3128\n","3136\n","3144\n","3152\n","3160\n","3168\n","3176\n","3184\n","3192\n","3200\n","3208\n","3216\n","3224\n","3232\n","3240\n","3248\n","3256\n","3264\n","3272\n","3280\n","3288\n","3296\n","3304\n","3312\n","3320\n","3328\n","3336\n","3344\n","3352\n","3360\n","3368\n","3376\n","3384\n","3392\n","3400\n","3408\n","3416\n","3424\n","3432\n","3440\n","3448\n","3456\n","3464\n","3472\n","3480\n","3488\n","3496\n","3504\n","3512\n","3520\n","3528\n","3536\n","3544\n","3552\n","3560\n","3568\n","3576\n","3584\n","3592\n","3600\n","3608\n","3616\n","3624\n","3632\n","3640\n","3648\n","3656\n","3664\n","3672\n","3680\n","3688\n","3696\n","3704\n","3712\n","3720\n","3728\n","3736\n","3744\n","3752\n","3760\n","3768\n","3776\n","3784\n","3792\n","3800\n","3808\n","3816\n","3824\n","3832\n","3840\n","3848\n","3856\n","3864\n","3872\n","3880\n","3888\n","3896\n","3904\n","3912\n","3920\n","3928\n","3936\n","3944\n","3952\n","3960\n","3968\n","3976\n","3984\n","3992\n","4000\n","4008\n","4016\n","4024\n","4032\n","4040\n","4048\n","4056\n","4064\n","4072\n","4080\n","4088\n","4096\n","4104\n","4112\n","4120\n","4128\n","4136\n","4144\n","4152\n","4160\n","4168\n","4176\n","4184\n","4192\n","4200\n","4208\n","4216\n","4224\n","4232\n","4240\n","4248\n","4256\n","4264\n","4272\n","4280\n","4288\n","4296\n","4304\n","4312\n","4320\n","4328\n","4336\n","4344\n","4352\n","4360\n","4368\n","4376\n","4384\n","4392\n","4400\n","4408\n","4416\n","4424\n","4432\n","4440\n","4448\n","4456\n","4464\n","4472\n","4480\n","4488\n","4496\n","4504\n","4512\n","4520\n","4528\n","4536\n","4544\n","4552\n","4560\n","4568\n","4576\n","4584\n","4592\n","4600\n","4608\n","4616\n","4624\n","4632\n","4640\n","4648\n","4656\n","4664\n","4672\n","4680\n","4688\n","4696\n","4704\n","4712\n","4720\n","4728\n","4736\n","4744\n","4752\n","4760\n","4768\n","4776\n","4784\n","4792\n","4800\n","4808\n","4816\n","4824\n","4832\n","4840\n","4848\n","4856\n","4864\n","4872\n","4880\n","4888\n","4896\n","4904\n","4912\n","4920\n","4928\n","4936\n","4944\n","4952\n","4960\n","4968\n","4976\n","4984\n","4992\n","5000\n","5008\n","5016\n","5024\n","5032\n","5040\n","5048\n","5056\n","5064\n","5072\n","5080\n","5088\n","5096\n","5104\n","5112\n","5120\n","5128\n","5136\n","5144\n","5152\n","5160\n","5168\n","5176\n","5184\n","5192\n","5200\n","5208\n","5216\n","5224\n","5232\n","5240\n","5248\n","5256\n","5264\n","5272\n","5280\n","5288\n","5296\n","5304\n","5312\n","5320\n","5328\n","5336\n","5344\n","5352\n","5360\n","5368\n","5376\n","5384\n","5392\n","5400\n","5408\n","5416\n","5424\n","5432\n","5440\n","5448\n","5456\n","5464\n","5472\n","5480\n","5488\n","5496\n","5504\n","5512\n","5520\n","5528\n","5536\n","5544\n","5552\n","5560\n","5568\n","5576\n","5584\n","5592\n","5600\n","5608\n","5616\n","5624\n","5632\n","5640\n","5648\n","5656\n","5664\n","5672\n","5680\n","5688\n","5696\n","5704\n","5712\n","5720\n","5728\n","5736\n","5744\n","5752\n","5760\n","5768\n","5776\n","5784\n","5792\n","5800\n","5808\n","5816\n","5824\n","5832\n","5840\n","5848\n","5856\n","5864\n","5872\n","5880\n","5888\n","5896\n","5904\n","5912\n","5920\n","5928\n","5936\n","5944\n","5952\n","5960\n","5968\n","5976\n","5984\n","5992\n","6000\n","6008\n","6016\n","6024\n","6032\n","6040\n","6048\n","6056\n","6064\n","6072\n","6080\n","6088\n","6096\n","6104\n","6112\n","6120\n","6128\n","6136\n","6144\n","6152\n","6160\n","6168\n","6176\n","6184\n","6192\n","6200\n","6208\n","6216\n","6224\n","6232\n","6240\n","6248\n","6256\n","6264\n","6272\n","6280\n","6288\n","6296\n","6304\n","6312\n","6320\n","6328\n","6336\n","6344\n","6352\n","6360\n","6368\n","6376\n","6384\n","6392\n","6400\n","6408\n","6416\n","6424\n","6432\n","6440\n","6448\n","6456\n","6464\n","6472\n","6480\n","6488\n","6496\n","6504\n","6512\n","6520\n","6528\n","6536\n","6544\n","6552\n","6560\n","6568\n","6576\n","6584\n","6592\n","6600\n","6608\n","6616\n","6624\n","6632\n","6640\n","6648\n","6656\n","6664\n","6672\n","6680\n","6688\n","6696\n","6704\n","6712\n","6720\n","6728\n","6736\n","6744\n","6752\n","6760\n","6768\n","6776\n","6784\n","6792\n","6800\n","6808\n","6816\n","6824\n","6832\n","6840\n","6848\n","6856\n","6864\n","6872\n","6880\n","6888\n","6896\n","6904\n","6912\n","6920\n","6928\n","6936\n","6944\n","6952\n","6960\n","6968\n","6976\n","6984\n","6992\n","7000\n","7008\n","7016\n","7024\n","7032\n","7040\n","7048\n","7056\n","7064\n","7072\n","7080\n","7088\n","7096\n","7104\n","7112\n","7120\n","7128\n","7136\n","7144\n","7152\n","7160\n","7168\n","7176\n","7184\n","7192\n","7200\n","7208\n","7216\n","7224\n","7232\n","7240\n","7248\n","7256\n","7264\n","7272\n","7280\n","7288\n","7296\n","7304\n","7312\n","7320\n","7328\n","7336\n","7344\n","7352\n","7360\n","7368\n","7376\n","7384\n","7392\n","7400\n","7408\n","7416\n","7424\n","7432\n","7440\n","7448\n","7456\n","7464\n","7472\n","7480\n","7488\n","7496\n","7504\n","7512\n","7520\n","7528\n","7536\n","7544\n","7552\n","7560\n","7568\n","7576\n","7584\n","7592\n","7600\n","7608\n","7616\n","7624\n","7632\n","7640\n","7648\n","7656\n","7664\n","7672\n","7680\n","7688\n","7696\n","7704\n","7712\n","7720\n","7728\n","7736\n","7744\n","7752\n","7760\n","7768\n","7776\n","7784\n","7792\n","7800\n","7808\n","7816\n","7824\n","7832\n","7840\n","7848\n","7856\n","7864\n","7872\n","7880\n","7888\n","7896\n","7904\n","7912\n","7920\n","7928\n","7936\n","7944\n","7952\n","7960\n","7968\n","7976\n","7984\n","7992\n","8000\n","8008\n","8016\n","8024\n","8032\n","8040\n","8048\n","8056\n","8064\n","8072\n","8080\n","8088\n","8096\n","8104\n","8112\n","8120\n","8128\n","8136\n","8144\n","8152\n","8160\n","8168\n","8176\n","8184\n","8192\n","8200\n","8208\n","8216\n","8224\n","8232\n","8240\n","8248\n","8256\n","8264\n","8272\n","8280\n","8288\n","8296\n","8304\n","8312\n","8320\n","8328\n","8336\n","8344\n","8352\n","8360\n","8368\n","8376\n","8384\n","8392\n","8400\n","8408\n","8416\n","8424\n","8432\n","8440\n","8448\n","8456\n","8464\n","8472\n","8480\n","8488\n","8496\n","8504\n","8512\n","8520\n","8528\n","8536\n","8544\n","8552\n","8560\n","8568\n","8576\n","8584\n","8592\n","8600\n","8608\n","8616\n","8624\n","8632\n","8640\n","8648\n","8656\n","8664\n","8672\n","8680\n","8688\n","8696\n","8704\n","8712\n","8720\n","8728\n","8736\n","8744\n","8752\n","8760\n","8768\n","8776\n","8784\n","8792\n","8800\n","8808\n","8816\n","8824\n","8832\n","8840\n","8848\n","8856\n","8864\n","8872\n","8880\n","8888\n","8896\n","8904\n","8912\n","8920\n","8928\n","8936\n","8944\n","8952\n","8960\n","8968\n","8976\n","8984\n","8992\n","9000\n","9008\n","9016\n","9024\n","9032\n","9040\n","9048\n","9056\n","9064\n","9072\n","9080\n","9088\n","9096\n","9104\n","9112\n","9120\n","9128\n","9136\n","9144\n","9152\n","9160\n","9168\n","9176\n","9184\n","9192\n","9200\n","9208\n","9216\n","9224\n","9232\n","9240\n","9248\n","9256\n","9264\n","9272\n","9280\n","9288\n","9296\n","9304\n","9312\n","9320\n","9328\n","9336\n","9344\n","9352\n","9360\n","9368\n","9376\n","9384\n","9392\n","9400\n","9408\n","9416\n","9424\n","9432\n","9440\n","9448\n","9456\n","9464\n","9472\n","9480\n","9488\n","9496\n","9504\n","9512\n","9520\n","9528\n","9536\n","9544\n","9552\n","9560\n","9568\n","9576\n","9584\n","9592\n","9600\n","9608\n","9616\n","9624\n","9632\n","9640\n","9648\n","9656\n","9664\n","9672\n","9680\n","9688\n","9696\n","9704\n","9712\n","9720\n","9728\n","9736\n","9744\n","9752\n","9760\n","9768\n","9776\n","9784\n","9792\n","9800\n","9808\n","9816\n","9824\n","9832\n","9840\n","9848\n","9856\n","9864\n","9872\n","9880\n","9888\n","9896\n","9904\n","9912\n","9920\n","9928\n","9936\n","9944\n","9952\n","9960\n","9968\n","9976\n","9984\n","9992\n","10000\n","10008\n","10016\n","10024\n","10032\n","10040\n","10048\n","10056\n","10064\n","10072\n","10080\n","10088\n","10096\n","10104\n","10112\n","10120\n","10128\n","10136\n","10144\n","10152\n","10160\n","10168\n","10176\n","10184\n","10192\n","10200\n","10208\n","10216\n","10224\n","10232\n","10240\n","10248\n","10256\n","10264\n","10272\n","10280\n","10288\n","10296\n","10304\n","10312\n","10320\n","10328\n","10336\n","10344\n","10352\n","10360\n","10368\n","10376\n","10384\n","10392\n","10400\n","10408\n","10416\n","10424\n","10432\n","10440\n","10448\n","10456\n","10464\n","10472\n","10480\n","10488\n","10496\n","10504\n","10512\n","10520\n","10528\n","10536\n","10544\n","10552\n","10560\n","10568\n","10576\n","10584\n","10592\n","10600\n","10608\n","10616\n","10624\n","10632\n","10640\n","10648\n","10656\n","10664\n","10672\n","10680\n","10688\n","10696\n","10704\n","10712\n","10720\n","10728\n","10736\n","10744\n","10752\n","10760\n","10768\n","10776\n","10784\n","10792\n","10800\n","10808\n","10816\n","10824\n","10832\n","10840\n","10848\n","10856\n","10864\n","10872\n","10880\n","10888\n","10896\n","10904\n","10912\n","10920\n","10928\n","10936\n","10944\n","10952\n","10960\n","10968\n","10976\n","10984\n","10992\n","11000\n","11008\n","11016\n","11024\n","11032\n","11040\n","11048\n","11056\n","11064\n","11072\n","11080\n","11088\n","11096\n","11104\n","11112\n","11120\n","11128\n","11136\n","11144\n","11152\n","11160\n","11168\n","11176\n","11184\n","11192\n","11200\n","11208\n","11216\n","11224\n","11232\n","11240\n","11248\n","11256\n","11264\n","11272\n","11280\n","11288\n","11296\n","11304\n","11312\n","11320\n","11328\n","11336\n","11344\n","11352\n","11360\n","11368\n","11376\n","11384\n","11392\n","11400\n","11408\n","11416\n","11424\n","11432\n","11440\n","11448\n","11456\n","11464\n","11472\n","11480\n","11488\n","11496\n","11504\n","11512\n","11520\n","11528\n","11536\n","11544\n","11552\n","11560\n","11568\n","11576\n","11584\n","11592\n","11600\n","11608\n","11616\n","11624\n","11632\n","11640\n","11648\n","11656\n","11664\n","11672\n","11680\n","11688\n","11696\n","11704\n","11712\n","11720\n","11728\n","11736\n","11744\n","11752\n","11760\n","11768\n","11776\n","11784\n","11792\n","11800\n","11808\n","11816\n","11824\n","11832\n","11840\n","11848\n","11856\n","11864\n","11872\n","11880\n","11888\n","11896\n","11904\n","11912\n","11920\n","11928\n","11936\n","11944\n","11952\n","11960\n","11968\n","11976\n","11984\n","11992\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 0.3518\n","Epoch 1 train accuracy: 0.9056\n","Epoch 1 dev loss: 1.4561\n","Epoch 1 dev accuracy: 0.6748\n","0\n","8\n","16\n","24\n","32\n","40\n","48\n","56\n","64\n","72\n","80\n","88\n","96\n","104\n","112\n","120\n","128\n","136\n","144\n","152\n","160\n","168\n","176\n","184\n","192\n","200\n","208\n","216\n","224\n","232\n","240\n","248\n","256\n","264\n","272\n","280\n","288\n","296\n","304\n","312\n","320\n","328\n","336\n","344\n","352\n","360\n","368\n","376\n","384\n","392\n","400\n","408\n","416\n","424\n","432\n","440\n","448\n","456\n","464\n","472\n","480\n","488\n","496\n","504\n","512\n","520\n","528\n","536\n","544\n","552\n","560\n","568\n","576\n","584\n","592\n","600\n","608\n","616\n","624\n","632\n","640\n","648\n","656\n","664\n","672\n","680\n","688\n","696\n","704\n","712\n","720\n","728\n","736\n","744\n","752\n","760\n","768\n","776\n","784\n","792\n","800\n","808\n","816\n","824\n","832\n","840\n","848\n","856\n","864\n","872\n","880\n","888\n","896\n","904\n","912\n","920\n","928\n","936\n","944\n","952\n","960\n","968\n","976\n","984\n","992\n","1000\n","1008\n","1016\n","1024\n","1032\n","1040\n","1048\n","1056\n","1064\n","1072\n","1080\n","1088\n","1096\n","1104\n","1112\n","1120\n","1128\n","1136\n","1144\n","1152\n","1160\n","1168\n","1176\n","1184\n","1192\n","1200\n","1208\n","1216\n","1224\n","1232\n","1240\n","1248\n","1256\n","1264\n","1272\n","1280\n","1288\n","1296\n","1304\n","1312\n","1320\n","1328\n","1336\n","1344\n","1352\n","1360\n","1368\n","1376\n","1384\n","1392\n","1400\n","1408\n","1416\n","1424\n","1432\n","1440\n","1448\n","1456\n","1464\n","1472\n","1480\n","1488\n","1496\n","1504\n","1512\n","1520\n","1528\n","1536\n","1544\n","1552\n","1560\n","1568\n","1576\n","1584\n","1592\n","1600\n","1608\n","1616\n","1624\n","1632\n","1640\n","1648\n","1656\n","1664\n","1672\n","1680\n","1688\n","1696\n","1704\n","1712\n","1720\n","1728\n","1736\n","1744\n","1752\n","1760\n","1768\n","1776\n","1784\n","1792\n","1800\n","1808\n","1816\n","1824\n","1832\n","1840\n","1848\n","1856\n","1864\n","1872\n","1880\n","1888\n","1896\n","1904\n","1912\n","1920\n","1928\n","1936\n","1944\n","1952\n","1960\n","1968\n","1976\n","1984\n","1992\n","2000\n","2008\n","2016\n","2024\n","2032\n","2040\n","2048\n","2056\n","2064\n","2072\n","2080\n","2088\n","2096\n","2104\n","2112\n","2120\n","2128\n","2136\n","2144\n","2152\n","2160\n","2168\n","2176\n","2184\n","2192\n","2200\n","2208\n","2216\n","2224\n","2232\n","2240\n","2248\n","2256\n","2264\n","2272\n","2280\n","2288\n","2296\n","2304\n","2312\n","2320\n","2328\n","2336\n","2344\n","2352\n","2360\n","2368\n","2376\n","2384\n","2392\n","2400\n","2408\n","2416\n","2424\n","2432\n","2440\n","2448\n","2456\n","2464\n","2472\n","2480\n","2488\n","2496\n","2504\n","2512\n","2520\n","2528\n","2536\n","2544\n","2552\n","2560\n","2568\n","2576\n","2584\n","2592\n","2600\n","2608\n","2616\n","2624\n","2632\n","2640\n","2648\n","2656\n","2664\n","2672\n","2680\n","2688\n","2696\n","2704\n","2712\n","2720\n","2728\n","2736\n","2744\n","2752\n","2760\n","2768\n","2776\n","2784\n","2792\n","2800\n","2808\n","2816\n","2824\n","2832\n","2840\n","2848\n","2856\n","2864\n","2872\n","2880\n","2888\n","2896\n","2904\n","2912\n","2920\n","2928\n","2936\n","2944\n","2952\n","2960\n","2968\n","2976\n","2984\n","2992\n","3000\n","3008\n","3016\n","3024\n","3032\n","3040\n","3048\n","3056\n","3064\n","3072\n","3080\n","3088\n","3096\n","3104\n","3112\n","3120\n","3128\n","3136\n","3144\n","3152\n","3160\n","3168\n","3176\n","3184\n","3192\n","3200\n","3208\n","3216\n","3224\n","3232\n","3240\n","3248\n","3256\n","3264\n","3272\n","3280\n","3288\n","3296\n","3304\n","3312\n","3320\n","3328\n","3336\n","3344\n","3352\n","3360\n","3368\n","3376\n","3384\n","3392\n","3400\n","3408\n","3416\n","3424\n","3432\n","3440\n","3448\n","3456\n","3464\n","3472\n","3480\n","3488\n","3496\n","3504\n","3512\n","3520\n","3528\n","3536\n","3544\n","3552\n","3560\n","3568\n","3576\n","3584\n","3592\n","3600\n","3608\n","3616\n","3624\n","3632\n","3640\n","3648\n","3656\n","3664\n","3672\n","3680\n","3688\n","3696\n","3704\n","3712\n","3720\n","3728\n","3736\n","3744\n","3752\n","3760\n","3768\n","3776\n","3784\n","3792\n","3800\n","3808\n","3816\n","3824\n","3832\n","3840\n","3848\n","3856\n","3864\n","3872\n","3880\n","3888\n","3896\n","3904\n","3912\n","3920\n","3928\n","3936\n","3944\n","3952\n","3960\n","3968\n","3976\n","3984\n","3992\n","4000\n","4008\n","4016\n","4024\n","4032\n","4040\n","4048\n","4056\n","4064\n","4072\n","4080\n","4088\n","4096\n","4104\n","4112\n","4120\n","4128\n","4136\n","4144\n","4152\n","4160\n","4168\n","4176\n","4184\n","4192\n","4200\n","4208\n","4216\n","4224\n","4232\n","4240\n","4248\n","4256\n","4264\n","4272\n","4280\n","4288\n","4296\n","4304\n","4312\n","4320\n","4328\n","4336\n","4344\n","4352\n","4360\n","4368\n","4376\n","4384\n","4392\n","4400\n","4408\n","4416\n","4424\n","4432\n","4440\n","4448\n","4456\n","4464\n","4472\n","4480\n","4488\n","4496\n","4504\n","4512\n","4520\n","4528\n","4536\n","4544\n","4552\n","4560\n","4568\n","4576\n","4584\n","4592\n","4600\n","4608\n","4616\n","4624\n","4632\n","4640\n","4648\n","4656\n","4664\n","4672\n","4680\n","4688\n","4696\n","4704\n","4712\n","4720\n","4728\n","4736\n","4744\n","4752\n","4760\n","4768\n","4776\n","4784\n","4792\n","4800\n","4808\n","4816\n","4824\n","4832\n","4840\n","4848\n","4856\n","4864\n","4872\n","4880\n","4888\n","4896\n","4904\n","4912\n","4920\n","4928\n","4936\n","4944\n","4952\n","4960\n","4968\n","4976\n","4984\n","4992\n","5000\n","5008\n","5016\n","5024\n","5032\n","5040\n","5048\n","5056\n","5064\n","5072\n","5080\n","5088\n","5096\n","5104\n","5112\n","5120\n","5128\n","5136\n","5144\n","5152\n","5160\n","5168\n","5176\n","5184\n","5192\n","5200\n","5208\n","5216\n","5224\n","5232\n","5240\n","5248\n","5256\n","5264\n","5272\n","5280\n","5288\n","5296\n","5304\n","5312\n","5320\n","5328\n","5336\n","5344\n","5352\n","5360\n","5368\n","5376\n","5384\n","5392\n","5400\n","5408\n","5416\n","5424\n","5432\n","5440\n","5448\n","5456\n","5464\n","5472\n","5480\n","5488\n","5496\n","5504\n","5512\n","5520\n","5528\n","5536\n","5544\n","5552\n","5560\n","5568\n","5576\n","5584\n","5592\n","5600\n","5608\n","5616\n","5624\n","5632\n","5640\n","5648\n","5656\n","5664\n","5672\n","5680\n","5688\n","5696\n","5704\n","5712\n","5720\n","5728\n","5736\n","5744\n","5752\n","5760\n","5768\n","5776\n","5784\n","5792\n","5800\n","5808\n","5816\n","5824\n","5832\n","5840\n","5848\n","5856\n","5864\n","5872\n","5880\n","5888\n","5896\n","5904\n","5912\n","5920\n","5928\n","5936\n","5944\n","5952\n","5960\n","5968\n","5976\n","5984\n","5992\n","6000\n","6008\n","6016\n","6024\n","6032\n","6040\n","6048\n","6056\n","6064\n","6072\n","6080\n","6088\n","6096\n","6104\n","6112\n","6120\n","6128\n","6136\n","6144\n","6152\n","6160\n","6168\n","6176\n","6184\n","6192\n","6200\n","6208\n","6216\n","6224\n","6232\n","6240\n","6248\n","6256\n","6264\n","6272\n","6280\n","6288\n","6296\n","6304\n","6312\n","6320\n","6328\n","6336\n","6344\n","6352\n","6360\n","6368\n","6376\n","6384\n","6392\n","6400\n","6408\n","6416\n","6424\n","6432\n","6440\n","6448\n","6456\n","6464\n","6472\n","6480\n","6488\n","6496\n","6504\n","6512\n","6520\n","6528\n","6536\n","6544\n","6552\n","6560\n","6568\n","6576\n","6584\n","6592\n","6600\n","6608\n","6616\n","6624\n","6632\n","6640\n","6648\n","6656\n","6664\n","6672\n","6680\n","6688\n","6696\n","6704\n","6712\n","6720\n","6728\n","6736\n","6744\n","6752\n","6760\n","6768\n","6776\n","6784\n","6792\n","6800\n","6808\n","6816\n","6824\n","6832\n","6840\n","6848\n","6856\n","6864\n","6872\n","6880\n","6888\n","6896\n","6904\n","6912\n","6920\n","6928\n","6936\n","6944\n","6952\n","6960\n","6968\n","6976\n","6984\n","6992\n","7000\n","7008\n","7016\n","7024\n","7032\n","7040\n","7048\n","7056\n","7064\n","7072\n","7080\n","7088\n","7096\n","7104\n","7112\n","7120\n","7128\n","7136\n","7144\n","7152\n","7160\n","7168\n","7176\n","7184\n","7192\n","7200\n","7208\n","7216\n","7224\n","7232\n","7240\n","7248\n","7256\n","7264\n","7272\n","7280\n","7288\n","7296\n","7304\n","7312\n","7320\n","7328\n","7336\n","7344\n","7352\n","7360\n","7368\n","7376\n","7384\n","7392\n","7400\n","7408\n","7416\n","7424\n","7432\n","7440\n","7448\n","7456\n","7464\n","7472\n","7480\n","7488\n","7496\n","7504\n","7512\n","7520\n","7528\n","7536\n","7544\n","7552\n","7560\n","7568\n","7576\n","7584\n","7592\n","7600\n","7608\n","7616\n","7624\n","7632\n","7640\n","7648\n","7656\n","7664\n","7672\n","7680\n","7688\n","7696\n","7704\n","7712\n","7720\n","7728\n","7736\n","7744\n","7752\n","7760\n","7768\n","7776\n","7784\n","7792\n","7800\n","7808\n","7816\n","7824\n","7832\n","7840\n","7848\n","7856\n","7864\n","7872\n","7880\n","7888\n","7896\n","7904\n","7912\n","7920\n","7928\n","7936\n","7944\n","7952\n","7960\n","7968\n","7976\n","7984\n","7992\n","8000\n","8008\n","8016\n","8024\n","8032\n","8040\n","8048\n","8056\n","8064\n","8072\n","8080\n","8088\n","8096\n","8104\n","8112\n","8120\n","8128\n","8136\n","8144\n","8152\n","8160\n","8168\n","8176\n","8184\n","8192\n","8200\n","8208\n","8216\n","8224\n","8232\n","8240\n","8248\n","8256\n","8264\n","8272\n","8280\n","8288\n","8296\n","8304\n","8312\n","8320\n","8328\n","8336\n","8344\n","8352\n","8360\n","8368\n","8376\n","8384\n","8392\n","8400\n","8408\n","8416\n","8424\n","8432\n","8440\n","8448\n","8456\n","8464\n","8472\n","8480\n","8488\n","8496\n","8504\n","8512\n","8520\n","8528\n","8536\n","8544\n","8552\n","8560\n","8568\n","8576\n","8584\n","8592\n","8600\n","8608\n","8616\n","8624\n","8632\n","8640\n","8648\n","8656\n","8664\n","8672\n","8680\n","8688\n","8696\n","8704\n","8712\n","8720\n","8728\n","8736\n","8744\n","8752\n","8760\n","8768\n","8776\n","8784\n","8792\n","8800\n","8808\n","8816\n","8824\n","8832\n","8840\n","8848\n","8856\n","8864\n","8872\n","8880\n","8888\n","8896\n","8904\n","8912\n","8920\n","8928\n","8936\n","8944\n","8952\n","8960\n","8968\n","8976\n","8984\n","8992\n","9000\n","9008\n","9016\n","9024\n","9032\n","9040\n","9048\n","9056\n","9064\n","9072\n","9080\n","9088\n","9096\n","9104\n","9112\n","9120\n","9128\n","9136\n","9144\n","9152\n","9160\n","9168\n","9176\n","9184\n","9192\n","9200\n","9208\n","9216\n","9224\n","9232\n","9240\n","9248\n","9256\n","9264\n","9272\n","9280\n","9288\n","9296\n","9304\n","9312\n","9320\n","9328\n","9336\n","9344\n","9352\n","9360\n","9368\n","9376\n","9384\n","9392\n","9400\n","9408\n","9416\n","9424\n","9432\n","9440\n","9448\n","9456\n","9464\n","9472\n","9480\n","9488\n","9496\n","9504\n","9512\n","9520\n","9528\n","9536\n","9544\n","9552\n","9560\n","9568\n","9576\n","9584\n","9592\n","9600\n","9608\n","9616\n","9624\n","9632\n","9640\n","9648\n","9656\n","9664\n","9672\n","9680\n","9688\n","9696\n","9704\n","9712\n","9720\n","9728\n","9736\n","9744\n","9752\n","9760\n","9768\n","9776\n","9784\n","9792\n","9800\n","9808\n","9816\n","9824\n","9832\n","9840\n","9848\n","9856\n","9864\n","9872\n","9880\n","9888\n","9896\n","9904\n","9912\n","9920\n","9928\n","9936\n","9944\n","9952\n","9960\n","9968\n","9976\n","9984\n","9992\n","10000\n","10008\n","10016\n","10024\n","10032\n","10040\n","10048\n","10056\n","10064\n","10072\n","10080\n","10088\n","10096\n","10104\n","10112\n","10120\n","10128\n","10136\n","10144\n","10152\n","10160\n","10168\n","10176\n","10184\n","10192\n","10200\n","10208\n","10216\n","10224\n","10232\n","10240\n","10248\n","10256\n","10264\n","10272\n","10280\n","10288\n","10296\n","10304\n","10312\n","10320\n","10328\n","10336\n","10344\n","10352\n","10360\n","10368\n","10376\n","10384\n","10392\n","10400\n","10408\n","10416\n","10424\n","10432\n","10440\n","10448\n","10456\n","10464\n","10472\n","10480\n","10488\n","10496\n","10504\n","10512\n","10520\n","10528\n","10536\n","10544\n","10552\n","10560\n","10568\n","10576\n","10584\n","10592\n","10600\n","10608\n","10616\n","10624\n","10632\n","10640\n","10648\n","10656\n","10664\n","10672\n","10680\n","10688\n","10696\n","10704\n","10712\n","10720\n","10728\n","10736\n","10744\n","10752\n","10760\n","10768\n","10776\n","10784\n","10792\n","10800\n","10808\n","10816\n","10824\n","10832\n","10840\n","10848\n","10856\n","10864\n","10872\n","10880\n","10888\n","10896\n","10904\n","10912\n","10920\n","10928\n","10936\n","10944\n","10952\n","10960\n","10968\n","10976\n","10984\n","10992\n","11000\n","11008\n","11016\n","11024\n","11032\n","11040\n","11048\n","11056\n","11064\n","11072\n","11080\n","11088\n","11096\n","11104\n","11112\n","11120\n","11128\n","11136\n","11144\n","11152\n","11160\n","11168\n","11176\n","11184\n","11192\n","11200\n","11208\n","11216\n","11224\n","11232\n","11240\n","11248\n","11256\n","11264\n","11272\n","11280\n","11288\n","11296\n","11304\n","11312\n","11320\n","11328\n","11336\n","11344\n","11352\n","11360\n","11368\n","11376\n","11384\n","11392\n","11400\n","11408\n","11416\n","11424\n","11432\n","11440\n","11448\n","11456\n","11464\n","11472\n","11480\n","11488\n","11496\n","11504\n","11512\n","11520\n","11528\n","11536\n","11544\n","11552\n","11560\n","11568\n","11576\n","11584\n","11592\n","11600\n","11608\n","11616\n","11624\n","11632\n","11640\n","11648\n","11656\n","11664\n","11672\n","11680\n","11688\n","11696\n","11704\n","11712\n","11720\n","11728\n","11736\n","11744\n","11752\n","11760\n","11768\n","11776\n","11784\n","11792\n","11800\n","11808\n","11816\n","11824\n","11832\n","11840\n","11848\n","11856\n","11864\n","11872\n","11880\n","11888\n","11896\n","11904\n","11912\n","11920\n","11928\n","11936\n","11944\n","11952\n","11960\n","11968\n","11976\n","11984\n","11992\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.2845\n","Epoch 2 train accuracy: 0.9208\n","Epoch 2 dev loss: 1.5642\n","Epoch 2 dev accuracy: 0.6719\n","\n"," ######## \n","\n","lr:0.0005, alpha:0.0001 @ epoch 1.\n","TL:0.3517950084544718, TA:0.9055995973645191.\n","DL:1.4560895394253177, DA:0.6747574191728593\n"]},{"output_type":"execute_result","data":{"text/plain":["Stats(train_loss=0.3517950084544718, train_accuracy=0.9055995973645191, dev_loss=1.4560895394253177, dev_accuracy=0.6747574191728593, epoch=1, lr=0.0005, alpha=0.0001, max_accuracy=0.6747574191728593)"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","source":["#### $\\color{red}{Run:}$"],"metadata":{"id":"ZBHJn5V8mIDu"}},{"cell_type":"code","source":["\"\"\"\n","Main Admin\n","\"\"\"\n","epochs = 30\n","max_accuracy = 0\n","path = \"class/models/GNN.3.pt\"\n","results = []\n","\n","\"\"\"\n","init random search\n","lr [10^-5 - 10^-1]\n","alpha [10^-5 - 10^-1]\n","bs [8, 32, 128]\n","\"\"\"\n","lr_low = -5\n","lr_high = -3\n","lr_range = lr_high - lr_low\n","\n","alpha_low = -5\n","alpha_high = -3\n","alpha_range = alpha_high - alpha_low\n","\n","d = 768\n","h = 400\n","c = 70\n","num_relations = 1\n","\n","count = 0\n","\n","\"\"\"\n","Hyperparameter Search\n","\"\"\"\n","\n","for i in range(3):\n","  # debug\n","  print(\"\\n################\\n\")\n","  print(f'round: {i}')\n","  # print(f'lr_low{lr_low}, lr_high{lr_high}, lr_range{lr_range}')\n","  # print(f'alpha_low{alpha_low}, lr_high{alpha_high}, lr_range{alpha_range}')\n","  print('max', max_accuracy)\n","  print(\"\\n################\\n\")\n","\n","\n","  for j in range(6):\n","    count += 1\n","    print(count)\n","\n","    # get config\n","    lr, alpha = gen_config(lr_low, lr_high, alpha_low, alpha_high)\n","    # define model\n","    model = GNNModel(d, h, c, num_relations)\n","    model = model.to(device)\n","\n","    # run training\n","    res = tv_run(epochs, model, lr, alpha, max_accuracy, path, verbose = 2)\n","    max_accuracy = res.max_accuracy\n","    results.append(res)\n","\n","  # get best result of the round or even so far\n","  stats = search_stats(results)\n","\n","\n","  print(stats) # debug\n","\n","  # reconfigure the new hypers\n","  lr = np.log10(stats.lr)\n","  lr_range = lr_range / 3\n","\n","  alpha = np.log10(stats.alpha)\n","  alpha_range = alpha_range / 3\n","\n","  config = gen_ranges(lr, lr_range, alpha, alpha_range)\n","  lr_low, lr_high, alpha_low, alpha_high = config\n","  lr_range = lr_high - lr_low\n","  alpha_range = alpha_high - alpha_low\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fSA1uTMSaN_g","outputId":"f5f2e2c6-ad01-448a-cfa1-b8d0326f4a6c","executionInfo":{"status":"ok","timestamp":1734607266028,"user_tz":-60,"elapsed":14076653,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","################\n","\n","round: 0\n","max 0\n","\n","################\n","\n","1\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 3.0942\n","Epoch 1 train accuracy: 0.2960\n","Epoch 1 dev loss: 1.9864\n","Epoch 1 dev accuracy: 0.4889\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 1.9579\n","Epoch 2 train accuracy: 0.5328\n","Epoch 2 dev loss: 1.4462\n","Epoch 2 dev accuracy: 0.5802\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 1.4683\n","Epoch 3 train accuracy: 0.6356\n","Epoch 3 dev loss: 1.2633\n","Epoch 3 dev accuracy: 0.6220\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 1.1842\n","Epoch 4 train accuracy: 0.6981\n","Epoch 4 dev loss: 1.1948\n","Epoch 4 dev accuracy: 0.6162\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.9946\n","Epoch 5 train accuracy: 0.7451\n","Epoch 5 dev loss: 1.1355\n","Epoch 5 dev accuracy: 0.6359\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.8639\n","Epoch 6 train accuracy: 0.7772\n","Epoch 6 dev loss: 1.1321\n","Epoch 6 dev accuracy: 0.6428\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.7508\n","Epoch 7 train accuracy: 0.8082\n","Epoch 7 dev loss: 1.1236\n","Epoch 7 dev accuracy: 0.6572\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.6669\n","Epoch 8 train accuracy: 0.8302\n","Epoch 8 dev loss: 1.1110\n","Epoch 8 dev accuracy: 0.6605\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.5908\n","Epoch 9 train accuracy: 0.8489\n","Epoch 9 dev loss: 1.1012\n","Epoch 9 dev accuracy: 0.6677\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.5328\n","Epoch 10 train accuracy: 0.8665\n","Epoch 10 dev loss: 1.1260\n","Epoch 10 dev accuracy: 0.6636\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.4907\n","Epoch 11 train accuracy: 0.8772\n","Epoch 11 dev loss: 1.1310\n","Epoch 11 dev accuracy: 0.6755\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.4461\n","Epoch 12 train accuracy: 0.8888\n","Epoch 12 dev loss: 1.1476\n","Epoch 12 dev accuracy: 0.6722\n","\n"," --------- \n","Epoch: 13\n","\n","Epoch 13 train loss: 0.4078\n","Epoch 13 train accuracy: 0.8987\n","Epoch 13 dev loss: 1.1543\n","Epoch 13 dev accuracy: 0.6725\n","\n"," --------- \n","Epoch: 14\n","\n","Epoch 14 train loss: 0.3802\n","Epoch 14 train accuracy: 0.9059\n","Epoch 14 dev loss: 1.1704\n","Epoch 14 dev accuracy: 0.6802\n","\n"," --------- \n","Epoch: 15\n","\n","Epoch 15 train loss: 0.3710\n","Epoch 15 train accuracy: 0.9066\n","Epoch 15 dev loss: 1.1480\n","Epoch 15 dev accuracy: 0.6832\n","\n"," --------- \n","Epoch: 16\n","\n","Epoch 16 train loss: 0.3394\n","Epoch 16 train accuracy: 0.9151\n","Epoch 16 dev loss: 1.2114\n","Epoch 16 dev accuracy: 0.6834\n","\n"," --------- \n","Epoch: 17\n","\n","Epoch 17 train loss: 0.3261\n","Epoch 17 train accuracy: 0.9182\n","Epoch 17 dev loss: 1.1984\n","Epoch 17 dev accuracy: 0.6848\n","\n"," --------- \n","Epoch: 18\n","\n","Epoch 18 train loss: 0.3012\n","Epoch 18 train accuracy: 0.9264\n","Epoch 18 dev loss: 1.1811\n","Epoch 18 dev accuracy: 0.6819\n","\n"," --------- \n","Epoch: 19\n","\n","Epoch 19 train loss: 0.2859\n","Epoch 19 train accuracy: 0.9285\n","Epoch 19 dev loss: 1.2142\n","Epoch 19 dev accuracy: 0.6800\n","\n"," --------- \n","Epoch: 20\n","\n","Epoch 20 train loss: 0.2720\n","Epoch 20 train accuracy: 0.9312\n","Epoch 20 dev loss: 1.2328\n","Epoch 20 dev accuracy: 0.6872\n","\n"," --------- \n","Epoch: 21\n","\n","Epoch 21 train loss: 0.2661\n","Epoch 21 train accuracy: 0.9318\n","Epoch 21 dev loss: 1.2401\n","Epoch 21 dev accuracy: 0.6840\n","\n"," --------- \n","Epoch: 22\n","\n","Epoch 22 train loss: 0.2511\n","Epoch 22 train accuracy: 0.9352\n","Epoch 22 dev loss: 1.2402\n","Epoch 22 dev accuracy: 0.6872\n","\n"," --------- \n","Epoch: 23\n","\n","Epoch 23 train loss: 0.2388\n","Epoch 23 train accuracy: 0.9384\n","Epoch 23 dev loss: 1.2579\n","Epoch 23 dev accuracy: 0.6905\n","\n"," --------- \n","Epoch: 24\n","\n","Epoch 24 train loss: 0.2300\n","Epoch 24 train accuracy: 0.9408\n","Epoch 24 dev loss: 1.2719\n","Epoch 24 dev accuracy: 0.6878\n","\n"," --------- \n","Epoch: 25\n","\n","Epoch 25 train loss: 0.2242\n","Epoch 25 train accuracy: 0.9415\n","Epoch 25 dev loss: 1.2595\n","Epoch 25 dev accuracy: 0.6953\n","\n"," --------- \n","Epoch: 26\n","\n","Epoch 26 train loss: 0.2219\n","Epoch 26 train accuracy: 0.9419\n","Epoch 26 dev loss: 1.2502\n","Epoch 26 dev accuracy: 0.6939\n","\n"," --------- \n","Epoch: 27\n","\n","Epoch 27 train loss: 0.2189\n","Epoch 27 train accuracy: 0.9428\n","Epoch 27 dev loss: 1.2930\n","Epoch 27 dev accuracy: 0.6884\n","\n"," --------- \n","Epoch: 28\n","\n","Epoch 28 train loss: 0.2071\n","Epoch 28 train accuracy: 0.9447\n","Epoch 28 dev loss: 1.2789\n","Epoch 28 dev accuracy: 0.6919\n","\n"," --------- \n","Epoch: 29\n","\n","Epoch 29 train loss: 0.2043\n","Epoch 29 train accuracy: 0.9461\n","Epoch 29 dev loss: 1.3000\n","Epoch 29 dev accuracy: 0.6937\n","\n"," --------- \n","Epoch: 30\n","\n","Epoch 30 train loss: 0.1899\n","Epoch 30 train accuracy: 0.9500\n","Epoch 30 dev loss: 1.2895\n","Epoch 30 dev accuracy: 0.6888\n","\n"," ######## \n","\n","lr:2.3e-05, alpha:0.00032 @ epoch 25.\n","TL:0.2242259551572303, TA:0.9415472732913018.\n","DL:1.2594871150800835, DA:0.6952763242959102\n","2\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.9401\n","Epoch 1 train accuracy: 0.5367\n","Epoch 1 dev loss: 1.2212\n","Epoch 1 dev accuracy: 0.6270\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.8554\n","Epoch 2 train accuracy: 0.7771\n","Epoch 2 dev loss: 1.1331\n","Epoch 2 dev accuracy: 0.6631\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.5638\n","Epoch 3 train accuracy: 0.8554\n","Epoch 3 dev loss: 1.1495\n","Epoch 3 dev accuracy: 0.6821\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.4330\n","Epoch 4 train accuracy: 0.8892\n","Epoch 4 dev loss: 1.2402\n","Epoch 4 dev accuracy: 0.6624\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.3588\n","Epoch 5 train accuracy: 0.9071\n","Epoch 5 dev loss: 1.2220\n","Epoch 5 dev accuracy: 0.6748\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.3159\n","Epoch 6 train accuracy: 0.9179\n","Epoch 6 dev loss: 1.2868\n","Epoch 6 dev accuracy: 0.6750\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2898\n","Epoch 7 train accuracy: 0.9239\n","Epoch 7 dev loss: 1.3394\n","Epoch 7 dev accuracy: 0.6763\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2568\n","Epoch 8 train accuracy: 0.9316\n","Epoch 8 dev loss: 1.3218\n","Epoch 8 dev accuracy: 0.6798\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2265\n","Epoch 9 train accuracy: 0.9392\n","Epoch 9 dev loss: 1.3334\n","Epoch 9 dev accuracy: 0.6819\n","\n"," ######## \n","\n","lr:9.7e-05, alpha:0.000101 @ epoch 3.\n","TL:0.5637732151349386, TA:0.8553985961598309.\n","DL:1.1495108109171057, DA:0.6821067602272038\n","3\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.1653\n","Epoch 1 train accuracy: 0.7004\n","Epoch 1 dev loss: 1.2398\n","Epoch 1 dev accuracy: 0.6480\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.5032\n","Epoch 2 train accuracy: 0.8646\n","Epoch 2 dev loss: 1.3616\n","Epoch 2 dev accuracy: 0.6527\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.3912\n","Epoch 3 train accuracy: 0.8931\n","Epoch 3 dev loss: 1.5623\n","Epoch 3 dev accuracy: 0.6662\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3421\n","Epoch 4 train accuracy: 0.9063\n","Epoch 4 dev loss: 1.7608\n","Epoch 4 dev accuracy: 0.6447\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.2916\n","Epoch 5 train accuracy: 0.9188\n","Epoch 5 dev loss: 1.6280\n","Epoch 5 dev accuracy: 0.6759\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2649\n","Epoch 6 train accuracy: 0.9255\n","Epoch 6 dev loss: 1.6029\n","Epoch 6 dev accuracy: 0.6767\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2589\n","Epoch 7 train accuracy: 0.9273\n","Epoch 7 dev loss: 1.4777\n","Epoch 7 dev accuracy: 0.6794\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2502\n","Epoch 8 train accuracy: 0.9306\n","Epoch 8 dev loss: 1.5713\n","Epoch 8 dev accuracy: 0.6791\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2355\n","Epoch 9 train accuracy: 0.9328\n","Epoch 9 dev loss: 1.6155\n","Epoch 9 dev accuracy: 0.6734\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2181\n","Epoch 10 train accuracy: 0.9380\n","Epoch 10 dev loss: 1.6108\n","Epoch 10 dev accuracy: 0.6890\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.2203\n","Epoch 11 train accuracy: 0.9371\n","Epoch 11 dev loss: 1.5936\n","Epoch 11 dev accuracy: 0.6926\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.2004\n","Epoch 12 train accuracy: 0.9427\n","Epoch 12 dev loss: 1.7295\n","Epoch 12 dev accuracy: 0.6778\n","\n"," --------- \n","Epoch: 13\n","\n","Epoch 13 train loss: 0.1782\n","Epoch 13 train accuracy: 0.9478\n","Epoch 13 dev loss: 1.7743\n","Epoch 13 dev accuracy: 0.6755\n","\n"," --------- \n","Epoch: 14\n","\n","Epoch 14 train loss: 0.1838\n","Epoch 14 train accuracy: 0.9463\n","Epoch 14 dev loss: 1.6837\n","Epoch 14 dev accuracy: 0.6780\n","\n"," --------- \n","Epoch: 15\n","\n","Epoch 15 train loss: 0.1681\n","Epoch 15 train accuracy: 0.9500\n","Epoch 15 dev loss: 1.7118\n","Epoch 15 dev accuracy: 0.6832\n","\n"," --------- \n","Epoch: 16\n","\n","Epoch 16 train loss: 0.1599\n","Epoch 16 train accuracy: 0.9528\n","Epoch 16 dev loss: 1.8113\n","Epoch 16 dev accuracy: 0.6814\n","\n"," --------- \n","Epoch: 17\n","\n","Epoch 17 train loss: 0.1677\n","Epoch 17 train accuracy: 0.9503\n","Epoch 17 dev loss: 1.7781\n","Epoch 17 dev accuracy: 0.6895\n","\n"," ######## \n","\n","lr:0.000399, alpha:6.1e-05 @ epoch 11.\n","TL:0.2202906826570009, TA:0.9371342759399622.\n","DL:1.593559372564102, DA:0.6925895276961629\n","4\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.8349\n","Epoch 1 train accuracy: 0.5535\n","Epoch 1 dev loss: 1.1942\n","Epoch 1 dev accuracy: 0.6354\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.7903\n","Epoch 2 train accuracy: 0.7923\n","Epoch 2 dev loss: 1.1584\n","Epoch 2 dev accuracy: 0.6530\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.5199\n","Epoch 3 train accuracy: 0.8667\n","Epoch 3 dev loss: 1.1562\n","Epoch 3 dev accuracy: 0.6761\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.4103\n","Epoch 4 train accuracy: 0.8938\n","Epoch 4 dev loss: 1.2395\n","Epoch 4 dev accuracy: 0.6811\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.3338\n","Epoch 5 train accuracy: 0.9139\n","Epoch 5 dev loss: 1.4488\n","Epoch 5 dev accuracy: 0.6591\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2889\n","Epoch 6 train accuracy: 0.9238\n","Epoch 6 dev loss: 1.3925\n","Epoch 6 dev accuracy: 0.6669\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2711\n","Epoch 7 train accuracy: 0.9288\n","Epoch 7 dev loss: 1.3608\n","Epoch 7 dev accuracy: 0.6762\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2399\n","Epoch 8 train accuracy: 0.9353\n","Epoch 8 dev loss: 1.4406\n","Epoch 8 dev accuracy: 0.6779\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2356\n","Epoch 9 train accuracy: 0.9368\n","Epoch 9 dev loss: 1.4924\n","Epoch 9 dev accuracy: 0.6655\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2145\n","Epoch 10 train accuracy: 0.9412\n","Epoch 10 dev loss: 1.4623\n","Epoch 10 dev accuracy: 0.6718\n","\n"," ######## \n","\n","lr:0.00011, alpha:3.9e-05 @ epoch 4.\n","TL:0.4102948072006305, TA:0.8938184680749073.\n","DL:1.2395232789302861, DA:0.6811011917857945\n","5\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 2.7073\n","Epoch 1 train accuracy: 0.3804\n","Epoch 1 dev loss: 1.6097\n","Epoch 1 dev accuracy: 0.5503\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 1.5062\n","Epoch 2 train accuracy: 0.6248\n","Epoch 2 dev loss: 1.2745\n","Epoch 2 dev accuracy: 0.6236\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 1.0918\n","Epoch 3 train accuracy: 0.7197\n","Epoch 3 dev loss: 1.1828\n","Epoch 3 dev accuracy: 0.6469\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.8370\n","Epoch 4 train accuracy: 0.7843\n","Epoch 4 dev loss: 1.1536\n","Epoch 4 dev accuracy: 0.6461\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.6890\n","Epoch 5 train accuracy: 0.8233\n","Epoch 5 dev loss: 1.1396\n","Epoch 5 dev accuracy: 0.6607\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.5745\n","Epoch 6 train accuracy: 0.8547\n","Epoch 6 dev loss: 1.1516\n","Epoch 6 dev accuracy: 0.6724\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.4814\n","Epoch 7 train accuracy: 0.8796\n","Epoch 7 dev loss: 1.1797\n","Epoch 7 dev accuracy: 0.6798\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.4317\n","Epoch 8 train accuracy: 0.8922\n","Epoch 8 dev loss: 1.1801\n","Epoch 8 dev accuracy: 0.6752\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.3798\n","Epoch 9 train accuracy: 0.9050\n","Epoch 9 dev loss: 1.2416\n","Epoch 9 dev accuracy: 0.6789\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.3445\n","Epoch 10 train accuracy: 0.9144\n","Epoch 10 dev loss: 1.2647\n","Epoch 10 dev accuracy: 0.6833\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.3089\n","Epoch 11 train accuracy: 0.9227\n","Epoch 11 dev loss: 1.2707\n","Epoch 11 dev accuracy: 0.6865\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.2950\n","Epoch 12 train accuracy: 0.9252\n","Epoch 12 dev loss: 1.2916\n","Epoch 12 dev accuracy: 0.6779\n","\n"," --------- \n","Epoch: 13\n","\n","Epoch 13 train loss: 0.2844\n","Epoch 13 train accuracy: 0.9275\n","Epoch 13 dev loss: 1.3037\n","Epoch 13 dev accuracy: 0.6883\n","\n"," --------- \n","Epoch: 14\n","\n","Epoch 14 train loss: 0.2604\n","Epoch 14 train accuracy: 0.9332\n","Epoch 14 dev loss: 1.2986\n","Epoch 14 dev accuracy: 0.6844\n","\n"," --------- \n","Epoch: 15\n","\n","Epoch 15 train loss: 0.2410\n","Epoch 15 train accuracy: 0.9371\n","Epoch 15 dev loss: 1.2910\n","Epoch 15 dev accuracy: 0.6834\n","\n"," --------- \n","Epoch: 16\n","\n","Epoch 16 train loss: 0.2359\n","Epoch 16 train accuracy: 0.9385\n","Epoch 16 dev loss: 1.3552\n","Epoch 16 dev accuracy: 0.6669\n","\n"," --------- \n","Epoch: 17\n","\n","Epoch 17 train loss: 0.2161\n","Epoch 17 train accuracy: 0.9429\n","Epoch 17 dev loss: 1.3814\n","Epoch 17 dev accuracy: 0.6810\n","\n"," --------- \n","Epoch: 18\n","\n","Epoch 18 train loss: 0.2015\n","Epoch 18 train accuracy: 0.9467\n","Epoch 18 dev loss: 1.3670\n","Epoch 18 dev accuracy: 0.6854\n","\n"," --------- \n","Epoch: 19\n","\n","Epoch 19 train loss: 0.2078\n","Epoch 19 train accuracy: 0.9457\n","Epoch 19 dev loss: 1.3814\n","Epoch 19 dev accuracy: 0.6804\n","\n"," ######## \n","\n","lr:3.7e-05, alpha:4.8e-05 @ epoch 13.\n","TL:0.2843890269945065, TA:0.9275243645661768.\n","DL:1.3036672693891933, DA:0.6882623951513193\n","6\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.1144\n","Epoch 1 train accuracy: 0.7116\n","Epoch 1 dev loss: 1.3024\n","Epoch 1 dev accuracy: 0.6552\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.4912\n","Epoch 2 train accuracy: 0.8675\n","Epoch 2 dev loss: 1.4575\n","Epoch 2 dev accuracy: 0.6469\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.3727\n","Epoch 3 train accuracy: 0.8978\n","Epoch 3 dev loss: 1.4910\n","Epoch 3 dev accuracy: 0.6589\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3243\n","Epoch 4 train accuracy: 0.9116\n","Epoch 4 dev loss: 1.4789\n","Epoch 4 dev accuracy: 0.6771\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.3135\n","Epoch 5 train accuracy: 0.9138\n","Epoch 5 dev loss: 1.5824\n","Epoch 5 dev accuracy: 0.6700\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2646\n","Epoch 6 train accuracy: 0.9267\n","Epoch 6 dev loss: 1.5204\n","Epoch 6 dev accuracy: 0.6803\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2565\n","Epoch 7 train accuracy: 0.9283\n","Epoch 7 dev loss: 1.6987\n","Epoch 7 dev accuracy: 0.6798\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2622\n","Epoch 8 train accuracy: 0.9283\n","Epoch 8 dev loss: 1.7119\n","Epoch 8 dev accuracy: 0.6617\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2173\n","Epoch 9 train accuracy: 0.9379\n","Epoch 9 dev loss: 1.6772\n","Epoch 9 dev accuracy: 0.6782\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2077\n","Epoch 10 train accuracy: 0.9414\n","Epoch 10 dev loss: 1.7192\n","Epoch 10 dev accuracy: 0.6911\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.2192\n","Epoch 11 train accuracy: 0.9371\n","Epoch 11 dev loss: 1.7044\n","Epoch 11 dev accuracy: 0.6877\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.2143\n","Epoch 12 train accuracy: 0.9402\n","Epoch 12 dev loss: 1.6984\n","Epoch 12 dev accuracy: 0.6842\n","\n"," --------- \n","Epoch: 13\n","\n","Epoch 13 train loss: 0.1813\n","Epoch 13 train accuracy: 0.9481\n","Epoch 13 dev loss: 2.0487\n","Epoch 13 dev accuracy: 0.6619\n","\n"," --------- \n","Epoch: 14\n","\n","Epoch 14 train loss: 0.1815\n","Epoch 14 train accuracy: 0.9468\n","Epoch 14 dev loss: 1.8211\n","Epoch 14 dev accuracy: 0.6685\n","\n"," --------- \n","Epoch: 15\n","\n","Epoch 15 train loss: 0.1816\n","Epoch 15 train accuracy: 0.9474\n","Epoch 15 dev loss: 1.9287\n","Epoch 15 dev accuracy: 0.7022\n","\n"," --------- \n","Epoch: 16\n","\n","Epoch 16 train loss: 0.1672\n","Epoch 16 train accuracy: 0.9497\n","Epoch 16 dev loss: 1.7998\n","Epoch 16 dev accuracy: 0.6731\n","\n"," --------- \n","Epoch: 17\n","\n","Epoch 17 train loss: 0.1776\n","Epoch 17 train accuracy: 0.9479\n","Epoch 17 dev loss: 1.9615\n","Epoch 17 dev accuracy: 0.6640\n","\n"," --------- \n","Epoch: 18\n","\n","Epoch 18 train loss: 0.1505\n","Epoch 18 train accuracy: 0.9560\n","Epoch 18 dev loss: 1.9240\n","Epoch 18 dev accuracy: 0.6684\n","\n"," --------- \n","Epoch: 19\n","\n","Epoch 19 train loss: 0.1477\n","Epoch 19 train accuracy: 0.9564\n","Epoch 19 dev loss: 1.8301\n","Epoch 19 dev accuracy: 0.6764\n","\n"," --------- \n","Epoch: 20\n","\n","Epoch 20 train loss: 0.1487\n","Epoch 20 train accuracy: 0.9548\n","Epoch 20 dev loss: 1.9489\n","Epoch 20 dev accuracy: 0.6800\n","\n"," --------- \n","Epoch: 21\n","\n","Epoch 21 train loss: 0.1503\n","Epoch 21 train accuracy: 0.9548\n","Epoch 21 dev loss: 2.0900\n","Epoch 21 dev accuracy: 0.6634\n","\n"," ######## \n","\n","lr:0.000458, alpha:0.000525 @ epoch 15.\n","TL:0.181590415709652, TA:0.9473572890572522.\n","DL:1.928712646967596, DA:0.7022279812094488\n","Stats(train_loss=0.181590415709652, train_accuracy=0.9473572890572522, dev_loss=1.928712646967596, dev_accuracy=0.7022279812094488, epoch=15, lr=0.000458, alpha=0.000525, max_accuracy=0.7022279812094488)\n","\n","################\n","\n","round: 1\n","max 0.7022279812094488\n","\n","################\n","\n","7\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.3194\n","Epoch 1 train accuracy: 0.6703\n","Epoch 1 dev loss: 1.1404\n","Epoch 1 dev accuracy: 0.6610\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.5399\n","Epoch 2 train accuracy: 0.8591\n","Epoch 2 dev loss: 1.3131\n","Epoch 2 dev accuracy: 0.6533\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.3890\n","Epoch 3 train accuracy: 0.8972\n","Epoch 3 dev loss: 1.3516\n","Epoch 3 dev accuracy: 0.6646\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3358\n","Epoch 4 train accuracy: 0.9094\n","Epoch 4 dev loss: 1.4531\n","Epoch 4 dev accuracy: 0.6692\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.2864\n","Epoch 5 train accuracy: 0.9218\n","Epoch 5 dev loss: 1.3942\n","Epoch 5 dev accuracy: 0.6784\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2764\n","Epoch 6 train accuracy: 0.9253\n","Epoch 6 dev loss: 1.4010\n","Epoch 6 dev accuracy: 0.6804\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2388\n","Epoch 7 train accuracy: 0.9343\n","Epoch 7 dev loss: 1.4924\n","Epoch 7 dev accuracy: 0.6818\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2246\n","Epoch 8 train accuracy: 0.9370\n","Epoch 8 dev loss: 1.5199\n","Epoch 8 dev accuracy: 0.6732\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2030\n","Epoch 9 train accuracy: 0.9425\n","Epoch 9 dev loss: 1.6010\n","Epoch 9 dev accuracy: 0.6465\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2153\n","Epoch 10 train accuracy: 0.9393\n","Epoch 10 dev loss: 1.4504\n","Epoch 10 dev accuracy: 0.7048\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.1962\n","Epoch 11 train accuracy: 0.9452\n","Epoch 11 dev loss: 1.5872\n","Epoch 11 dev accuracy: 0.6768\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.1941\n","Epoch 12 train accuracy: 0.9459\n","Epoch 12 dev loss: 1.5955\n","Epoch 12 dev accuracy: 0.6729\n","\n"," --------- \n","Epoch: 13\n","\n","Epoch 13 train loss: 0.1770\n","Epoch 13 train accuracy: 0.9482\n","Epoch 13 dev loss: 1.6817\n","Epoch 13 dev accuracy: 0.6952\n","\n"," --------- \n","Epoch: 14\n","\n","Epoch 14 train loss: 0.1747\n","Epoch 14 train accuracy: 0.9504\n","Epoch 14 dev loss: 1.6174\n","Epoch 14 dev accuracy: 0.6901\n","\n"," --------- \n","Epoch: 15\n","\n","Epoch 15 train loss: 0.1670\n","Epoch 15 train accuracy: 0.9517\n","Epoch 15 dev loss: 1.6261\n","Epoch 15 dev accuracy: 0.6860\n","\n"," --------- \n","Epoch: 16\n","\n","Epoch 16 train loss: 0.1590\n","Epoch 16 train accuracy: 0.9538\n","Epoch 16 dev loss: 1.6190\n","Epoch 16 dev accuracy: 0.6759\n","\n"," ######## \n","\n","lr:0.000241, alpha:0.000497 @ epoch 10.\n","TL:0.21530038304502766, TA:0.9392935180224953.\n","DL:1.4504363209052689, DA:0.7047604500393153\n","8\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.3773\n","Epoch 1 train accuracy: 0.6524\n","Epoch 1 dev loss: 1.2380\n","Epoch 1 dev accuracy: 0.6583\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.5338\n","Epoch 2 train accuracy: 0.8594\n","Epoch 2 dev loss: 1.2624\n","Epoch 2 dev accuracy: 0.6814\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.3988\n","Epoch 3 train accuracy: 0.8938\n","Epoch 3 dev loss: 1.4008\n","Epoch 3 dev accuracy: 0.6590\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3214\n","Epoch 4 train accuracy: 0.9138\n","Epoch 4 dev loss: 1.4002\n","Epoch 4 dev accuracy: 0.6838\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.3041\n","Epoch 5 train accuracy: 0.9174\n","Epoch 5 dev loss: 1.4876\n","Epoch 5 dev accuracy: 0.6823\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2720\n","Epoch 6 train accuracy: 0.9254\n","Epoch 6 dev loss: 1.5499\n","Epoch 6 dev accuracy: 0.6945\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2489\n","Epoch 7 train accuracy: 0.9312\n","Epoch 7 dev loss: 1.5029\n","Epoch 7 dev accuracy: 0.6771\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2285\n","Epoch 8 train accuracy: 0.9364\n","Epoch 8 dev loss: 1.5746\n","Epoch 8 dev accuracy: 0.6994\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2107\n","Epoch 9 train accuracy: 0.9415\n","Epoch 9 dev loss: 1.7036\n","Epoch 9 dev accuracy: 0.6693\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2039\n","Epoch 10 train accuracy: 0.9419\n","Epoch 10 dev loss: 1.7601\n","Epoch 10 dev accuracy: 0.6928\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.1906\n","Epoch 11 train accuracy: 0.9461\n","Epoch 11 dev loss: 1.6451\n","Epoch 11 dev accuracy: 0.6776\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.1865\n","Epoch 12 train accuracy: 0.9462\n","Epoch 12 dev loss: 1.6845\n","Epoch 12 dev accuracy: 0.6907\n","\n"," --------- \n","Epoch: 13\n","\n","Epoch 13 train loss: 0.1681\n","Epoch 13 train accuracy: 0.9519\n","Epoch 13 dev loss: 1.7456\n","Epoch 13 dev accuracy: 0.6803\n","\n"," --------- \n","Epoch: 14\n","\n","Epoch 14 train loss: 0.1630\n","Epoch 14 train accuracy: 0.9529\n","Epoch 14 dev loss: 1.6024\n","Epoch 14 dev accuracy: 0.6909\n","\n"," ######## \n","\n","lr:0.000242, alpha:0.000511 @ epoch 8.\n","TL:0.22849918086640536, TA:0.9363784649634366.\n","DL:1.574611223641139, DA:0.6993959324792892\n","9\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.0987\n","Epoch 1 train accuracy: 0.7115\n","Epoch 1 dev loss: 1.4228\n","Epoch 1 dev accuracy: 0.6351\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.5102\n","Epoch 2 train accuracy: 0.8602\n","Epoch 2 dev loss: 1.4855\n","Epoch 2 dev accuracy: 0.6479\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.3900\n","Epoch 3 train accuracy: 0.8917\n","Epoch 3 dev loss: 1.6482\n","Epoch 3 dev accuracy: 0.6391\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3582\n","Epoch 4 train accuracy: 0.9019\n","Epoch 4 dev loss: 1.4900\n","Epoch 4 dev accuracy: 0.6689\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.3156\n","Epoch 5 train accuracy: 0.9132\n","Epoch 5 dev loss: 1.4807\n","Epoch 5 dev accuracy: 0.6603\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2886\n","Epoch 6 train accuracy: 0.9188\n","Epoch 6 dev loss: 1.7683\n","Epoch 6 dev accuracy: 0.6770\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2829\n","Epoch 7 train accuracy: 0.9222\n","Epoch 7 dev loss: 1.5871\n","Epoch 7 dev accuracy: 0.6799\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2562\n","Epoch 8 train accuracy: 0.9289\n","Epoch 8 dev loss: 1.5253\n","Epoch 8 dev accuracy: 0.6922\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2547\n","Epoch 9 train accuracy: 0.9285\n","Epoch 9 dev loss: 1.7230\n","Epoch 9 dev accuracy: 0.6670\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2536\n","Epoch 10 train accuracy: 0.9305\n","Epoch 10 dev loss: 1.8831\n","Epoch 10 dev accuracy: 0.6573\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.2372\n","Epoch 11 train accuracy: 0.9337\n","Epoch 11 dev loss: 1.8957\n","Epoch 11 dev accuracy: 0.6504\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.2119\n","Epoch 12 train accuracy: 0.9384\n","Epoch 12 dev loss: 1.8931\n","Epoch 12 dev accuracy: 0.6480\n","\n"," --------- \n","Epoch: 13\n","\n","Epoch 13 train loss: 0.2103\n","Epoch 13 train accuracy: 0.9404\n","Epoch 13 dev loss: 1.9213\n","Epoch 13 dev accuracy: 0.6551\n","\n"," --------- \n","Epoch: 14\n","\n","Epoch 14 train loss: 0.1943\n","Epoch 14 train accuracy: 0.9439\n","Epoch 14 dev loss: 2.0676\n","Epoch 14 dev accuracy: 0.6676\n","\n"," ######## \n","\n","lr:0.000581, alpha:0.000746 @ epoch 8.\n","TL:0.2562340653607001, TA:0.9288926545330517.\n","DL:1.52531009637322, DA:0.6921797486856192\n","10\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.0797\n","Epoch 1 train accuracy: 0.7176\n","Epoch 1 dev loss: 1.4196\n","Epoch 1 dev accuracy: 0.6373\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.4996\n","Epoch 2 train accuracy: 0.8627\n","Epoch 2 dev loss: 1.4090\n","Epoch 2 dev accuracy: 0.6672\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.3996\n","Epoch 3 train accuracy: 0.8900\n","Epoch 3 dev loss: 1.8460\n","Epoch 3 dev accuracy: 0.6346\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3481\n","Epoch 4 train accuracy: 0.9043\n","Epoch 4 dev loss: 1.5762\n","Epoch 4 dev accuracy: 0.6510\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.3190\n","Epoch 5 train accuracy: 0.9117\n","Epoch 5 dev loss: 1.7280\n","Epoch 5 dev accuracy: 0.6483\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2726\n","Epoch 6 train accuracy: 0.9234\n","Epoch 6 dev loss: 1.7469\n","Epoch 6 dev accuracy: 0.6696\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2798\n","Epoch 7 train accuracy: 0.9223\n","Epoch 7 dev loss: 1.8130\n","Epoch 7 dev accuracy: 0.6467\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2822\n","Epoch 8 train accuracy: 0.9223\n","Epoch 8 dev loss: 1.8074\n","Epoch 8 dev accuracy: 0.6613\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2414\n","Epoch 9 train accuracy: 0.9311\n","Epoch 9 dev loss: 1.7058\n","Epoch 9 dev accuracy: 0.6584\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2596\n","Epoch 10 train accuracy: 0.9296\n","Epoch 10 dev loss: 1.8148\n","Epoch 10 dev accuracy: 0.6616\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.2453\n","Epoch 11 train accuracy: 0.9303\n","Epoch 11 dev loss: 1.8139\n","Epoch 11 dev accuracy: 0.6651\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.2096\n","Epoch 12 train accuracy: 0.9391\n","Epoch 12 dev loss: 2.0064\n","Epoch 12 dev accuracy: 0.6633\n","\n"," ######## \n","\n","lr:0.000675, alpha:0.000519 @ epoch 6.\n","TL:0.27264129792526365, TA:0.923388443098015.\n","DL:1.7469294788266292, DA:0.6695711696450555\n","11\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.0803\n","Epoch 1 train accuracy: 0.7157\n","Epoch 1 dev loss: 1.4087\n","Epoch 1 dev accuracy: 0.6388\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.5036\n","Epoch 2 train accuracy: 0.8630\n","Epoch 2 dev loss: 1.6143\n","Epoch 2 dev accuracy: 0.6422\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.3949\n","Epoch 3 train accuracy: 0.8904\n","Epoch 3 dev loss: 1.4839\n","Epoch 3 dev accuracy: 0.6730\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3626\n","Epoch 4 train accuracy: 0.9008\n","Epoch 4 dev loss: 1.6482\n","Epoch 4 dev accuracy: 0.6468\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.2859\n","Epoch 5 train accuracy: 0.9204\n","Epoch 5 dev loss: 1.5418\n","Epoch 5 dev accuracy: 0.6532\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2805\n","Epoch 6 train accuracy: 0.9211\n","Epoch 6 dev loss: 1.7478\n","Epoch 6 dev accuracy: 0.6627\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2693\n","Epoch 7 train accuracy: 0.9243\n","Epoch 7 dev loss: 1.7188\n","Epoch 7 dev accuracy: 0.6702\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2498\n","Epoch 8 train accuracy: 0.9303\n","Epoch 8 dev loss: 1.7454\n","Epoch 8 dev accuracy: 0.6694\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2355\n","Epoch 9 train accuracy: 0.9332\n","Epoch 9 dev loss: 1.7458\n","Epoch 9 dev accuracy: 0.6679\n","\n"," ######## \n","\n","lr:0.000579, alpha:0.000682 @ epoch 3.\n","TL:0.3948674385622144, TA:0.8903781196844693.\n","DL:1.4839360645265127, DA:0.6729963488172575\n","12\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.2047\n","Epoch 1 train accuracy: 0.6917\n","Epoch 1 dev loss: 1.2871\n","Epoch 1 dev accuracy: 0.6405\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.5017\n","Epoch 2 train accuracy: 0.8656\n","Epoch 2 dev loss: 1.3620\n","Epoch 2 dev accuracy: 0.6568\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.3929\n","Epoch 3 train accuracy: 0.8949\n","Epoch 3 dev loss: 1.3685\n","Epoch 3 dev accuracy: 0.6714\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3322\n","Epoch 4 train accuracy: 0.9097\n","Epoch 4 dev loss: 1.5247\n","Epoch 4 dev accuracy: 0.6779\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.2883\n","Epoch 5 train accuracy: 0.9205\n","Epoch 5 dev loss: 1.4510\n","Epoch 5 dev accuracy: 0.6953\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2661\n","Epoch 6 train accuracy: 0.9264\n","Epoch 6 dev loss: 1.6518\n","Epoch 6 dev accuracy: 0.6682\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2585\n","Epoch 7 train accuracy: 0.9293\n","Epoch 7 dev loss: 1.5676\n","Epoch 7 dev accuracy: 0.6749\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2250\n","Epoch 8 train accuracy: 0.9359\n","Epoch 8 dev loss: 1.5467\n","Epoch 8 dev accuracy: 0.6875\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2334\n","Epoch 9 train accuracy: 0.9345\n","Epoch 9 dev loss: 1.5872\n","Epoch 9 dev accuracy: 0.6815\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2094\n","Epoch 10 train accuracy: 0.9397\n","Epoch 10 dev loss: 1.8288\n","Epoch 10 dev accuracy: 0.6847\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.1950\n","Epoch 11 train accuracy: 0.9444\n","Epoch 11 dev loss: 1.6242\n","Epoch 11 dev accuracy: 0.6956\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.1867\n","Epoch 12 train accuracy: 0.9463\n","Epoch 12 dev loss: 1.6224\n","Epoch 12 dev accuracy: 0.7025\n","\n"," --------- \n","Epoch: 13\n","\n","Epoch 13 train loss: 0.1862\n","Epoch 13 train accuracy: 0.9469\n","Epoch 13 dev loss: 1.7309\n","Epoch 13 dev accuracy: 0.6847\n","\n"," --------- \n","Epoch: 14\n","\n","Epoch 14 train loss: 0.1681\n","Epoch 14 train accuracy: 0.9510\n","Epoch 14 dev loss: 1.8688\n","Epoch 14 dev accuracy: 0.6610\n","\n"," --------- \n","Epoch: 15\n","\n","Epoch 15 train loss: 0.1534\n","Epoch 15 train accuracy: 0.9544\n","Epoch 15 dev loss: 2.2765\n","Epoch 15 dev accuracy: 0.6263\n","\n"," --------- \n","Epoch: 16\n","\n","Epoch 16 train loss: 0.1453\n","Epoch 16 train accuracy: 0.9564\n","Epoch 16 dev loss: 1.8247\n","Epoch 16 dev accuracy: 0.6836\n","\n"," --------- \n","Epoch: 17\n","\n","Epoch 17 train loss: 0.1454\n","Epoch 17 train accuracy: 0.9564\n","Epoch 17 dev loss: 1.9049\n","Epoch 17 dev accuracy: 0.6720\n","\n"," --------- \n","Epoch: 18\n","\n","Epoch 18 train loss: 0.1443\n","Epoch 18 train accuracy: 0.9564\n","Epoch 18 dev loss: 1.8749\n","Epoch 18 dev accuracy: 0.6886\n","\n"," ######## \n","\n","lr:0.000341, alpha:0.00031 @ epoch 12.\n","TL:0.18667483444170405, TA:0.9462790566713399.\n","DL:1.6223895991692288, DA:0.7024951184736418\n","Stats(train_loss=0.21530038304502766, train_accuracy=0.9392935180224953, dev_loss=1.4504363209052689, dev_accuracy=0.7047604500393153, epoch=10, lr=0.000241, alpha=0.000497, max_accuracy=0.7047604500393153)\n","\n","################\n","\n","round: 2\n","max 0.7047604500393153\n","\n","################\n","\n","13\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.2476\n","Epoch 1 train accuracy: 0.6849\n","Epoch 1 dev loss: 1.2716\n","Epoch 1 dev accuracy: 0.6431\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.5055\n","Epoch 2 train accuracy: 0.8660\n","Epoch 2 dev loss: 1.4024\n","Epoch 2 dev accuracy: 0.6557\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.3933\n","Epoch 3 train accuracy: 0.8960\n","Epoch 3 dev loss: 1.4775\n","Epoch 3 dev accuracy: 0.6649\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3104\n","Epoch 4 train accuracy: 0.9143\n","Epoch 4 dev loss: 1.6665\n","Epoch 4 dev accuracy: 0.6741\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.2899\n","Epoch 5 train accuracy: 0.9206\n","Epoch 5 dev loss: 1.5992\n","Epoch 5 dev accuracy: 0.6751\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2668\n","Epoch 6 train accuracy: 0.9255\n","Epoch 6 dev loss: 1.6969\n","Epoch 6 dev accuracy: 0.6683\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2487\n","Epoch 7 train accuracy: 0.9297\n","Epoch 7 dev loss: 1.6640\n","Epoch 7 dev accuracy: 0.6799\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2452\n","Epoch 8 train accuracy: 0.9311\n","Epoch 8 dev loss: 1.5134\n","Epoch 8 dev accuracy: 0.6920\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2048\n","Epoch 9 train accuracy: 0.9421\n","Epoch 9 dev loss: 1.6374\n","Epoch 9 dev accuracy: 0.6713\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2137\n","Epoch 10 train accuracy: 0.9399\n","Epoch 10 dev loss: 1.6204\n","Epoch 10 dev accuracy: 0.6994\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.1976\n","Epoch 11 train accuracy: 0.9432\n","Epoch 11 dev loss: 1.7629\n","Epoch 11 dev accuracy: 0.6865\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.1941\n","Epoch 12 train accuracy: 0.9445\n","Epoch 12 dev loss: 1.6276\n","Epoch 12 dev accuracy: 0.7020\n","\n"," --------- \n","Epoch: 13\n","\n","Epoch 13 train loss: 0.1741\n","Epoch 13 train accuracy: 0.9497\n","Epoch 13 dev loss: 1.6920\n","Epoch 13 dev accuracy: 0.6819\n","\n"," --------- \n","Epoch: 14\n","\n","Epoch 14 train loss: 0.1816\n","Epoch 14 train accuracy: 0.9491\n","Epoch 14 dev loss: 1.9657\n","Epoch 14 dev accuracy: 0.6790\n","\n"," --------- \n","Epoch: 15\n","\n","Epoch 15 train loss: 0.1710\n","Epoch 15 train accuracy: 0.9502\n","Epoch 15 dev loss: 1.7468\n","Epoch 15 dev accuracy: 0.6784\n","\n"," --------- \n","Epoch: 16\n","\n","Epoch 16 train loss: 0.1527\n","Epoch 16 train accuracy: 0.9549\n","Epoch 16 dev loss: 1.7909\n","Epoch 16 dev accuracy: 0.6864\n","\n"," --------- \n","Epoch: 17\n","\n","Epoch 17 train loss: 0.1571\n","Epoch 17 train accuracy: 0.9537\n","Epoch 17 dev loss: 1.8040\n","Epoch 17 dev accuracy: 0.6836\n","\n"," --------- \n","Epoch: 18\n","\n","Epoch 18 train loss: 0.1358\n","Epoch 18 train accuracy: 0.9592\n","Epoch 18 dev loss: 1.6931\n","Epoch 18 dev accuracy: 0.6883\n","\n"," ######## \n","\n","lr:0.000298, alpha:0.000416 @ epoch 12.\n","TL:0.19410322182128828, TA:0.9444935309221778.\n","DL:1.6276241890820986, DA:0.7019626812237675\n","14\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.2400\n","Epoch 1 train accuracy: 0.6850\n","Epoch 1 dev loss: 1.2186\n","Epoch 1 dev accuracy: 0.6528\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.5021\n","Epoch 2 train accuracy: 0.8666\n","Epoch 2 dev loss: 1.3057\n","Epoch 2 dev accuracy: 0.6762\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.4005\n","Epoch 3 train accuracy: 0.8945\n","Epoch 3 dev loss: 1.3816\n","Epoch 3 dev accuracy: 0.6751\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3524\n","Epoch 4 train accuracy: 0.9034\n","Epoch 4 dev loss: 1.4453\n","Epoch 4 dev accuracy: 0.6530\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.2938\n","Epoch 5 train accuracy: 0.9206\n","Epoch 5 dev loss: 1.5686\n","Epoch 5 dev accuracy: 0.6447\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2642\n","Epoch 6 train accuracy: 0.9267\n","Epoch 6 dev loss: 1.4928\n","Epoch 6 dev accuracy: 0.6607\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2417\n","Epoch 7 train accuracy: 0.9331\n","Epoch 7 dev loss: 1.6662\n","Epoch 7 dev accuracy: 0.6803\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2322\n","Epoch 8 train accuracy: 0.9346\n","Epoch 8 dev loss: 1.4472\n","Epoch 8 dev accuracy: 0.6954\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2179\n","Epoch 9 train accuracy: 0.9393\n","Epoch 9 dev loss: 1.4935\n","Epoch 9 dev accuracy: 0.6878\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2190\n","Epoch 10 train accuracy: 0.9383\n","Epoch 10 dev loss: 1.5973\n","Epoch 10 dev accuracy: 0.6866\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.1898\n","Epoch 11 train accuracy: 0.9465\n","Epoch 11 dev loss: 1.5918\n","Epoch 11 dev accuracy: 0.6840\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.2046\n","Epoch 12 train accuracy: 0.9406\n","Epoch 12 dev loss: 1.4935\n","Epoch 12 dev accuracy: 0.6886\n","\n"," --------- \n","Epoch: 13\n","\n","Epoch 13 train loss: 0.1787\n","Epoch 13 train accuracy: 0.9480\n","Epoch 13 dev loss: 1.5555\n","Epoch 13 dev accuracy: 0.6930\n","\n"," --------- \n","Epoch: 14\n","\n","Epoch 14 train loss: 0.1693\n","Epoch 14 train accuracy: 0.9508\n","Epoch 14 dev loss: 1.6734\n","Epoch 14 dev accuracy: 0.6867\n","\n"," ######## \n","\n","lr:0.00029, alpha:0.000571 @ epoch 8.\n","TL:0.23222963602996122, TA:0.9346071051809349.\n","DL:1.447165093780132, DA:0.6953584497245129\n","15\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.3824\n","Epoch 1 train accuracy: 0.6526\n","Epoch 1 dev loss: 1.2245\n","Epoch 1 dev accuracy: 0.6408\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.5565\n","Epoch 2 train accuracy: 0.8543\n","Epoch 2 dev loss: 1.2678\n","Epoch 2 dev accuracy: 0.6568\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.4023\n","Epoch 3 train accuracy: 0.8954\n","Epoch 3 dev loss: 1.3512\n","Epoch 3 dev accuracy: 0.6627\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3328\n","Epoch 4 train accuracy: 0.9115\n","Epoch 4 dev loss: 1.3721\n","Epoch 4 dev accuracy: 0.6592\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.2977\n","Epoch 5 train accuracy: 0.9207\n","Epoch 5 dev loss: 1.4278\n","Epoch 5 dev accuracy: 0.6709\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2746\n","Epoch 6 train accuracy: 0.9261\n","Epoch 6 dev loss: 1.4794\n","Epoch 6 dev accuracy: 0.6725\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2527\n","Epoch 7 train accuracy: 0.9301\n","Epoch 7 dev loss: 1.5057\n","Epoch 7 dev accuracy: 0.6685\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2221\n","Epoch 8 train accuracy: 0.9371\n","Epoch 8 dev loss: 1.5645\n","Epoch 8 dev accuracy: 0.6563\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2067\n","Epoch 9 train accuracy: 0.9423\n","Epoch 9 dev loss: 1.5791\n","Epoch 9 dev accuracy: 0.6693\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2008\n","Epoch 10 train accuracy: 0.9433\n","Epoch 10 dev loss: 1.5218\n","Epoch 10 dev accuracy: 0.6825\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.1904\n","Epoch 11 train accuracy: 0.9461\n","Epoch 11 dev loss: 1.6820\n","Epoch 11 dev accuracy: 0.6729\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.1917\n","Epoch 12 train accuracy: 0.9451\n","Epoch 12 dev loss: 1.6609\n","Epoch 12 dev accuracy: 0.6664\n","\n"," --------- \n","Epoch: 13\n","\n","Epoch 13 train loss: 0.1815\n","Epoch 13 train accuracy: 0.9478\n","Epoch 13 dev loss: 1.5587\n","Epoch 13 dev accuracy: 0.6894\n","\n"," --------- \n","Epoch: 14\n","\n","Epoch 14 train loss: 0.1511\n","Epoch 14 train accuracy: 0.9545\n","Epoch 14 dev loss: 1.6663\n","Epoch 14 dev accuracy: 0.6686\n","\n"," --------- \n","Epoch: 15\n","\n","Epoch 15 train loss: 0.1637\n","Epoch 15 train accuracy: 0.9532\n","Epoch 15 dev loss: 1.6308\n","Epoch 15 dev accuracy: 0.6821\n","\n"," --------- \n","Epoch: 16\n","\n","Epoch 16 train loss: 0.1513\n","Epoch 16 train accuracy: 0.9561\n","Epoch 16 dev loss: 1.5976\n","Epoch 16 dev accuracy: 0.6751\n","\n"," --------- \n","Epoch: 17\n","\n","Epoch 17 train loss: 0.1408\n","Epoch 17 train accuracy: 0.9567\n","Epoch 17 dev loss: 1.5069\n","Epoch 17 dev accuracy: 0.6970\n","\n"," --------- \n","Epoch: 18\n","\n","Epoch 18 train loss: 0.1452\n","Epoch 18 train accuracy: 0.9578\n","Epoch 18 dev loss: 1.6990\n","Epoch 18 dev accuracy: 0.6845\n","\n"," --------- \n","Epoch: 19\n","\n","Epoch 19 train loss: 0.1288\n","Epoch 19 train accuracy: 0.9619\n","Epoch 19 dev loss: 1.6622\n","Epoch 19 dev accuracy: 0.6946\n","\n"," --------- \n","Epoch: 20\n","\n","Epoch 20 train loss: 0.1262\n","Epoch 20 train accuracy: 0.9621\n","Epoch 20 dev loss: 1.8066\n","Epoch 20 dev accuracy: 0.6838\n","\n"," --------- \n","Epoch: 21\n","\n","Epoch 21 train loss: 0.1305\n","Epoch 21 train accuracy: 0.9610\n","Epoch 21 dev loss: 1.6364\n","Epoch 21 dev accuracy: 0.7094\n","\n"," --------- \n","Epoch: 22\n","\n","Epoch 22 train loss: 0.1177\n","Epoch 22 train accuracy: 0.9644\n","Epoch 22 dev loss: 1.7064\n","Epoch 22 dev accuracy: 0.6849\n","\n"," --------- \n","Epoch: 23\n","\n","Epoch 23 train loss: 0.1209\n","Epoch 23 train accuracy: 0.9641\n","Epoch 23 dev loss: 1.7053\n","Epoch 23 dev accuracy: 0.6798\n","\n"," --------- \n","Epoch: 24\n","\n","Epoch 24 train loss: 0.1072\n","Epoch 24 train accuracy: 0.9672\n","Epoch 24 dev loss: 1.8026\n","Epoch 24 dev accuracy: 0.7025\n","\n"," --------- \n","Epoch: 25\n","\n","Epoch 25 train loss: 0.0987\n","Epoch 25 train accuracy: 0.9703\n","Epoch 25 dev loss: 1.6377\n","Epoch 25 dev accuracy: 0.7045\n","\n"," --------- \n","Epoch: 26\n","\n","Epoch 26 train loss: 0.0895\n","Epoch 26 train accuracy: 0.9723\n","Epoch 26 dev loss: 1.8252\n","Epoch 26 dev accuracy: 0.6881\n","\n"," --------- \n","Epoch: 27\n","\n","Epoch 27 train loss: 0.1049\n","Epoch 27 train accuracy: 0.9687\n","Epoch 27 dev loss: 1.9238\n","Epoch 27 dev accuracy: 0.6833\n","\n"," ######## \n","\n","lr:0.00022, alpha:0.000519 @ epoch 21.\n","TL:0.13050361182043949, TA:0.9609997436623077.\n","DL:1.6364245035638487, DA:0.7093572360587586\n","16\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.2892\n","Epoch 1 train accuracy: 0.6736\n","Epoch 1 dev loss: 1.2501\n","Epoch 1 dev accuracy: 0.6420\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.5191\n","Epoch 2 train accuracy: 0.8617\n","Epoch 2 dev loss: 1.3912\n","Epoch 2 dev accuracy: 0.6580\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.3619\n","Epoch 3 train accuracy: 0.9026\n","Epoch 3 dev loss: 1.3195\n","Epoch 3 dev accuracy: 0.6755\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3301\n","Epoch 4 train accuracy: 0.9107\n","Epoch 4 dev loss: 1.4381\n","Epoch 4 dev accuracy: 0.6751\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.2831\n","Epoch 5 train accuracy: 0.9226\n","Epoch 5 dev loss: 1.4919\n","Epoch 5 dev accuracy: 0.6754\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2561\n","Epoch 6 train accuracy: 0.9280\n","Epoch 6 dev loss: 1.4022\n","Epoch 6 dev accuracy: 0.6899\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2478\n","Epoch 7 train accuracy: 0.9307\n","Epoch 7 dev loss: 1.5449\n","Epoch 7 dev accuracy: 0.6790\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2281\n","Epoch 8 train accuracy: 0.9353\n","Epoch 8 dev loss: 1.5738\n","Epoch 8 dev accuracy: 0.6795\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2152\n","Epoch 9 train accuracy: 0.9382\n","Epoch 9 dev loss: 1.4291\n","Epoch 9 dev accuracy: 0.6893\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2139\n","Epoch 10 train accuracy: 0.9408\n","Epoch 10 dev loss: 1.5828\n","Epoch 10 dev accuracy: 0.6878\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.2137\n","Epoch 11 train accuracy: 0.9389\n","Epoch 11 dev loss: 1.6207\n","Epoch 11 dev accuracy: 0.6762\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.1825\n","Epoch 12 train accuracy: 0.9472\n","Epoch 12 dev loss: 1.7150\n","Epoch 12 dev accuracy: 0.6816\n","\n"," ######## \n","\n","lr:0.000288, alpha:0.000431 @ epoch 6.\n","TL:0.25609488533840824, TA:0.9279931454275563.\n","DL:1.4022422830534165, DA:0.68994707223318\n","17\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.2625\n","Epoch 1 train accuracy: 0.6803\n","Epoch 1 dev loss: 1.1577\n","Epoch 1 dev accuracy: 0.6555\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.5022\n","Epoch 2 train accuracy: 0.8663\n","Epoch 2 dev loss: 1.3219\n","Epoch 2 dev accuracy: 0.6751\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.3880\n","Epoch 3 train accuracy: 0.8966\n","Epoch 3 dev loss: 1.4741\n","Epoch 3 dev accuracy: 0.6622\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3481\n","Epoch 4 train accuracy: 0.9061\n","Epoch 4 dev loss: 1.3969\n","Epoch 4 dev accuracy: 0.6950\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.2858\n","Epoch 5 train accuracy: 0.9232\n","Epoch 5 dev loss: 1.4903\n","Epoch 5 dev accuracy: 0.6760\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2626\n","Epoch 6 train accuracy: 0.9272\n","Epoch 6 dev loss: 1.5106\n","Epoch 6 dev accuracy: 0.6846\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2459\n","Epoch 7 train accuracy: 0.9322\n","Epoch 7 dev loss: 1.5883\n","Epoch 7 dev accuracy: 0.6688\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2276\n","Epoch 8 train accuracy: 0.9362\n","Epoch 8 dev loss: 1.6187\n","Epoch 8 dev accuracy: 0.6694\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2240\n","Epoch 9 train accuracy: 0.9374\n","Epoch 9 dev loss: 1.5253\n","Epoch 9 dev accuracy: 0.6680\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2037\n","Epoch 10 train accuracy: 0.9414\n","Epoch 10 dev loss: 1.4348\n","Epoch 10 dev accuracy: 0.7006\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.1972\n","Epoch 11 train accuracy: 0.9440\n","Epoch 11 dev loss: 1.6074\n","Epoch 11 dev accuracy: 0.6918\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.1774\n","Epoch 12 train accuracy: 0.9483\n","Epoch 12 dev loss: 1.4645\n","Epoch 12 dev accuracy: 0.6962\n","\n"," --------- \n","Epoch: 13\n","\n","Epoch 13 train loss: 0.1656\n","Epoch 13 train accuracy: 0.9517\n","Epoch 13 dev loss: 1.6005\n","Epoch 13 dev accuracy: 0.6961\n","\n"," --------- \n","Epoch: 14\n","\n","Epoch 14 train loss: 0.1769\n","Epoch 14 train accuracy: 0.9486\n","Epoch 14 dev loss: 1.5873\n","Epoch 14 dev accuracy: 0.6873\n","\n"," --------- \n","Epoch: 15\n","\n","Epoch 15 train loss: 0.1527\n","Epoch 15 train accuracy: 0.9555\n","Epoch 15 dev loss: 1.6870\n","Epoch 15 dev accuracy: 0.6935\n","\n"," --------- \n","Epoch: 16\n","\n","Epoch 16 train loss: 0.1488\n","Epoch 16 train accuracy: 0.9566\n","Epoch 16 dev loss: 1.6680\n","Epoch 16 dev accuracy: 0.6851\n","\n"," ######## \n","\n","lr:0.000287, alpha:0.000566 @ epoch 10.\n","TL:0.20374596196909744, TA:0.9413920388782553.\n","DL:1.4348027333184454, DA:0.7005554538000853\n","18\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.4171\n","Epoch 1 train accuracy: 0.6457\n","Epoch 1 dev loss: 1.2413\n","Epoch 1 dev accuracy: 0.6486\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.5714\n","Epoch 2 train accuracy: 0.8498\n","Epoch 2 dev loss: 1.3950\n","Epoch 2 dev accuracy: 0.6617\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.3966\n","Epoch 3 train accuracy: 0.8949\n","Epoch 3 dev loss: 1.3427\n","Epoch 3 dev accuracy: 0.6745\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3373\n","Epoch 4 train accuracy: 0.9092\n","Epoch 4 dev loss: 1.3860\n","Epoch 4 dev accuracy: 0.6783\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.2824\n","Epoch 5 train accuracy: 0.9236\n","Epoch 5 dev loss: 1.4404\n","Epoch 5 dev accuracy: 0.6715\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2562\n","Epoch 6 train accuracy: 0.9293\n","Epoch 6 dev loss: 1.4955\n","Epoch 6 dev accuracy: 0.6893\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2449\n","Epoch 7 train accuracy: 0.9322\n","Epoch 7 dev loss: 1.5023\n","Epoch 7 dev accuracy: 0.7043\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2371\n","Epoch 8 train accuracy: 0.9358\n","Epoch 8 dev loss: 1.5038\n","Epoch 8 dev accuracy: 0.6850\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2055\n","Epoch 9 train accuracy: 0.9416\n","Epoch 9 dev loss: 1.6173\n","Epoch 9 dev accuracy: 0.6921\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2057\n","Epoch 10 train accuracy: 0.9418\n","Epoch 10 dev loss: 1.7149\n","Epoch 10 dev accuracy: 0.6604\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.1992\n","Epoch 11 train accuracy: 0.9429\n","Epoch 11 dev loss: 1.6888\n","Epoch 11 dev accuracy: 0.6767\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.1709\n","Epoch 12 train accuracy: 0.9512\n","Epoch 12 dev loss: 1.7368\n","Epoch 12 dev accuracy: 0.6853\n","\n"," --------- \n","Epoch: 13\n","\n","Epoch 13 train loss: 0.1623\n","Epoch 13 train accuracy: 0.9526\n","Epoch 13 dev loss: 1.6206\n","Epoch 13 dev accuracy: 0.6773\n","\n"," ######## \n","\n","lr:0.000214, alpha:0.000475 @ epoch 7.\n","TL:0.244851594692717, TA:0.9321955278850781.\n","DL:1.502331281029848, DA:0.7043465018187623\n","Stats(train_loss=0.13050361182043949, train_accuracy=0.9609997436623077, dev_loss=1.6364245035638487, dev_accuracy=0.7093572360587586, epoch=21, lr=0.00022, alpha=0.000519, max_accuracy=0.7093572360587586)\n"]}]}]}