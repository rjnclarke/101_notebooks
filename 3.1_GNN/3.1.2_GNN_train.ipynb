{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","toc_visible":true,"authorship_tag":"ABX9TyOGfgaV1CxVqCBToOBRy6Zs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Text Classification - Training a GNN\n"],"metadata":{"id":"FBjat8lrIEsf"}},{"cell_type":"markdown","source":["## $\\color{blue}{Sections:}$\n","\n","* Preamble\n","1.   Admin\n","2.   Dataset\n","3.   Model\n","4.   Train - Validate\n","5.   Training Loop"],"metadata":{"id":"O5wHwOnkIKvx"}},{"cell_type":"markdown","source":["## $\\color{blue}{Preamble:}$\n","\n","We now train a GNN in basic PyTorch. The model will look like a GCN. Inference willbe completed in another notebook as the whole graph must be uploaded at once."],"metadata":{"id":"2LYAHLRYIY_C"}},{"cell_type":"markdown","source":["## $\\color{blue}{Admin}$\n","* Install relevant Libraries\n","* Import relevant Libraries"],"metadata":{"id":"udtGvzIPItVT"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vfqeSzgDH285","executionInfo":{"status":"ok","timestamp":1742821938317,"user_tz":-60,"elapsed":4584,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"d611b9eb-23bb-42a0-f03e-5eb8d6924d49"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}],"source":["import torch\n","import pandas as pd\n","from google.colab import drive\n","import numpy as np\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","source":["drive.mount(\"/content/drive\")\n","%cd '/content/drive/MyDrive'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FSXQ2wAfPWoF","executionInfo":{"status":"ok","timestamp":1742821989213,"user_tz":-60,"elapsed":50873,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"03daa62b-8e7e-4ead-ba7f-941f6d7e5b5b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive\n"]}]},{"cell_type":"markdown","source":["## $\\color{blue}{Data}$\n","\n","* Connect to Drive\n","* Load the data\n","* Load adjacency matrices"],"metadata":{"id":"q4Mn5bkiJDE8"}},{"cell_type":"code","source":["path = 'class/datasets/'\n","df_train = pd.read_pickle(path + 'df_train_augmentation_ft')\n","df_dev = pd.read_pickle(path + 'df_dev_augmentation_ft')\n","df_test = pd.read_pickle(path + 'df_test_augmentation_ft')\n"],"metadata":{"id":"gPwH-5O5JHeM","executionInfo":{"status":"ok","timestamp":1742822004612,"user_tz":-60,"elapsed":10905,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["df_train.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2w_7xEV9PbeV","executionInfo":{"status":"ok","timestamp":1742822013397,"user_tz":-60,"elapsed":25,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"9700d38a-bffd-4ddd-8f82-63a61bf73ce3"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['master', 'book_idx', 'chapter_idx', 'content', 'vanilla_embedding.1',\n","       'direct_ft_augmented_embedding', 'ner_responses'],\n","      dtype='object')"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["df1 = df_train[['book_idx', 'chapter_idx', 'content', 'direct_ft_augmented_embedding', 'ner_responses']]\n","df2 = df_dev[['book_idx', 'chapter_idx', 'content', 'direct_ft_augmented_embedding', 'ner_responses']]\n","df_val = pd.concat([df2,df1])"],"metadata":{"id":"QprCXj6GPmcZ","executionInfo":{"status":"ok","timestamp":1742822015123,"user_tz":-60,"elapsed":18,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["df_val.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WYYjv_PQQBQ7","executionInfo":{"status":"ok","timestamp":1742822018569,"user_tz":-60,"elapsed":16,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"a149d337-2883-4963-e87a-2a5cf10fcbbf"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(21220, 5)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["path = 'class/tensors/adj_{}.pt'\n","\n","# train\n","# train_people = torch.load(path.format('train_people'))\n","# train_locations = torch.load(path.format('train_locations'))\n","train_entities = torch.load(path.format('train_augmented_entities'))\n","\n","# # dev\n","# dev_people = torch.load(path.format('dev_people'))\n","# dev_locations = torch.load(path.format('dev_locations'))\n","dev_entities = torch.load(path.format('dev_augmented_entities'))\n","\n","# # val (contains the adjacency matrix for both the training and the development set)\n","# val_people = torch.load(path.format('val_people'))\n","# val_locations = torch.load(path.format('val_locations'))\n","val_entities = torch.load(path.format('val_augmented_entities'))"],"metadata":{"id":"DXpjrdiiJSO0","executionInfo":{"status":"ok","timestamp":1742822055246,"user_tz":-60,"elapsed":35788,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## $\\color{blue}{Dataset:}$"],"metadata":{"id":"Tml4hS4iKIGX"}},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","from copy import deepcopy\n","\n","def sample_neighborhood(A, inds, neighbor_max, branch_max, seed=None):\n","    # Set the random seed for deterministic responses\n","    if seed is not None:\n","        np.random.seed(seed)\n","\n","    np.random.shuffle(A)  # Shuffle the list of adjacency matrices in place\n","    sampled_indices = set(inds)  # Initialize the set of sampled indices\n","\n","    for ind in inds:  # Iterate through node in mini-batch\n","        # print(\"-----\\n ind: \", ind)\n","        break_to_outer = False\n","        neighbors = set()\n","\n","        for adj in A:  # Iterate through all adjacency matrices\n","            if break_to_outer:\n","              break\n","\n","            # Get the indices of all neighbors that idx links to\n","            disclude = set([ind]) | sampled_indices\n","            new_neighbors = [neighbor.item() for neighbor in (adj[ind] > 0).nonzero(as_tuple=True)[0] if neighbor.item() not in disclude]\n","            neighbors.update(new_neighbors)\n","\n","            if len(neighbors) >= neighbor_max:  # Check if we have too many neighbors\n","                # Take a random subset using np.random.choice\n","                neighbors = set(np.random.choice(list(neighbors), neighbor_max, replace=False))\n","\n","\n","            copy_neighbors = deepcopy(neighbors)\n","            for idx in copy_neighbors:\n","                if break_to_outer:\n","                  break\n","\n","                neighbors_neighbors = set()\n","                for adj in A:\n","                    disclude = set([ind,idx]) | sampled_indices | neighbors\n","                    new_neighbors_neighbors = [neighbor.item() for neighbor in (adj[idx] > 0).nonzero(as_tuple=True)[0] if neighbor.item() not in disclude]\n","                    if len(new_neighbors_neighbors) > neighbor_max:\n","                      new_neighbors_neighbors = set(np.random.choice(list(new_neighbors_neighbors), neighbor_max, replace = False))\n","                    neighbors_neighbors.update(new_neighbors_neighbors)\n","                    if len(neighbors) + len(neighbors_neighbors) >= branch_max:\n","                      neighbors_neighbors = set(np.random.choice(list(neighbors_neighbors), branch_max - len(neighbors), replace=False))\n","                      neighbors.update(neighbors_neighbors)\n","                      break_to_outer = True\n","                      break\n","\n","                    neighbors.update(neighbors_neighbors)\n","                    # print(\"New value of neighbors with new neighbors_neighbors: \", neighbors)\n","\n","        sampled_indices.update(neighbors)  # Add new neighbors\n","        # print(f\"____\\n END OF ind {ind}; sampled indeices is now {sampled_indices}\")\n","\n","    sampled_indices = [int(el) for el in sampled_indices]\n","\n","    return sampled_indices\n"],"metadata":{"id":"wvPKQX4B_k9J","executionInfo":{"status":"ok","timestamp":1742822055251,"user_tz":-60,"elapsed":2,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["#check the conditions"],"metadata":{"id":"gQKaLkU1jxOb"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","a = torch.Tensor([\n","    [0,1,0,0,0,0,0,1],\n","    [1,0,0,1,0,1,0,0],\n","    [0,1,0,1,0,0,1,0],\n","    [0,1,0,0,1,0,0,1],\n","    [1,1,0,0,1,0,0,0],\n","    [0,0,1,1,0,1,1,0],\n","    [0,0,0,1,0,0,0,1],\n","    [0,0,0,1,0,1,1,1]\n","])\n","A = []\n","A.append(a)\n","inds = [1,2]\n","nm = 2\n","bm = 4\n","seed=42"],"metadata":{"id":"B5qimRXRPy4k","executionInfo":{"status":"ok","timestamp":1742822055314,"user_tz":-60,"elapsed":61,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","def neighbor_analysis(primary_inds: list[int], inds: list[int], adjacency_matrix: torch.Tensor):\n","    result = []\n","\n","    # Loop over each primary index\n","    for primary_idx in primary_inds:\n","        # Get neighbors for the primary index\n","        neighbors = []\n","        for ind in inds:\n","            # Check if ind is a neighbor of primary_idx\n","            if adjacency_matrix[primary_idx, ind] == 1:\n","                neighbors.append(ind)\n","\n","        # Find neighbors of neighbors (n + 1 neighbors)\n","        neighbors_of_neighbors = []\n","        for neighbor in neighbors:\n","            for ind in inds:\n","                # Check if ind is a neighbor of the current neighbor\n","                if adjacency_matrix[neighbor, ind] == 1 and ind != primary_idx:\n","                    neighbors_of_neighbors.append(ind)\n","\n","        # Remove duplicates for the neighbors of neighbors\n","        neighbors_of_neighbors = list(set(neighbors_of_neighbors))\n","\n","        # Add the tuple to results\n","        result.append((primary_idx, neighbors, neighbors_of_neighbors))\n","\n","    return result"],"metadata":{"id":"pj2hFslUIQu5","executionInfo":{"status":"ok","timestamp":1742822055321,"user_tz":-60,"elapsed":3,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["import random\n","inds = random.sample(list(range(12000)),6)\n","print(inds)\n","#inds = list(range(101,105))\n","sample = sample_neighborhood([train_entities], inds, 4, 16)\n","print(sample)\n","print(len(sample))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3_FRZ_VOZt0D","executionInfo":{"status":"ok","timestamp":1742821432673,"user_tz":-60,"elapsed":16,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"a6ed9d62-faaf-4e8d-a867-c7e6ef5608aa"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["[1811, 4896, 2133, 2576, 6407, 9118]\n","[2946, 6407, 15881, 6154, 12937, 12812, 2576, 3858, 1811, 14356, 17299, 6164, 11671, 12823, 1818, 14620, 9118, 6046, 4896, 12833, 13087, 15396, 8741, 3110, 17572, 6184, 1962, 2095, 6195, 15668, 15669, 4788, 1977, 1851, 6204, 6208, 14718, 6345, 4306, 2133, 12634, 16989, 2142, 13920, 6370, 7402, 17261, 15727, 13047, 6136, 13051, 5885, 638]\n","53\n"]}]},{"cell_type":"code","source":["analysis = neighbor_analysis(inds, sample, train_entities)\n","for item in analysis:\n","  print(item)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"flg7EyIjIm9X","executionInfo":{"status":"ok","timestamp":1742820629530,"user_tz":-60,"elapsed":17,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"d0ff3baa-1e9e-4fd5-d0a7-314c0ade59e9"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["(7782, [16911, 8095, 16549, 17580, 16559, 17594, 18107, 17224, 17992, 8009, 12243, 16989, 17251, 16885, 8699, 8061], [17251, 16549, 17224, 17992, 8009, 17580, 8061, 16559, 16911, 12243, 16885, 8699, 17594, 18107, 16989, 8095])\n","(7572, [], [])\n","(1572, [1426, 533, 1562, 1692, 1569, 11560, 12599, 1605, 1742, 1615, 11856, 11606], [1569, 1025, 1605, 11560, 1742, 1615, 11856, 1426, 1843, 533, 11606, 12599, 1562, 11547, 1692, 3774])\n","(9939, [], [])\n","(2646, [13462, 13466, 12325, 1331, 2642], [1315, 12325, 2642, 1331, 13462, 1305, 13466, 1308, 1310])\n","(4608, [], [])\n"]}]},{"cell_type":"code","source":["# inds = random.sample(list(range(12000)),4)\n","inds = list(range(101,105))\n","sample = sample_neighborhood([train_entities], inds, 16, 64, seed=42)\n","print(sample)\n","print(len(sample))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qK4tP7_j-ZH4","executionInfo":{"status":"ok","timestamp":1742820631478,"user_tz":-60,"elapsed":21,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"23274c42-3672-4c12-ee14-5fa564e2d760"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["[14338, 8, 11785, 13835, 524, 18, 3602, 3611, 3617, 1058, 3621, 2094, 3632, 14384, 14386, 2609, 11828, 14388, 14387, 54, 56, 7231, 13377, 1090, 3651, 65, 71, 14920, 3656, 74, 14922, 3659, 77, 3662, 76, 3664, 80, 1104, 1108, 3669, 3668, 84, 1112, 88, 90, 14430, 14432, 99, 101, 102, 103, 104, 12905, 1127, 1128, 106, 14447, 112, 131, 1156, 1157, 4231, 140, 13968, 13973, 1175, 153, 3738, 3737, 667, 160, 2721, 162, 1187, 3750, 3240, 14510, 178, 1206, 189, 1215, 12479, 194, 12484, 12492, 204, 206, 14038, 13533, 4332, 13038, 3832, 761, 13051, 11010, 1289, 11022, 11023, 787, 11027, 795, 11048, 11054, 11055, 11056, 14642, 1843, 11064, 11070, 11071, 11073, 12099, 12101, 11078, 12102, 11080, 11081, 11082, 3403, 11079, 11085, 846, 11088, 3409, 849, 11090, 11089, 11091, 854, 3415, 1367, 11097, 11098, 11102, 16735, 12126, 11103, 12131, 13669, 11622, 11111, 11116, 12653, 3438, 3439, 11120, 11122, 11125, 12150, 14201, 11130, 3962, 11132, 12155, 12668, 14203, 3458, 3472, 12177, 3987, 11159, 12185, 2972, 3999, 3491, 11172, 3493, 4004, 14764, 1454, 1460, 12214, 11203, 3524, 12232, 14284, 1485, 1487, 3536, 14802, 3540, 12244, 12247, 12249, 3547, 14813, 14814, 6112, 4067, 4069, 3563, 11755, 14317, 14320, 4082, 1015, 11775]\n","197\n"]}]},{"cell_type":"code","source":["class GNNDataset(Dataset):\n","  def __init__(self, H, A, labels, meta_indices, neighbor_max=4, branch_max=16, seed=None):\n","    \"\"\"Custom dataset with neighborhood sampling\n","\n","    Args:\n","      H : torch.tensor\n","        input embeddings (n x d)\n","\n","      A : list[torch.tensor]\n","        list of (n x n)\n","\n","      labels : torch.LongTensor\n","        y\n","\n","      meta_indices : torch.LongTensor\n","        index of datapoint to filter validation score\n","\n","      neighbor_max : int\n","        max neighbors for each node in mini-batch\n","\n","      batch_max : int\n","        max size of batch\n","\n","    \"\"\"\n","    # All inits must be tensors\n","    self.H = H.to(device)\n","    self.A = [a.to(device) for a in A]\n","    self.labels = labels.to(device)\n","    self.meta_indices = meta_indices\n","    self.neighbor_max = neighbor_max\n","    self.branch_max = branch_max\n","    self.seed = seed\n","\n","  def __len__(self):\n","    return len(self.labels)\n","\n","  def __getitem__(self, inds):\n","    # print('\\n####################\\n')\n","    # print('GET ITEM CALLED', 'INDS:', inds)\n","    # Sample neighborhood\n","\n","    # get inds in list\n","    inds = inds.tolist() if torch.is_tensor(inds) else (inds if isinstance(inds,list) else [inds])\n","\n","    # return the required inds\n","    sampled_indices = sample_neighborhood(self.A, inds, self.neighbor_max, self.branch_max,seed=self.seed)\n","\n","    # get the input for the required inds\n","    H_batch = self.H[sampled_indices]\n","\n","    # get the adjacency matrix for the required inds\n","    A_batch = [self.A[k][sampled_indices][:, sampled_indices] for k in range(len(self.A))]\n","\n","    # get the labels for the required inds\n","    labels_batch = self.labels[sampled_indices]\n","\n","    # get meta indices\n","    index_batch = self.meta_indices[sampled_indices]\n","\n","    return H_batch, A_batch, labels_batch, index_batch"],"metadata":{"id":"9Rag8OCtKo1r","executionInfo":{"status":"ok","timestamp":1742822055330,"user_tz":-60,"elapsed":2,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["df_train.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EITOrwRGNK7d","executionInfo":{"status":"ok","timestamp":1742820658407,"user_tz":-60,"elapsed":19,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"fbad0141-8e72-4623-bd1a-b4e997b520a8"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['master', 'book_idx', 'chapter_idx', 'content', 'vanilla_embedding.1',\n","       'direct_ft_augmented_embedding', 'ner_responses'],\n","      dtype='object')"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["H = torch.stack(list(torch.Tensor(df_train['direct_ft_augmented_embedding'])))\n","labels = torch.LongTensor(list(df_train['chapter_idx']))\n","A = []\n","A.append(train_entities)\n","meta_indices = torch.LongTensor(list(range(df_train.shape[0])))"],"metadata":{"id":"NWXfTAo_PxGe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742820728907,"user_tz":-60,"elapsed":131,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"418aeef6-4bd8-4008-f079-bb8f21cfa67c"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-28-4984614584e5>:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n","  H = torch.stack(list(torch.Tensor(df_train['direct_ft_augmented_embedding'])))\n"]}]},{"cell_type":"code","source":["train_dataset = GNNDataset(H, A, labels, meta_indices, neighbor_max=4, branch_max=10)\n","\n","# Prevent dataloader from calling a single index at a time\n","custom_sampler = torch.utils.data.sampler.BatchSampler(\n","    torch.utils.data.sampler.RandomSampler(train_dataset),\n","    batch_size=16,\n","    drop_last=False)\n","\n","\n","train_loader = DataLoader(train_dataset, sampler = custom_sampler)\n"],"metadata":{"id":"AxzIHGzKRr2V","executionInfo":{"status":"ok","timestamp":1742820826325,"user_tz":-60,"elapsed":410,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["# Check batches\n","\n","# Number of batches to inspect\n","num_batches_to_check = 2\n","\n","for batch_idx, (inputs, adjacency, labels, indices) in enumerate(train_loader):\n","    print('\\n##########################\\n')\n","    print(f\"Batch {batch_idx + 1}/{num_batches_to_check}:\")\n","    print('-' * 10)\n","    print(\"Inputs:\")\n","    print(f\"  Type: {type(inputs)}\")\n","    print(f\"  Shape: {inputs.size()}\")\n","    print('-' * 10)\n","    print(\"Adjacency:\")\n","    print(f\"  Type: {type(adjacency)}\")\n","    print(f\"  Shape: {adjacency[0].size()}\")\n","    print('-' * 10)\n","    print(\"Indices:\")\n","    print(f\"  Type: {type(indices)}\")\n","    print(f\"  Shape: {indices.size()}\")\n","    print(indices)\n","    print('-' * 10)\n","    print(\"Labels:\")\n","    print(f\"  Type: {type(labels)}\")\n","    print(f\"  Shape: {labels.size()}\")\n","    print(labels)\n","\n","    # Stop after inspecting the desired number of batches\n","    if batch_idx + 1 >= num_batches_to_check:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"id":"2MW9c2A1WE08","executionInfo":{"status":"error","timestamp":1742820846835,"user_tz":-60,"elapsed":18,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"956513f9-7e7d-4bce-f08f-71ade06fcaa8"},"execution_count":34,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-65059180b7a9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_batches_to_check\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjacency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n##########################\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Batch {batch_idx + 1}/{num_batches_to_check}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-9b42331c22bd>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, inds)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# return the required inds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0msampled_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_neighborhood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbor_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch_max\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# get the input for the required inds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-e8f6d70b6b9e>\u001b[0m in \u001b[0;36msample_neighborhood\u001b[0;34m(A, inds, neighbor_max, branch_max, seed)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# Get the indices of all neighbors that idx links to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mdisclude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0msampled_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mnew_neighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mneighbor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mneighbor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mneighbor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdisclude\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mneighbors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_neighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}]},{"cell_type":"code","source":["H = torch.stack(list(torch.Tensor(df_val['direct_ft_augmented_embedding'])))\n","labels = torch.LongTensor(list(df_val['chapter_idx']))\n","A = []\n","A.append(val_entities)\n","meta_indices = torch.LongTensor(list(range(df_val.shape[0])))"],"metadata":{"id":"k5YldoFcXCHa","executionInfo":{"status":"ok","timestamp":1742820894165,"user_tz":-60,"elapsed":49,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["validation_dataset = GNNDataset(H, A, labels, meta_indices, neighbor_max=4, branch_max=10, seed=42)\n","\n","# Prevent dataloader from calling a single index at a time\n","custom_validation_sampler = torch.utils.data.sampler.BatchSampler(\n","    torch.utils.data.sampler.SequentialSampler(validation_dataset),\n","    batch_size=16,\n","    drop_last=False)\n","\n","\n","train_loader_fixed = DataLoader(validation_dataset, sampler = custom_validation_sampler)"],"metadata":{"id":"dL7DV1_T9yZz","colab":{"base_uri":"https://localhost:8080/","height":402},"executionInfo":{"status":"error","timestamp":1742821598585,"user_tz":-60,"elapsed":55,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"62645d2f-fbb1-4935-dabe-252d57f7b00e"},"execution_count":56,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-56-ffda41cedcc8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidation_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGNNDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbor_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbranch_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Prevent dataloader from calling a single index at a time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m custom_validation_sampler = torch.utils.data.sampler.BatchSampler(\n\u001b[1;32m      5\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-9b42331c22bd>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, H, A, labels, meta_indices, neighbor_max, branch_max, seed)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \"\"\"\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# All inits must be tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}]},{"cell_type":"code","source":["# Check batches\n","\n","# Number of batches to inspect\n","num_batches_to_check = 2\n","\n","for batch_idx, (inputs, adjacency, labels, indices) in enumerate(train_loader_fixed):\n","    print('\\n##########################\\n')\n","    print(f\"Batch {batch_idx + 1}/{num_batches_to_check}:\")\n","    print('-' * 10)\n","    print(\"Inputs:\")\n","    print(f\"  Type: {type(inputs)}\")\n","    print(f\"  Shape: {inputs.size()}\")\n","    print('-' * 10)\n","    print(\"Adjacency:\")\n","    print(f\"  Type: {type(adjacency)}\")\n","    print(f\"  Shape: {adjacency[0].size()}\")\n","    print('-' * 10)\n","    print(\"Indices:\")\n","    print(f\"  Type: {type(indices)}\")\n","    print(f\"  Shape: {indices.size()}\")\n","    print(indices)\n","    print('-' * 10)\n","    print(\"Labels:\")\n","    print(f\"  Type: {type(labels)}\")\n","    print(f\"  Shape: {labels.size()}\")\n","    print(labels)\n","\n","    # Stop after inspecting the desired number of batches\n","    if batch_idx + 1 >= num_batches_to_check:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"PTddMbIyWSej","executionInfo":{"status":"error","timestamp":1742821604788,"user_tz":-60,"elapsed":6,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"cb5e2805-787b-4baf-d93d-40e3498dd049"},"execution_count":57,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'train_loader_fixed' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-57-09af06d5777f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_batches_to_check\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjacency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader_fixed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n##########################\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Batch {batch_idx + 1}/{num_batches_to_check}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_loader_fixed' is not defined"]}]},{"cell_type":"markdown","source":["The dataloader seems to be working correctly, the implementation of custom sampling on all indices at once leads to DataLoaders collate function inserting a new dimension that it will stach against. Because all indices are dealt with at once, there is no stacking.\n","\n","The simple solution will be to simply squeeze the tensors in the training loop. The validation loader eradicates randomness from the process."],"metadata":{"id":"6H8779aHR9wL"}},{"cell_type":"markdown","source":["## $\\color{blue}{Model:}$"],"metadata":{"id":"WrZ8xkIsSnvw"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class GNNLayer(nn.Module):\n","    def __init__(self, in_features, out_features, num_relations=1, dropout=0.4):\n","        super(GNNLayer, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.num_relations = num_relations\n","        self.dropout = dropout\n","\n","        self.T = nn.ParameterList([nn.Parameter(torch.Tensor(in_features, out_features)) for _ in range(num_relations)])\n","        self.E = nn.ParameterList([nn.Parameter(torch.Tensor(in_features, out_features)) for _ in range(num_relations)])\n","\n","        # Batch normalization\n","        self.batch_norm = nn.BatchNorm1d(out_features)\n","\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        for t in self.T:\n","            nn.init.xavier_uniform_(t)\n","        for e in self.E:\n","            nn.init.xavier_uniform_(e)\n","\n","    def forward(self, H, A):\n","        H_out = torch.zeros_like(H)\n","        for k in range(self.num_relations):\n","            messages_projection = A[k].T @ H @ self.E[k]\n","            degrees = A[k].sum(dim=1, keepdim=True)\n","            degrees[degrees == 0] = 1.0\n","            messages_projection /= degrees\n","\n","            self_projection = H @ self.T[k]\n","\n","            # Include skip connection\n","            H_out += F.leaky_relu(self_projection + messages_projection) + H\n","\n","        # Apply batch normalization\n","        H_out = self.batch_norm(H_out)\n","\n","        # Apply dropout\n","        H_out = F.dropout(H_out, p=self.dropout, training=self.training)\n","\n","        return H_out\n","\n","class GNNModel(nn.Module):\n","    def __init__(self, d, h, c, num_relations=1, num_layers=3, dropout=0.4):\n","        super(GNNModel, self).__init__()\n","        self.num_layers = num_layers\n","        self.gnn_layers = nn.ModuleList([GNNLayer(d, d, num_relations, dropout) for _ in range(num_layers)])\n","        self.fc1 = nn.Linear(d, h)\n","        self.batch_norm_fc1 = nn.BatchNorm1d(h)\n","        self.fc2 = nn.Linear(h, c)\n","        self.dropout = dropout\n","\n","    def forward(self, H, A):\n","        for layer in self.gnn_layers:\n","            H = layer(H, A)\n","\n","        H = F.relu(self.batch_norm_fc1(self.fc1(H)))\n","        H = F.dropout(H, p=self.dropout, training=self.training)\n","        Output = self.fc2(H)\n","        return Output\n","\n","    def forward_layer(self, H, A, layer_idx):\n","        \"\"\"Forward pass for a specific layer.\"\"\"\n","        H = self.gnn_layers[layer_idx](H, A)\n","        return H\n"],"metadata":{"id":"64sKwqPvPXmM","executionInfo":{"status":"ok","timestamp":1742824673877,"user_tz":-60,"elapsed":26,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["class GNNModel(nn.Module):\n","   def __init__(self, d, h, c, num_relations=1, num_layers=2):\n","      super(GNNModel, self).__init__()\n","      self.num_layers = num_layers\n","      self.gnn_layers = nn.ModuleList([GNNLayer(d, d) for _ in range(num_layers)])\n","      self.fc1 = nn.Linear(d, h)\n","      self.fc2 = nn.Linear(h, c)\n","\n","   def forward(self, H, A):\n","      for layer in self.gnn_layers:\n","         H = layer(H, A)\n","      # Classification\n","      H = F.relu(self.fc1(H))\n","      Output = self.fc2(H)\n","      return Output"],"metadata":{"id":"yb0uyLqeVPc3","executionInfo":{"status":"ok","timestamp":1742824676649,"user_tz":-60,"elapsed":14,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["import torch.optim as optim\n","\n","d = 768\n","h = 400   # hidden dimension of fully connected layer\n","c = 70   # number of classes\n","num_relations = 1   # number of relationship types\n","\n","# Model, Loss, Optimizer\n","model = GNNModel(d, h, c, num_relations)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n"],"metadata":{"id":"NP8Q4qpiVp-1","executionInfo":{"status":"ok","timestamp":1742824681196,"user_tz":-60,"elapsed":27,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["def count_parameters_per_module(model):\n","    print(\"Module and parameter counts:\")\n","\n","    for name, module in model.named_modules():\n","        # Skip the top-level module (the model itself)\n","        if not isinstance(module, nn.Module) or name == \"\":\n","            continue\n","\n","        param_count = sum(p.numel() for p in module.parameters() if p.requires_grad)\n","\n","        if param_count > 0:  # Only print modules that have parameters\n","            print(f\"{name}: {param_count} parameters\")"],"metadata":{"id":"2zm1UjLWZUBB","executionInfo":{"status":"ok","timestamp":1742822059883,"user_tz":-60,"elapsed":2,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["count_parameters_per_module(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5LxQpA1fYB5e","executionInfo":{"status":"ok","timestamp":1742822060187,"user_tz":-60,"elapsed":25,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"4a2cde42-9aa0-444a-e553-6765d667c529"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Module and parameter counts:\n","gnn_layers: 2362368 parameters\n","gnn_layers.0: 1181184 parameters\n","gnn_layers.0.T: 589824 parameters\n","gnn_layers.0.E: 589824 parameters\n","gnn_layers.0.batch_norm: 1536 parameters\n","gnn_layers.1: 1181184 parameters\n","gnn_layers.1.T: 589824 parameters\n","gnn_layers.1.E: 589824 parameters\n","gnn_layers.1.batch_norm: 1536 parameters\n","fc1: 307600 parameters\n","fc2: 28070 parameters\n"]}]},{"cell_type":"markdown","source":["## $\\color{blue}{Train-Validate:}$"],"metadata":{"id":"5so7JQvHdAvF"}},{"cell_type":"code","source":["def accuracy(outputs, labels):\n","    # argmax to get predicted classes\n","    _, predicted = torch.max(outputs, 1)\n","\n","    # count correct\n","    correct = (predicted == labels).sum().item()\n","\n","    # get average\n","    acc = correct / labels.size(0)  # Total number of samples\n","    return acc"],"metadata":{"id":"EZhnvYLtWbqk","executionInfo":{"status":"ok","timestamp":1742822060191,"user_tz":-60,"elapsed":2,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","def train(model, train_loader, criterion, optimizer):\n","    model.train()\n","    epoch_train_losses = []\n","    epoch_train_accuracy = []\n","\n","    for batch_idx, (H, A, y, indices) in enumerate(train_loader):\n","        print('train_batch;', batch_idx)\n","        optimizer.zero_grad()\n","\n","        H = H.squeeze(0)\n","        A = [a.squeeze(0) for a in A]\n","        y = y.squeeze(0)\n","\n","        out = model(H,A)\n","        train_loss = criterion(out, y)\n","        train_accuracy = accuracy(out, y)\n","\n","\n","        epoch_train_losses.append(train_loss.item())\n","        epoch_train_accuracy.append(train_accuracy)\n","\n","        # Backpropagation and optimization\n","        train_loss.backward()\n","        optimizer.step()\n","\n","    return np.mean(epoch_train_losses), np.mean(epoch_train_accuracy)"],"metadata":{"id":"EftvJs1pdDlQ","executionInfo":{"status":"ok","timestamp":1742823405924,"user_tz":-60,"elapsed":55,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["def validate(model, dev_loader, criterion, threshold=746):\n","    model.eval()\n","    epoch_dev_losses = []\n","    epoch_dev_accuracy = []\n","    pred_holder = []\n","    real_holder = []\n","\n","    with torch.no_grad():\n","        for batch_idx, (H, A, y, indices) in enumerate(dev_loader):\n","            print('val_batch', batch_idx)\n","            H = H.squeeze(0)\n","            A = [a.squeeze(0) for a in A]\n","            y = y.squeeze(0)\n","            indices = indices.squeeze(0)\n","\n","            out = model(H, A)\n","\n","            # Filter out training points\n","            mask = indices < threshold\n","            filtered_out = out[mask]\n","            filtered_y = y[mask]\n","\n","            # Calculate loss and accuracy only on filtered outputs\n","            if filtered_out.size(0) > 0:  # Ensure there are samples to evaluate\n","                dev_loss = criterion(filtered_out, filtered_y)\n","                dev_accuracy = accuracy(filtered_out, filtered_y)\n","\n","                epoch_dev_losses.append(dev_loss.item())\n","                epoch_dev_accuracy.append(dev_accuracy)\n","\n","    # Avoid division by zero if no validation points were processed\n","    return np.mean(epoch_dev_losses), np.mean(epoch_dev_accuracy)"],"metadata":{"id":"FEfSsAmimuEU","executionInfo":{"status":"ok","timestamp":1742823407871,"user_tz":-60,"elapsed":4,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":["## $\\color{blue}{Training:}$"],"metadata":{"id":"dyzhkkmcR29K"}},{"cell_type":"code","source":["from collections import namedtuple\n","Stats = namedtuple('Stats', [\n","    'train_loss',\n","    'train_accuracy',\n","    'dev_loss',\n","    'dev_accuracy',\n","    'epoch',\n","    'lr',\n","    'alpha',\n","    'max_accuracy'\n","])"],"metadata":{"id":"8gM37WLCNRJ6","executionInfo":{"status":"ok","timestamp":1742822065089,"user_tz":-60,"elapsed":5,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["def gen_config(lr_low, lr_high, alpha_low, alpha_high):\n","  np.random.seed()\n","  lr = round(10**float(np.random.uniform(lr_low,lr_high)),6)\n","  alpha = round(10**float(np.random.uniform(alpha_low,alpha_high)),6)\n","  return lr, alpha"],"metadata":{"id":"E_DaEjRlNXl7","executionInfo":{"status":"ok","timestamp":1742822067505,"user_tz":-60,"elapsed":6,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["def gen_ranges( lr, lr_range, alpha, alpha_range):\n","\n","  lr_center = lr\n","  lr_low = lr_center - lr_range/2\n","  lr_high = lr_center + lr_range/2\n","  lr_diff = lr_high - lr_low\n","\n","  alpha_center = alpha\n","  alpha_low = alpha_center - alpha_range/2\n","  alpha_high = alpha_center + alpha_range/2\n","  alpha_diff = alpha_high - alpha_low\n","\n","  return (lr_low, lr_high, alpha_low, alpha_high)"],"metadata":{"id":"4V9JC3PUNbpf","executionInfo":{"status":"ok","timestamp":1742822068511,"user_tz":-60,"elapsed":6,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["def search_stats(results):\n","  best_stats = None\n","  max_dev_accuracy = 0\n","  for i in range(len(results)):\n","    acc = results[i].dev_accuracy\n","    if acc > max_dev_accuracy:\n","      best_stats = results[i]\n","      max_dev_accuracy = acc\n","  return best_stats"],"metadata":{"id":"-sNDLKonNj_Q","executionInfo":{"status":"ok","timestamp":1742822070086,"user_tz":-60,"elapsed":5,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["def tv_run(epochs, model, lr, alpha, max_accuracy, path, verbose = 0):\n","  \"\"\"\n","  Runs a training setup\n","  verbose == 1 - print model results\n","  verbose == 2 -> print epoch and model results\n","  \"\"\"\n","  model = model.to(device)\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=alpha)\n","\n","  # Prepare data loaders\n","  train_loader = DataLoader(train_dataset, sampler = custom_train_sampler)\n","  dev_loader = DataLoader(validation_dataset, sampler = custom_validation_sampler)\n","\n","  # Hold epoch stats\n","  train_losses = []\n","  train_accuracy = []\n","  dev_losses = []\n","  dev_accuracy = []\n","  epoch_holder = []\n","\n","  # Break if no improvement\n","  current_best = 0\n","  no_improvement = 0\n","\n","\n","  # Run epochs\n","  for epoch in range(epochs):\n","\n","    # break out of epochs\n","    if no_improvement >= 6:\n","      break\n","\n","    # call training and validation functions\n","    train_loss, train_acc = train(model, train_loader, criterion, optimizer)\n","    dev_loss, dev_acc = validate(model, dev_loader, criterion)\n","\n","    # Store epoch stats\n","    train_losses.append(train_loss)\n","    train_accuracy.append(train_acc)\n","    dev_losses.append(dev_loss)\n","    dev_accuracy.append(dev_acc)\n","    epoch_holder.append(epoch + 1)\n","\n","    # check for improvement\n","    if dev_acc > current_best:\n","      current_best = dev_acc\n","      no_improvement = 0\n","    else:\n","      no_improvement += 1\n","\n","    # save best model\n","    if dev_acc > max_accuracy:\n","      torch.save(model.state_dict(), path)\n","      max_accuracy = dev_acc\n","\n","\n","    # optionally print epoch results\n","    if verbose == 2:\n","      print(f'\\n --------- \\nEpoch: {epoch + 1}\\n')\n","      print(f'Epoch {epoch + 1} train loss: {train_loss:.4f}')\n","      print(f'Epoch {epoch + 1} train accuracy: {train_acc:.4f}')\n","      print(f'Epoch {epoch + 1} dev loss: {dev_loss:.4f}')\n","      print(f'Epoch {epoch + 1} dev accuracy: {dev_acc:.4f}')\n","\n","      # save best results\n","  max_ind = np.argmax(dev_accuracy)\n","\n","  stats = Stats(\n","      train_losses[max_ind],\n","      train_accuracy[max_ind],\n","      dev_losses[max_ind],\n","      dev_accuracy[max_ind],\n","      epoch_holder[max_ind],\n","      lr, alpha,\n","      max_accuracy\n","  )\n","\n","  # optionally print model results\n","  if verbose in [1,2]:\n","    print('\\n ######## \\n')\n","    print(f'lr:{stats.lr}, alpha:{stats.alpha} @ epoch {stats.epoch}.')\n","    print(f'TL:{stats.train_loss}, TA:{stats.train_accuracy}.')\n","    print(f'DL:{stats.dev_loss}, DA:{stats.dev_accuracy}')\n","\n","  return stats"],"metadata":{"id":"J3PQBpDIYnib","executionInfo":{"status":"ok","timestamp":1742824699667,"user_tz":-60,"elapsed":19,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":["#### $\\color{red}{Sanity-check:}$"],"metadata":{"id":"1C-wVpsPUasU"}},{"cell_type":"code","source":["# model\n","model = GNNModel(d, h, c, num_relations)"],"metadata":{"id":"h9dVcM1VaN6F","executionInfo":{"status":"ok","timestamp":1742822076521,"user_tz":-60,"elapsed":20,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["df_val.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dP-qsz0cRWAt","executionInfo":{"status":"ok","timestamp":1742821753247,"user_tz":-60,"elapsed":25,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"79438b61-df67-4cbf-cbc2-19c8b0593862"},"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['book_idx', 'chapter_idx', 'content', 'direct_ft_augmented_embedding',\n","       'ner_responses'],\n","      dtype='object')"]},"metadata":{},"execution_count":64}]},{"cell_type":"code","source":["torch.Tensor(np.stack(df_train['direct_ft_augmented_embedding'].to_list())).size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CPNAG5UmS28D","executionInfo":{"status":"ok","timestamp":1742822346784,"user_tz":-60,"elapsed":5,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"ca485ca4-5660-4c8a-8d33-ad17cdab8b7c"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([20474, 768])"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["# training loader\n","H_train = torch.Tensor(np.stack(df_train['direct_ft_augmented_embedding'].to_list()))\n","labels_train = torch.LongTensor(list(df_train['chapter_idx']))\n","A_train = []\n","A_train.append(train_entities)\n","train_indices = torch.LongTensor(list(range(df_train.shape[0])))\n","\n","train_dataset = GNNDataset(H_train, A_train, labels_train, train_indices, neighbor_max=4, branch_max=16)\n","\n","# Prevent dataloader from calling a single index at a time\n","custom_train_sampler = torch.utils.data.sampler.BatchSampler(\n","    torch.utils.data.sampler.RandomSampler(train_dataset),\n","    batch_size=64,\n","    drop_last=False)\n","\n","\n","train_loader = DataLoader(train_dataset, sampler = custom_train_sampler)\n","\n","# training loader\n","df1 = df_train[['direct_ft_augmented_embedding', 'chapter_idx']]\n","df2 = df_dev[['direct_ft_augmented_embedding', 'chapter_idx']]\n","df_val = pd.concat([df2, df1])\n","H_val = torch.Tensor(np.stack(df_val['direct_ft_augmented_embedding'].to_list()))\n","labels_val = torch.LongTensor(list(df_val['chapter_idx']))\n","A_val = []\n","A_val.append(val_entities)\n","val_indices = torch.LongTensor(list(range(df_val.shape[0])))\n","\n","\n","\n","validation_dataset = GNNDataset(H_val, A_val, labels_val, val_indices, neighbor_max=4, branch_max=16, seed=42)\n","\n","# Prevent dataloader from calling a single index at a time\n","custom_validation_sampler = torch.utils.data.sampler.BatchSampler(\n","    torch.utils.data.sampler.SequentialSampler(validation_dataset),\n","    batch_size=64,\n","    drop_last=False)\n","\n","\n","dev_loader = DataLoader(validation_dataset, sampler = custom_validation_sampler)\n","\n"],"metadata":{"id":"9pxw5ELFaN8t","executionInfo":{"status":"ok","timestamp":1742824721474,"user_tz":-60,"elapsed":882,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["epochs = 20\n","lr = 0.00018\n","alpha = 0.0006\n","path = \"class/models/GNN_augmented_ft.pt\"\n","max_accuracy = 0.7168"],"metadata":{"id":"Qf0cxrsteYzM","executionInfo":{"status":"ok","timestamp":1742824752417,"user_tz":-60,"elapsed":22,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["tv_run(epochs, model, lr, alpha, max_accuracy, path, verbose = 2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"RPYCNxFAUZPu","executionInfo":{"status":"error","timestamp":1742826527446,"user_tz":-60,"elapsed":1770492,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"8197eea6-1ddb-4aac-f280-e734d8cb89ab","collapsed":true},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["train_batch; 0\n","train_batch; 1\n","train_batch; 2\n","train_batch; 3\n","train_batch; 4\n","train_batch; 5\n","train_batch; 6\n","train_batch; 7\n","train_batch; 8\n","train_batch; 9\n","train_batch; 10\n","train_batch; 11\n","train_batch; 12\n","train_batch; 13\n","train_batch; 14\n","train_batch; 15\n","train_batch; 16\n","train_batch; 17\n","train_batch; 18\n","train_batch; 19\n","train_batch; 20\n","train_batch; 21\n","train_batch; 22\n","train_batch; 23\n","train_batch; 24\n","train_batch; 25\n","train_batch; 26\n","train_batch; 27\n","train_batch; 28\n","train_batch; 29\n","train_batch; 30\n","train_batch; 31\n","train_batch; 32\n","train_batch; 33\n","train_batch; 34\n","train_batch; 35\n","train_batch; 36\n","train_batch; 37\n","train_batch; 38\n","train_batch; 39\n","train_batch; 40\n","train_batch; 41\n","train_batch; 42\n","train_batch; 43\n","train_batch; 44\n","train_batch; 45\n","train_batch; 46\n","train_batch; 47\n","train_batch; 48\n","train_batch; 49\n","train_batch; 50\n","train_batch; 51\n","train_batch; 52\n","train_batch; 53\n","train_batch; 54\n","train_batch; 55\n","train_batch; 56\n","train_batch; 57\n","train_batch; 58\n","train_batch; 59\n","train_batch; 60\n","train_batch; 61\n","train_batch; 62\n","train_batch; 63\n","train_batch; 64\n","train_batch; 65\n","train_batch; 66\n","train_batch; 67\n","train_batch; 68\n","train_batch; 69\n","train_batch; 70\n","train_batch; 71\n","train_batch; 72\n","train_batch; 73\n","train_batch; 74\n","train_batch; 75\n","train_batch; 76\n","train_batch; 77\n","train_batch; 78\n","train_batch; 79\n","train_batch; 80\n","train_batch; 81\n","train_batch; 82\n","train_batch; 83\n","train_batch; 84\n","train_batch; 85\n","train_batch; 86\n","train_batch; 87\n","train_batch; 88\n","train_batch; 89\n","train_batch; 90\n","train_batch; 91\n","train_batch; 92\n","train_batch; 93\n","train_batch; 94\n","train_batch; 95\n","train_batch; 96\n","train_batch; 97\n","train_batch; 98\n","train_batch; 99\n","train_batch; 100\n","train_batch; 101\n","train_batch; 102\n","train_batch; 103\n","train_batch; 104\n","train_batch; 105\n","train_batch; 106\n","train_batch; 107\n","train_batch; 108\n","train_batch; 109\n","train_batch; 110\n","train_batch; 111\n","train_batch; 112\n","train_batch; 113\n","train_batch; 114\n","train_batch; 115\n","train_batch; 116\n","train_batch; 117\n","train_batch; 118\n","train_batch; 119\n","train_batch; 120\n","train_batch; 121\n","train_batch; 122\n","train_batch; 123\n","train_batch; 124\n","train_batch; 125\n","train_batch; 126\n","train_batch; 127\n","train_batch; 128\n","train_batch; 129\n","train_batch; 130\n","train_batch; 131\n","train_batch; 132\n","train_batch; 133\n","train_batch; 134\n","train_batch; 135\n","train_batch; 136\n","train_batch; 137\n","train_batch; 138\n","train_batch; 139\n","train_batch; 140\n","train_batch; 141\n","train_batch; 142\n","train_batch; 143\n","train_batch; 144\n","train_batch; 145\n","train_batch; 146\n","train_batch; 147\n","train_batch; 148\n","train_batch; 149\n","train_batch; 150\n","train_batch; 151\n","train_batch; 152\n","train_batch; 153\n","train_batch; 154\n","train_batch; 155\n","train_batch; 156\n","train_batch; 157\n","train_batch; 158\n","train_batch; 159\n","train_batch; 160\n","train_batch; 161\n","train_batch; 162\n","train_batch; 163\n","train_batch; 164\n","train_batch; 165\n","train_batch; 166\n","train_batch; 167\n","train_batch; 168\n","train_batch; 169\n","train_batch; 170\n","train_batch; 171\n","train_batch; 172\n","train_batch; 173\n","train_batch; 174\n","train_batch; 175\n","train_batch; 176\n","train_batch; 177\n","train_batch; 178\n","train_batch; 179\n","train_batch; 180\n","train_batch; 181\n","train_batch; 182\n","train_batch; 183\n","train_batch; 184\n","train_batch; 185\n","train_batch; 186\n","train_batch; 187\n","train_batch; 188\n","train_batch; 189\n","train_batch; 190\n","train_batch; 191\n","train_batch; 192\n","train_batch; 193\n","train_batch; 194\n","train_batch; 195\n","train_batch; 196\n","train_batch; 197\n","train_batch; 198\n","train_batch; 199\n","train_batch; 200\n","train_batch; 201\n","train_batch; 202\n","train_batch; 203\n","train_batch; 204\n","train_batch; 205\n","train_batch; 206\n","train_batch; 207\n","train_batch; 208\n","train_batch; 209\n","train_batch; 210\n","train_batch; 211\n","train_batch; 212\n","train_batch; 213\n","train_batch; 214\n","train_batch; 215\n","train_batch; 216\n","train_batch; 217\n","train_batch; 218\n","train_batch; 219\n","train_batch; 220\n","train_batch; 221\n","train_batch; 222\n","train_batch; 223\n","train_batch; 224\n","train_batch; 225\n","train_batch; 226\n","train_batch; 227\n","train_batch; 228\n","train_batch; 229\n","train_batch; 230\n","train_batch; 231\n","train_batch; 232\n","train_batch; 233\n","train_batch; 234\n","train_batch; 235\n","train_batch; 236\n","train_batch; 237\n","train_batch; 238\n","train_batch; 239\n","train_batch; 240\n","train_batch; 241\n","train_batch; 242\n","train_batch; 243\n","train_batch; 244\n","train_batch; 245\n","train_batch; 246\n","train_batch; 247\n","train_batch; 248\n","train_batch; 249\n","train_batch; 250\n","train_batch; 251\n","train_batch; 252\n","train_batch; 253\n","train_batch; 254\n","train_batch; 255\n","train_batch; 256\n","train_batch; 257\n","train_batch; 258\n","train_batch; 259\n","train_batch; 260\n","train_batch; 261\n","train_batch; 262\n","train_batch; 263\n","train_batch; 264\n","train_batch; 265\n","train_batch; 266\n","train_batch; 267\n","train_batch; 268\n","train_batch; 269\n","train_batch; 270\n","train_batch; 271\n","train_batch; 272\n","train_batch; 273\n","train_batch; 274\n","train_batch; 275\n","train_batch; 276\n","train_batch; 277\n","train_batch; 278\n","train_batch; 279\n","train_batch; 280\n","train_batch; 281\n","train_batch; 282\n","train_batch; 283\n","train_batch; 284\n","train_batch; 285\n","train_batch; 286\n","train_batch; 287\n","train_batch; 288\n","train_batch; 289\n","train_batch; 290\n","train_batch; 291\n","train_batch; 292\n","train_batch; 293\n","train_batch; 294\n","train_batch; 295\n","train_batch; 296\n","train_batch; 297\n","train_batch; 298\n","train_batch; 299\n","train_batch; 300\n","train_batch; 301\n","train_batch; 302\n","train_batch; 303\n","train_batch; 304\n","train_batch; 305\n","train_batch; 306\n","train_batch; 307\n","train_batch; 308\n","train_batch; 309\n","train_batch; 310\n","train_batch; 311\n","train_batch; 312\n","train_batch; 313\n","train_batch; 314\n","train_batch; 315\n","train_batch; 316\n","train_batch; 317\n","train_batch; 318\n","train_batch; 319\n","val_batch 0\n","val_batch 1\n","val_batch 2\n","val_batch 3\n","val_batch 4\n","val_batch 5\n","val_batch 6\n","val_batch 7\n","val_batch 8\n","val_batch 9\n","val_batch 10\n","val_batch 11\n","val_batch 12\n","val_batch 13\n","val_batch 14\n","val_batch 15\n","val_batch 16\n","val_batch 17\n","val_batch 18\n","val_batch 19\n","val_batch 20\n","val_batch 21\n","val_batch 22\n","val_batch 23\n","val_batch 24\n","val_batch 25\n","val_batch 26\n","val_batch 27\n","val_batch 28\n","val_batch 29\n","val_batch 30\n","val_batch 31\n","val_batch 32\n","val_batch 33\n","val_batch 34\n","val_batch 35\n","val_batch 36\n","val_batch 37\n","val_batch 38\n","val_batch 39\n","val_batch 40\n","val_batch 41\n","val_batch 42\n","val_batch 43\n","val_batch 44\n","val_batch 45\n","val_batch 46\n","val_batch 47\n","val_batch 48\n","val_batch 49\n","val_batch 50\n","val_batch 51\n","val_batch 52\n","val_batch 53\n","val_batch 54\n","val_batch 55\n","val_batch 56\n","val_batch 57\n","val_batch 58\n","val_batch 59\n","val_batch 60\n","val_batch 61\n","val_batch 62\n","val_batch 63\n","val_batch 64\n","val_batch 65\n","val_batch 66\n","val_batch 67\n","val_batch 68\n","val_batch 69\n","val_batch 70\n","val_batch 71\n","val_batch 72\n","val_batch 73\n","val_batch 74\n","val_batch 75\n","val_batch 76\n","val_batch 77\n","val_batch 78\n","val_batch 79\n","val_batch 80\n","val_batch 81\n","val_batch 82\n","val_batch 83\n","val_batch 84\n","val_batch 85\n","val_batch 86\n","val_batch 87\n","val_batch 88\n","val_batch 89\n","val_batch 90\n","val_batch 91\n","val_batch 92\n","val_batch 93\n","val_batch 94\n","val_batch 95\n","val_batch 96\n","val_batch 97\n","val_batch 98\n","val_batch 99\n","val_batch 100\n","val_batch 101\n","val_batch 102\n","val_batch 103\n","val_batch 104\n","val_batch 105\n","val_batch 106\n","val_batch 107\n","val_batch 108\n","val_batch 109\n","val_batch 110\n","val_batch 111\n","val_batch 112\n","val_batch 113\n","val_batch 114\n","val_batch 115\n","val_batch 116\n","val_batch 117\n","val_batch 118\n","val_batch 119\n","val_batch 120\n","val_batch 121\n","val_batch 122\n","val_batch 123\n","val_batch 124\n","val_batch 125\n","val_batch 126\n","val_batch 127\n","val_batch 128\n","val_batch 129\n","val_batch 130\n","val_batch 131\n","val_batch 132\n","val_batch 133\n","val_batch 134\n","val_batch 135\n","val_batch 136\n","val_batch 137\n","val_batch 138\n","val_batch 139\n","val_batch 140\n","val_batch 141\n","val_batch 142\n","val_batch 143\n","val_batch 144\n","val_batch 145\n","val_batch 146\n","val_batch 147\n","val_batch 148\n","val_batch 149\n","val_batch 150\n","val_batch 151\n","val_batch 152\n","val_batch 153\n","val_batch 154\n","val_batch 155\n","val_batch 156\n","val_batch 157\n","val_batch 158\n","val_batch 159\n","val_batch 160\n","val_batch 161\n","val_batch 162\n","val_batch 163\n","val_batch 164\n","val_batch 165\n","val_batch 166\n","val_batch 167\n","val_batch 168\n","val_batch 169\n","val_batch 170\n","val_batch 171\n","val_batch 172\n","val_batch 173\n","val_batch 174\n","val_batch 175\n","val_batch 176\n","val_batch 177\n","val_batch 178\n","val_batch 179\n","val_batch 180\n","val_batch 181\n","val_batch 182\n","val_batch 183\n","val_batch 184\n","val_batch 185\n","val_batch 186\n","val_batch 187\n","val_batch 188\n","val_batch 189\n","val_batch 190\n","val_batch 191\n","val_batch 192\n","val_batch 193\n","val_batch 194\n","val_batch 195\n","val_batch 196\n","val_batch 197\n","val_batch 198\n","val_batch 199\n","val_batch 200\n","val_batch 201\n","val_batch 202\n","val_batch 203\n","val_batch 204\n","val_batch 205\n","val_batch 206\n","val_batch 207\n","val_batch 208\n","val_batch 209\n","val_batch 210\n","val_batch 211\n","val_batch 212\n","val_batch 213\n","val_batch 214\n","val_batch 215\n","val_batch 216\n","val_batch 217\n","val_batch 218\n","val_batch 219\n","val_batch 220\n","val_batch 221\n","val_batch 222\n","val_batch 223\n","val_batch 224\n","val_batch 225\n","val_batch 226\n","val_batch 227\n","val_batch 228\n","val_batch 229\n","val_batch 230\n","val_batch 231\n","val_batch 232\n","val_batch 233\n","val_batch 234\n","val_batch 235\n","val_batch 236\n","val_batch 237\n","val_batch 238\n","val_batch 239\n","val_batch 240\n","val_batch 241\n","val_batch 242\n","val_batch 243\n","val_batch 244\n","val_batch 245\n","val_batch 246\n","val_batch 247\n","val_batch 248\n","val_batch 249\n","val_batch 250\n","val_batch 251\n","val_batch 252\n","val_batch 253\n","val_batch 254\n","val_batch 255\n","val_batch 256\n","val_batch 257\n","val_batch 258\n","val_batch 259\n","val_batch 260\n","val_batch 261\n","val_batch 262\n","val_batch 263\n","val_batch 264\n","val_batch 265\n","val_batch 266\n","val_batch 267\n","val_batch 268\n","val_batch 269\n","val_batch 270\n","val_batch 271\n","val_batch 272\n","val_batch 273\n","val_batch 274\n","val_batch 275\n","val_batch 276\n","val_batch 277\n","val_batch 278\n","val_batch 279\n","val_batch 280\n","val_batch 281\n","val_batch 282\n","val_batch 283\n","val_batch 284\n","val_batch 285\n","val_batch 286\n","val_batch 287\n","val_batch 288\n","val_batch 289\n","val_batch 290\n","val_batch 291\n","val_batch 292\n","val_batch 293\n","val_batch 294\n","val_batch 295\n","val_batch 296\n","val_batch 297\n","val_batch 298\n","val_batch 299\n","val_batch 300\n","val_batch 301\n","val_batch 302\n","val_batch 303\n","val_batch 304\n","val_batch 305\n","val_batch 306\n","val_batch 307\n","val_batch 308\n","val_batch 309\n","val_batch 310\n","val_batch 311\n","val_batch 312\n","val_batch 313\n","val_batch 314\n","val_batch 315\n","val_batch 316\n","val_batch 317\n","val_batch 318\n","val_batch 319\n","val_batch 320\n","val_batch 321\n","val_batch 322\n","val_batch 323\n","val_batch 324\n","val_batch 325\n","val_batch 326\n","val_batch 327\n","val_batch 328\n","val_batch 329\n","val_batch 330\n","val_batch 331\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 0.4460\n","Epoch 1 train accuracy: 0.9398\n","Epoch 1 dev loss: 1.1316\n","Epoch 1 dev accuracy: 0.7110\n","train_batch; 0\n","train_batch; 1\n","train_batch; 2\n","train_batch; 3\n","train_batch; 4\n","train_batch; 5\n","train_batch; 6\n","train_batch; 7\n","train_batch; 8\n","train_batch; 9\n","train_batch; 10\n","train_batch; 11\n","train_batch; 12\n","train_batch; 13\n","train_batch; 14\n","train_batch; 15\n","train_batch; 16\n","train_batch; 17\n","train_batch; 18\n","train_batch; 19\n","train_batch; 20\n","train_batch; 21\n","train_batch; 22\n","train_batch; 23\n","train_batch; 24\n","train_batch; 25\n","train_batch; 26\n","train_batch; 27\n","train_batch; 28\n","train_batch; 29\n","train_batch; 30\n","train_batch; 31\n","train_batch; 32\n","train_batch; 33\n","train_batch; 34\n","train_batch; 35\n","train_batch; 36\n","train_batch; 37\n","train_batch; 38\n","train_batch; 39\n","train_batch; 40\n","train_batch; 41\n","train_batch; 42\n","train_batch; 43\n","train_batch; 44\n","train_batch; 45\n","train_batch; 46\n","train_batch; 47\n","train_batch; 48\n","train_batch; 49\n","train_batch; 50\n","train_batch; 51\n","train_batch; 52\n","train_batch; 53\n","train_batch; 54\n","train_batch; 55\n","train_batch; 56\n","train_batch; 57\n","train_batch; 58\n","train_batch; 59\n","train_batch; 60\n","train_batch; 61\n","train_batch; 62\n","train_batch; 63\n","train_batch; 64\n","train_batch; 65\n","train_batch; 66\n","train_batch; 67\n","train_batch; 68\n","train_batch; 69\n","train_batch; 70\n","train_batch; 71\n","train_batch; 72\n","train_batch; 73\n","train_batch; 74\n","train_batch; 75\n","train_batch; 76\n","train_batch; 77\n","train_batch; 78\n","train_batch; 79\n","train_batch; 80\n","train_batch; 81\n","train_batch; 82\n","train_batch; 83\n","train_batch; 84\n","train_batch; 85\n","train_batch; 86\n","train_batch; 87\n","train_batch; 88\n","train_batch; 89\n","train_batch; 90\n","train_batch; 91\n","train_batch; 92\n","train_batch; 93\n","train_batch; 94\n","train_batch; 95\n","train_batch; 96\n","train_batch; 97\n","train_batch; 98\n","train_batch; 99\n","train_batch; 100\n","train_batch; 101\n","train_batch; 102\n","train_batch; 103\n","train_batch; 104\n","train_batch; 105\n","train_batch; 106\n","train_batch; 107\n","train_batch; 108\n","train_batch; 109\n","train_batch; 110\n","train_batch; 111\n","train_batch; 112\n","train_batch; 113\n","train_batch; 114\n","train_batch; 115\n","train_batch; 116\n","train_batch; 117\n","train_batch; 118\n","train_batch; 119\n","train_batch; 120\n","train_batch; 121\n","train_batch; 122\n","train_batch; 123\n","train_batch; 124\n","train_batch; 125\n","train_batch; 126\n","train_batch; 127\n","train_batch; 128\n","train_batch; 129\n","train_batch; 130\n","train_batch; 131\n","train_batch; 132\n","train_batch; 133\n","train_batch; 134\n","train_batch; 135\n","train_batch; 136\n","train_batch; 137\n","train_batch; 138\n","train_batch; 139\n","train_batch; 140\n","train_batch; 141\n","train_batch; 142\n","train_batch; 143\n","train_batch; 144\n","train_batch; 145\n","train_batch; 146\n","train_batch; 147\n","train_batch; 148\n","train_batch; 149\n","train_batch; 150\n","train_batch; 151\n","train_batch; 152\n","train_batch; 153\n","train_batch; 154\n","train_batch; 155\n","train_batch; 156\n","train_batch; 157\n","train_batch; 158\n","train_batch; 159\n","train_batch; 160\n","train_batch; 161\n","train_batch; 162\n","train_batch; 163\n","train_batch; 164\n","train_batch; 165\n","train_batch; 166\n","train_batch; 167\n","train_batch; 168\n","train_batch; 169\n","train_batch; 170\n","train_batch; 171\n","train_batch; 172\n","train_batch; 173\n","train_batch; 174\n","train_batch; 175\n","train_batch; 176\n","train_batch; 177\n","train_batch; 178\n","train_batch; 179\n","train_batch; 180\n","train_batch; 181\n","train_batch; 182\n","train_batch; 183\n","train_batch; 184\n","train_batch; 185\n","train_batch; 186\n","train_batch; 187\n","train_batch; 188\n","train_batch; 189\n","train_batch; 190\n","train_batch; 191\n","train_batch; 192\n","train_batch; 193\n","train_batch; 194\n","train_batch; 195\n","train_batch; 196\n","train_batch; 197\n","train_batch; 198\n","train_batch; 199\n","train_batch; 200\n","train_batch; 201\n","train_batch; 202\n","train_batch; 203\n","train_batch; 204\n","train_batch; 205\n","train_batch; 206\n","train_batch; 207\n","train_batch; 208\n","train_batch; 209\n","train_batch; 210\n","train_batch; 211\n","train_batch; 212\n","train_batch; 213\n","train_batch; 214\n","train_batch; 215\n","train_batch; 216\n","train_batch; 217\n","train_batch; 218\n","train_batch; 219\n","train_batch; 220\n","train_batch; 221\n","train_batch; 222\n","train_batch; 223\n","train_batch; 224\n","train_batch; 225\n","train_batch; 226\n","train_batch; 227\n","train_batch; 228\n","train_batch; 229\n","train_batch; 230\n","train_batch; 231\n","train_batch; 232\n","train_batch; 233\n","train_batch; 234\n","train_batch; 235\n","train_batch; 236\n","train_batch; 237\n","train_batch; 238\n","train_batch; 239\n","train_batch; 240\n","train_batch; 241\n","train_batch; 242\n","train_batch; 243\n","train_batch; 244\n","train_batch; 245\n","train_batch; 246\n","train_batch; 247\n","train_batch; 248\n","train_batch; 249\n","train_batch; 250\n","train_batch; 251\n","train_batch; 252\n","train_batch; 253\n","train_batch; 254\n","train_batch; 255\n","train_batch; 256\n","train_batch; 257\n","train_batch; 258\n","train_batch; 259\n","train_batch; 260\n","train_batch; 261\n","train_batch; 262\n","train_batch; 263\n","train_batch; 264\n","train_batch; 265\n","train_batch; 266\n","train_batch; 267\n","train_batch; 268\n","train_batch; 269\n","train_batch; 270\n","train_batch; 271\n","train_batch; 272\n","train_batch; 273\n","train_batch; 274\n","train_batch; 275\n","train_batch; 276\n","train_batch; 277\n","train_batch; 278\n","train_batch; 279\n","train_batch; 280\n","train_batch; 281\n","train_batch; 282\n","train_batch; 283\n","train_batch; 284\n","train_batch; 285\n","train_batch; 286\n","train_batch; 287\n","train_batch; 288\n","train_batch; 289\n","train_batch; 290\n","train_batch; 291\n","train_batch; 292\n","train_batch; 293\n","train_batch; 294\n","train_batch; 295\n","train_batch; 296\n","train_batch; 297\n","train_batch; 298\n","train_batch; 299\n","train_batch; 300\n","train_batch; 301\n","train_batch; 302\n","train_batch; 303\n","train_batch; 304\n","train_batch; 305\n","train_batch; 306\n","train_batch; 307\n","train_batch; 308\n","train_batch; 309\n","train_batch; 310\n","train_batch; 311\n","train_batch; 312\n","train_batch; 313\n","train_batch; 314\n","train_batch; 315\n","train_batch; 316\n","train_batch; 317\n","train_batch; 318\n","train_batch; 319\n","val_batch 0\n","val_batch 1\n","val_batch 2\n","val_batch 3\n","val_batch 4\n","val_batch 5\n","val_batch 6\n","val_batch 7\n","val_batch 8\n","val_batch 9\n","val_batch 10\n","val_batch 11\n","val_batch 12\n","val_batch 13\n","val_batch 14\n","val_batch 15\n","val_batch 16\n","val_batch 17\n","val_batch 18\n","val_batch 19\n","val_batch 20\n","val_batch 21\n","val_batch 22\n","val_batch 23\n","val_batch 24\n","val_batch 25\n","val_batch 26\n","val_batch 27\n","val_batch 28\n","val_batch 29\n","val_batch 30\n","val_batch 31\n","val_batch 32\n","val_batch 33\n","val_batch 34\n","val_batch 35\n","val_batch 36\n","val_batch 37\n","val_batch 38\n","val_batch 39\n","val_batch 40\n","val_batch 41\n","val_batch 42\n","val_batch 43\n","val_batch 44\n","val_batch 45\n","val_batch 46\n","val_batch 47\n","val_batch 48\n","val_batch 49\n","val_batch 50\n","val_batch 51\n","val_batch 52\n","val_batch 53\n","val_batch 54\n","val_batch 55\n","val_batch 56\n","val_batch 57\n","val_batch 58\n","val_batch 59\n","val_batch 60\n","val_batch 61\n","val_batch 62\n","val_batch 63\n","val_batch 64\n","val_batch 65\n","val_batch 66\n","val_batch 67\n","val_batch 68\n","val_batch 69\n","val_batch 70\n","val_batch 71\n","val_batch 72\n","val_batch 73\n","val_batch 74\n","val_batch 75\n","val_batch 76\n","val_batch 77\n","val_batch 78\n","val_batch 79\n","val_batch 80\n","val_batch 81\n","val_batch 82\n","val_batch 83\n","val_batch 84\n","val_batch 85\n","val_batch 86\n","val_batch 87\n","val_batch 88\n","val_batch 89\n","val_batch 90\n","val_batch 91\n","val_batch 92\n","val_batch 93\n","val_batch 94\n","val_batch 95\n","val_batch 96\n","val_batch 97\n","val_batch 98\n","val_batch 99\n","val_batch 100\n","val_batch 101\n","val_batch 102\n","val_batch 103\n","val_batch 104\n","val_batch 105\n","val_batch 106\n","val_batch 107\n","val_batch 108\n","val_batch 109\n","val_batch 110\n","val_batch 111\n","val_batch 112\n","val_batch 113\n","val_batch 114\n","val_batch 115\n","val_batch 116\n","val_batch 117\n","val_batch 118\n","val_batch 119\n","val_batch 120\n","val_batch 121\n","val_batch 122\n","val_batch 123\n","val_batch 124\n","val_batch 125\n","val_batch 126\n","val_batch 127\n","val_batch 128\n","val_batch 129\n","val_batch 130\n","val_batch 131\n","val_batch 132\n","val_batch 133\n","val_batch 134\n","val_batch 135\n","val_batch 136\n","val_batch 137\n","val_batch 138\n","val_batch 139\n","val_batch 140\n","val_batch 141\n","val_batch 142\n","val_batch 143\n","val_batch 144\n","val_batch 145\n","val_batch 146\n","val_batch 147\n","val_batch 148\n","val_batch 149\n","val_batch 150\n","val_batch 151\n","val_batch 152\n","val_batch 153\n","val_batch 154\n","val_batch 155\n","val_batch 156\n","val_batch 157\n","val_batch 158\n","val_batch 159\n","val_batch 160\n","val_batch 161\n","val_batch 162\n","val_batch 163\n","val_batch 164\n","val_batch 165\n","val_batch 166\n","val_batch 167\n","val_batch 168\n","val_batch 169\n","val_batch 170\n","val_batch 171\n","val_batch 172\n","val_batch 173\n","val_batch 174\n","val_batch 175\n","val_batch 176\n","val_batch 177\n","val_batch 178\n","val_batch 179\n","val_batch 180\n","val_batch 181\n","val_batch 182\n","val_batch 183\n","val_batch 184\n","val_batch 185\n","val_batch 186\n","val_batch 187\n","val_batch 188\n","val_batch 189\n","val_batch 190\n","val_batch 191\n","val_batch 192\n","val_batch 193\n","val_batch 194\n","val_batch 195\n","val_batch 196\n","val_batch 197\n","val_batch 198\n","val_batch 199\n","val_batch 200\n","val_batch 201\n","val_batch 202\n","val_batch 203\n","val_batch 204\n","val_batch 205\n","val_batch 206\n","val_batch 207\n","val_batch 208\n","val_batch 209\n","val_batch 210\n","val_batch 211\n","val_batch 212\n","val_batch 213\n","val_batch 214\n","val_batch 215\n","val_batch 216\n","val_batch 217\n","val_batch 218\n","val_batch 219\n","val_batch 220\n","val_batch 221\n","val_batch 222\n","val_batch 223\n","val_batch 224\n","val_batch 225\n","val_batch 226\n","val_batch 227\n","val_batch 228\n","val_batch 229\n","val_batch 230\n","val_batch 231\n","val_batch 232\n","val_batch 233\n","val_batch 234\n","val_batch 235\n","val_batch 236\n","val_batch 237\n","val_batch 238\n","val_batch 239\n","val_batch 240\n","val_batch 241\n","val_batch 242\n","val_batch 243\n","val_batch 244\n","val_batch 245\n","val_batch 246\n","val_batch 247\n","val_batch 248\n","val_batch 249\n","val_batch 250\n","val_batch 251\n","val_batch 252\n","val_batch 253\n","val_batch 254\n","val_batch 255\n","val_batch 256\n","val_batch 257\n","val_batch 258\n","val_batch 259\n","val_batch 260\n","val_batch 261\n","val_batch 262\n","val_batch 263\n","val_batch 264\n","val_batch 265\n","val_batch 266\n","val_batch 267\n","val_batch 268\n","val_batch 269\n","val_batch 270\n","val_batch 271\n","val_batch 272\n","val_batch 273\n","val_batch 274\n","val_batch 275\n","val_batch 276\n","val_batch 277\n","val_batch 278\n","val_batch 279\n","val_batch 280\n","val_batch 281\n","val_batch 282\n","val_batch 283\n","val_batch 284\n","val_batch 285\n","val_batch 286\n","val_batch 287\n","val_batch 288\n","val_batch 289\n","val_batch 290\n","val_batch 291\n","val_batch 292\n","val_batch 293\n","val_batch 294\n","val_batch 295\n","val_batch 296\n","val_batch 297\n","val_batch 298\n","val_batch 299\n","val_batch 300\n","val_batch 301\n","val_batch 302\n","val_batch 303\n","val_batch 304\n","val_batch 305\n","val_batch 306\n","val_batch 307\n","val_batch 308\n","val_batch 309\n","val_batch 310\n","val_batch 311\n","val_batch 312\n","val_batch 313\n","val_batch 314\n","val_batch 315\n","val_batch 316\n","val_batch 317\n","val_batch 318\n","val_batch 319\n","val_batch 320\n","val_batch 321\n","val_batch 322\n","val_batch 323\n","val_batch 324\n","val_batch 325\n","val_batch 326\n","val_batch 327\n","val_batch 328\n","val_batch 329\n","val_batch 330\n","val_batch 331\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.0063\n","Epoch 2 train accuracy: 1.0000\n","Epoch 2 dev loss: 1.2548\n","Epoch 2 dev accuracy: 0.7093\n","train_batch; 0\n","train_batch; 1\n","train_batch; 2\n","train_batch; 3\n","train_batch; 4\n","train_batch; 5\n","train_batch; 6\n","train_batch; 7\n","train_batch; 8\n","train_batch; 9\n","train_batch; 10\n","train_batch; 11\n","train_batch; 12\n","train_batch; 13\n","train_batch; 14\n","train_batch; 15\n","train_batch; 16\n","train_batch; 17\n","train_batch; 18\n","train_batch; 19\n","train_batch; 20\n","train_batch; 21\n","train_batch; 22\n","train_batch; 23\n","train_batch; 24\n","train_batch; 25\n","train_batch; 26\n","train_batch; 27\n","train_batch; 28\n","train_batch; 29\n","train_batch; 30\n","train_batch; 31\n","train_batch; 32\n","train_batch; 33\n","train_batch; 34\n","train_batch; 35\n","train_batch; 36\n","train_batch; 37\n","train_batch; 38\n","train_batch; 39\n","train_batch; 40\n","train_batch; 41\n","train_batch; 42\n","train_batch; 43\n","train_batch; 44\n","train_batch; 45\n","train_batch; 46\n","train_batch; 47\n","train_batch; 48\n","train_batch; 49\n","train_batch; 50\n","train_batch; 51\n","train_batch; 52\n","train_batch; 53\n","train_batch; 54\n","train_batch; 55\n","train_batch; 56\n","train_batch; 57\n","train_batch; 58\n","train_batch; 59\n","train_batch; 60\n","train_batch; 61\n","train_batch; 62\n","train_batch; 63\n","train_batch; 64\n","train_batch; 65\n","train_batch; 66\n","train_batch; 67\n","train_batch; 68\n","train_batch; 69\n","train_batch; 70\n","train_batch; 71\n","train_batch; 72\n","train_batch; 73\n","train_batch; 74\n","train_batch; 75\n","train_batch; 76\n","train_batch; 77\n","train_batch; 78\n","train_batch; 79\n","train_batch; 80\n","train_batch; 81\n","train_batch; 82\n","train_batch; 83\n","train_batch; 84\n","train_batch; 85\n","train_batch; 86\n","train_batch; 87\n","train_batch; 88\n","train_batch; 89\n","train_batch; 90\n","train_batch; 91\n","train_batch; 92\n","train_batch; 93\n","train_batch; 94\n","train_batch; 95\n","train_batch; 96\n","train_batch; 97\n","train_batch; 98\n","train_batch; 99\n","train_batch; 100\n","train_batch; 101\n","train_batch; 102\n","train_batch; 103\n","train_batch; 104\n","train_batch; 105\n","train_batch; 106\n","train_batch; 107\n","train_batch; 108\n","train_batch; 109\n","train_batch; 110\n","train_batch; 111\n","train_batch; 112\n","train_batch; 113\n","train_batch; 114\n","train_batch; 115\n","train_batch; 116\n","train_batch; 117\n","train_batch; 118\n","train_batch; 119\n","train_batch; 120\n","train_batch; 121\n","train_batch; 122\n","train_batch; 123\n","train_batch; 124\n","train_batch; 125\n","train_batch; 126\n","train_batch; 127\n","train_batch; 128\n","train_batch; 129\n","train_batch; 130\n","train_batch; 131\n","train_batch; 132\n","train_batch; 133\n","train_batch; 134\n","train_batch; 135\n","train_batch; 136\n","train_batch; 137\n","train_batch; 138\n","train_batch; 139\n","train_batch; 140\n","train_batch; 141\n","train_batch; 142\n","train_batch; 143\n","train_batch; 144\n","train_batch; 145\n","train_batch; 146\n","train_batch; 147\n","train_batch; 148\n","train_batch; 149\n","train_batch; 150\n","train_batch; 151\n","train_batch; 152\n","train_batch; 153\n","train_batch; 154\n","train_batch; 155\n","train_batch; 156\n","train_batch; 157\n","train_batch; 158\n","train_batch; 159\n","train_batch; 160\n","train_batch; 161\n","train_batch; 162\n","train_batch; 163\n","train_batch; 164\n","train_batch; 165\n","train_batch; 166\n","train_batch; 167\n","train_batch; 168\n","train_batch; 169\n","train_batch; 170\n","train_batch; 171\n","train_batch; 172\n","train_batch; 173\n","train_batch; 174\n","train_batch; 175\n","train_batch; 176\n","train_batch; 177\n","train_batch; 178\n","train_batch; 179\n","train_batch; 180\n","train_batch; 181\n","train_batch; 182\n","train_batch; 183\n","train_batch; 184\n","train_batch; 185\n","train_batch; 186\n","train_batch; 187\n","train_batch; 188\n","train_batch; 189\n","train_batch; 190\n","train_batch; 191\n","train_batch; 192\n","train_batch; 193\n","train_batch; 194\n","train_batch; 195\n","train_batch; 196\n","train_batch; 197\n","train_batch; 198\n","train_batch; 199\n","train_batch; 200\n","train_batch; 201\n","train_batch; 202\n","train_batch; 203\n","train_batch; 204\n","train_batch; 205\n","train_batch; 206\n","train_batch; 207\n","train_batch; 208\n","train_batch; 209\n","train_batch; 210\n","train_batch; 211\n","train_batch; 212\n","train_batch; 213\n","train_batch; 214\n","train_batch; 215\n","train_batch; 216\n","train_batch; 217\n","train_batch; 218\n","train_batch; 219\n","train_batch; 220\n","train_batch; 221\n","train_batch; 222\n","train_batch; 223\n","train_batch; 224\n","train_batch; 225\n","train_batch; 226\n","train_batch; 227\n","train_batch; 228\n","train_batch; 229\n","train_batch; 230\n","train_batch; 231\n","train_batch; 232\n","train_batch; 233\n","train_batch; 234\n","train_batch; 235\n","train_batch; 236\n","train_batch; 237\n","train_batch; 238\n","train_batch; 239\n","train_batch; 240\n","train_batch; 241\n","train_batch; 242\n","train_batch; 243\n","train_batch; 244\n","train_batch; 245\n","train_batch; 246\n","train_batch; 247\n","train_batch; 248\n","train_batch; 249\n","train_batch; 250\n","train_batch; 251\n","train_batch; 252\n","train_batch; 253\n","train_batch; 254\n","train_batch; 255\n","train_batch; 256\n","train_batch; 257\n","train_batch; 258\n","train_batch; 259\n","train_batch; 260\n","train_batch; 261\n","train_batch; 262\n","train_batch; 263\n","train_batch; 264\n","train_batch; 265\n","train_batch; 266\n","train_batch; 267\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-ac7af5c6a030>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtv_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-46-8d90258af09f>\u001b[0m in \u001b[0;36mtv_run\u001b[0;34m(epochs, model, lr, alpha, max_accuracy, path, verbose)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# call training and validation functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mdev_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-38-fd842e91a41a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepoch_train_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_batch;'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-9b42331c22bd>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, inds)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# return the required inds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0msampled_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_neighborhood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbor_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch_max\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# get the input for the required inds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-e8f6d70b6b9e>\u001b[0m in \u001b[0;36msample_neighborhood\u001b[0;34m(A, inds, neighbor_max, branch_max, seed)\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0madj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0mdisclude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0msampled_indices\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                     \u001b[0mnew_neighbors_neighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mneighbor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mneighbor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mneighbor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdisclude\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_neighbors_neighbors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mneighbor_max\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                       \u001b[0mnew_neighbors_neighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_neighbors_neighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbor_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-e8f6d70b6b9e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0madj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0mdisclude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0msampled_indices\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                     \u001b[0mnew_neighbors_neighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mneighbor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mneighbor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mneighbor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdisclude\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_neighbors_neighbors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mneighbor_max\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                       \u001b[0mnew_neighbors_neighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_neighbors_neighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbor_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["#### $\\color{red}{Run:}$"],"metadata":{"id":"ZBHJn5V8mIDu"}},{"cell_type":"code","source":["\"\"\"\n","Main Admin\n","\"\"\"\n","epochs = 30\n","max_accuracy = 0\n","path = \"class/models/GNN.3.pt\"\n","results = []\n","\n","\"\"\"\n","init random search\n","lr [10^-5 - 10^-1]\n","alpha [10^-5 - 10^-1]\n","bs [8, 32, 128]\n","\"\"\"\n","lr_low = -5\n","lr_high = -3\n","lr_range = lr_high - lr_low\n","\n","alpha_low = -5\n","alpha_high = -3\n","alpha_range = alpha_high - alpha_low\n","\n","d = 768\n","h = 400\n","c = 70\n","num_relations = 1\n","\n","count = 0\n","\n","\"\"\"\n","Hyperparameter Search\n","\"\"\"\n","\n","for i in range(3):\n","  # debug\n","  print(\"\\n################\\n\")\n","  print(f'round: {i}')\n","  # print(f'lr_low{lr_low}, lr_high{lr_high}, lr_range{lr_range}')\n","  # print(f'alpha_low{alpha_low}, lr_high{alpha_high}, lr_range{alpha_range}')\n","  print('max', max_accuracy)\n","  print(\"\\n################\\n\")\n","\n","\n","  for j in range(6):\n","    count += 1\n","    print(count)\n","\n","    # get config\n","    lr, alpha = gen_config(lr_low, lr_high, alpha_low, alpha_high)\n","    # define model\n","    model = GNNModel(d, h, c, num_relations)\n","    model = model.to(device)\n","\n","    # run training\n","    res = tv_run(epochs, model, lr, alpha, max_accuracy, path, verbose = 2)\n","    max_accuracy = res.max_accuracy\n","    results.append(res)\n","\n","  # get best result of the round or even so far\n","  stats = search_stats(results)\n","\n","\n","  print(stats) # debug\n","\n","  # reconfigure the new hypers\n","  lr = np.log10(stats.lr)\n","  lr_range = lr_range / 3\n","\n","  alpha = np.log10(stats.alpha)\n","  alpha_range = alpha_range / 3\n","\n","  config = gen_ranges(lr, lr_range, alpha, alpha_range)\n","  lr_low, lr_high, alpha_low, alpha_high = config\n","  lr_range = lr_high - lr_low\n","  alpha_range = alpha_high - alpha_low\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fSA1uTMSaN_g","outputId":"f5f2e2c6-ad01-448a-cfa1-b8d0326f4a6c","executionInfo":{"status":"ok","timestamp":1734607266028,"user_tz":-60,"elapsed":14076653,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","################\n","\n","round: 0\n","max 0\n","\n","################\n","\n","1\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 3.0942\n","Epoch 1 train accuracy: 0.2960\n","Epoch 1 dev loss: 1.9864\n","Epoch 1 dev accuracy: 0.4889\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 1.9579\n","Epoch 2 train accuracy: 0.5328\n","Epoch 2 dev loss: 1.4462\n","Epoch 2 dev accuracy: 0.5802\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 1.4683\n","Epoch 3 train accuracy: 0.6356\n","Epoch 3 dev loss: 1.2633\n","Epoch 3 dev accuracy: 0.6220\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 1.1842\n","Epoch 4 train accuracy: 0.6981\n","Epoch 4 dev loss: 1.1948\n","Epoch 4 dev accuracy: 0.6162\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.9946\n","Epoch 5 train accuracy: 0.7451\n","Epoch 5 dev loss: 1.1355\n","Epoch 5 dev accuracy: 0.6359\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.8639\n","Epoch 6 train accuracy: 0.7772\n","Epoch 6 dev loss: 1.1321\n","Epoch 6 dev accuracy: 0.6428\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.7508\n","Epoch 7 train accuracy: 0.8082\n","Epoch 7 dev loss: 1.1236\n","Epoch 7 dev accuracy: 0.6572\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.6669\n","Epoch 8 train accuracy: 0.8302\n","Epoch 8 dev loss: 1.1110\n","Epoch 8 dev accuracy: 0.6605\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.5908\n","Epoch 9 train accuracy: 0.8489\n","Epoch 9 dev loss: 1.1012\n","Epoch 9 dev accuracy: 0.6677\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.5328\n","Epoch 10 train accuracy: 0.8665\n","Epoch 10 dev loss: 1.1260\n","Epoch 10 dev accuracy: 0.6636\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.4907\n","Epoch 11 train accuracy: 0.8772\n","Epoch 11 dev loss: 1.1310\n","Epoch 11 dev accuracy: 0.6755\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.4461\n","Epoch 12 train accuracy: 0.8888\n","Epoch 12 dev loss: 1.1476\n","Epoch 12 dev accuracy: 0.6722\n","\n"," --------- \n","Epoch: 13\n","\n","Epoch 13 train loss: 0.4078\n","Epoch 13 train accuracy: 0.8987\n","Epoch 13 dev loss: 1.1543\n","Epoch 13 dev accuracy: 0.6725\n","\n"," --------- \n","Epoch: 14\n","\n","Epoch 14 train loss: 0.3802\n","Epoch 14 train accuracy: 0.9059\n","Epoch 14 dev loss: 1.1704\n","Epoch 14 dev accuracy: 0.6802\n","\n"," --------- \n","Epoch: 15\n","\n","Epoch 15 train loss: 0.3710\n","Epoch 15 train accuracy: 0.9066\n","Epoch 15 dev loss: 1.1480\n","Epoch 15 dev accuracy: 0.6832\n","\n"," --------- \n","Epoch: 16\n","\n","Epoch 16 train loss: 0.3394\n","Epoch 16 train accuracy: 0.9151\n","Epoch 16 dev loss: 1.2114\n","Epoch 16 dev accuracy: 0.6834\n","\n"," --------- \n","Epoch: 17\n","\n","Epoch 17 train loss: 0.3261\n","Epoch 17 train accuracy: 0.9182\n","Epoch 17 dev loss: 1.1984\n","Epoch 17 dev accuracy: 0.6848\n","\n"," --------- \n","Epoch: 18\n","\n","Epoch 18 train loss: 0.3012\n","Epoch 18 train accuracy: 0.9264\n","Epoch 18 dev loss: 1.1811\n","Epoch 18 dev accuracy: 0.6819\n","\n"," --------- \n","Epoch: 19\n","\n","Epoch 19 train loss: 0.2859\n","Epoch 19 train accuracy: 0.9285\n","Epoch 19 dev loss: 1.2142\n","Epoch 19 dev accuracy: 0.6800\n","\n"," --------- \n","Epoch: 20\n","\n","Epoch 20 train loss: 0.2720\n","Epoch 20 train accuracy: 0.9312\n","Epoch 20 dev loss: 1.2328\n","Epoch 20 dev accuracy: 0.6872\n","\n"," --------- \n","Epoch: 21\n","\n","Epoch 21 train loss: 0.2661\n","Epoch 21 train accuracy: 0.9318\n","Epoch 21 dev loss: 1.2401\n","Epoch 21 dev accuracy: 0.6840\n","\n"," --------- \n","Epoch: 22\n","\n","Epoch 22 train loss: 0.2511\n","Epoch 22 train accuracy: 0.9352\n","Epoch 22 dev loss: 1.2402\n","Epoch 22 dev accuracy: 0.6872\n","\n"," --------- \n","Epoch: 23\n","\n","Epoch 23 train loss: 0.2388\n","Epoch 23 train accuracy: 0.9384\n","Epoch 23 dev loss: 1.2579\n","Epoch 23 dev accuracy: 0.6905\n","\n"," --------- \n","Epoch: 24\n","\n","Epoch 24 train loss: 0.2300\n","Epoch 24 train accuracy: 0.9408\n","Epoch 24 dev loss: 1.2719\n","Epoch 24 dev accuracy: 0.6878\n","\n"," --------- \n","Epoch: 25\n","\n","Epoch 25 train loss: 0.2242\n","Epoch 25 train accuracy: 0.9415\n","Epoch 25 dev loss: 1.2595\n","Epoch 25 dev accuracy: 0.6953\n","\n"," --------- \n","Epoch: 26\n","\n","Epoch 26 train loss: 0.2219\n","Epoch 26 train accuracy: 0.9419\n","Epoch 26 dev loss: 1.2502\n","Epoch 26 dev accuracy: 0.6939\n","\n"," --------- \n","Epoch: 27\n","\n","Epoch 27 train loss: 0.2189\n","Epoch 27 train accuracy: 0.9428\n","Epoch 27 dev loss: 1.2930\n","Epoch 27 dev accuracy: 0.6884\n","\n"," --------- \n","Epoch: 28\n","\n","Epoch 28 train loss: 0.2071\n","Epoch 28 train accuracy: 0.9447\n","Epoch 28 dev loss: 1.2789\n","Epoch 28 dev accuracy: 0.6919\n","\n"," --------- \n","Epoch: 29\n","\n","Epoch 29 train loss: 0.2043\n","Epoch 29 train accuracy: 0.9461\n","Epoch 29 dev loss: 1.3000\n","Epoch 29 dev accuracy: 0.6937\n","\n"," --------- \n","Epoch: 30\n","\n","Epoch 30 train loss: 0.1899\n","Epoch 30 train accuracy: 0.9500\n","Epoch 30 dev loss: 1.2895\n","Epoch 30 dev accuracy: 0.6888\n","\n"," ######## \n","\n","lr:2.3e-05, alpha:0.00032 @ epoch 25.\n","TL:0.2242259551572303, TA:0.9415472732913018.\n","DL:1.2594871150800835, DA:0.6952763242959102\n","2\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.9401\n","Epoch 1 train accuracy: 0.5367\n","Epoch 1 dev loss: 1.2212\n","Epoch 1 dev accuracy: 0.6270\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.8554\n","Epoch 2 train accuracy: 0.7771\n","Epoch 2 dev loss: 1.1331\n","Epoch 2 dev accuracy: 0.6631\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.5638\n","Epoch 3 train accuracy: 0.8554\n","Epoch 3 dev loss: 1.1495\n","Epoch 3 dev accuracy: 0.6821\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.4330\n","Epoch 4 train accuracy: 0.8892\n","Epoch 4 dev loss: 1.2402\n","Epoch 4 dev accuracy: 0.6624\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.3588\n","Epoch 5 train accuracy: 0.9071\n","Epoch 5 dev loss: 1.2220\n","Epoch 5 dev accuracy: 0.6748\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.3159\n","Epoch 6 train accuracy: 0.9179\n","Epoch 6 dev loss: 1.2868\n","Epoch 6 dev accuracy: 0.6750\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2898\n","Epoch 7 train accuracy: 0.9239\n","Epoch 7 dev loss: 1.3394\n","Epoch 7 dev accuracy: 0.6763\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2568\n","Epoch 8 train accuracy: 0.9316\n","Epoch 8 dev loss: 1.3218\n","Epoch 8 dev accuracy: 0.6798\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2265\n","Epoch 9 train accuracy: 0.9392\n","Epoch 9 dev loss: 1.3334\n","Epoch 9 dev accuracy: 0.6819\n","\n"," ######## \n","\n","lr:9.7e-05, alpha:0.000101 @ epoch 3.\n","TL:0.5637732151349386, TA:0.8553985961598309.\n","DL:1.1495108109171057, DA:0.6821067602272038\n","3\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.1653\n","Epoch 1 train accuracy: 0.7004\n","Epoch 1 dev loss: 1.2398\n","Epoch 1 dev accuracy: 0.6480\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.5032\n","Epoch 2 train accuracy: 0.8646\n","Epoch 2 dev loss: 1.3616\n","Epoch 2 dev accuracy: 0.6527\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.3912\n","Epoch 3 train accuracy: 0.8931\n","Epoch 3 dev loss: 1.5623\n","Epoch 3 dev accuracy: 0.6662\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3421\n","Epoch 4 train accuracy: 0.9063\n","Epoch 4 dev loss: 1.7608\n","Epoch 4 dev accuracy: 0.6447\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.2916\n","Epoch 5 train accuracy: 0.9188\n","Epoch 5 dev loss: 1.6280\n","Epoch 5 dev accuracy: 0.6759\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2649\n","Epoch 6 train accuracy: 0.9255\n","Epoch 6 dev loss: 1.6029\n","Epoch 6 dev accuracy: 0.6767\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2589\n","Epoch 7 train accuracy: 0.9273\n","Epoch 7 dev loss: 1.4777\n","Epoch 7 dev accuracy: 0.6794\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2502\n","Epoch 8 train accuracy: 0.9306\n","Epoch 8 dev loss: 1.5713\n","Epoch 8 dev accuracy: 0.6791\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2355\n","Epoch 9 train accuracy: 0.9328\n","Epoch 9 dev loss: 1.6155\n","Epoch 9 dev accuracy: 0.6734\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2181\n","Epoch 10 train accuracy: 0.9380\n","Epoch 10 dev loss: 1.6108\n","Epoch 10 dev accuracy: 0.6890\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.2203\n","Epoch 11 train accuracy: 0.9371\n","Epoch 11 dev loss: 1.5936\n","Epoch 11 dev accuracy: 0.6926\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.2004\n","Epoch 12 train accuracy: 0.9427\n","Epoch 12 dev loss: 1.7295\n","Epoch 12 dev accuracy: 0.6778\n","\n"," --------- \n","Epoch: 13\n","\n","Epoch 13 train loss: 0.1782\n","Epoch 13 train accuracy: 0.9478\n","Epoch 13 dev loss: 1.7743\n","Epoch 13 dev accuracy: 0.6755\n","\n"," --------- \n","Epoch: 14\n","\n","Epoch 14 train loss: 0.1838\n","Epoch 14 train accuracy: 0.9463\n","Epoch 14 dev loss: 1.6837\n","Epoch 14 dev accuracy: 0.6780\n","\n"," --------- \n","Epoch: 15\n","\n","Epoch 15 train loss: 0.1681\n","Epoch 15 train accuracy: 0.9500\n","Epoch 15 dev loss: 1.7118\n","Epoch 15 dev accuracy: 0.6832\n","\n"," --------- \n","Epoch: 16\n","\n","Epoch 16 train loss: 0.1599\n","Epoch 16 train accuracy: 0.9528\n","Epoch 16 dev loss: 1.8113\n","Epoch 16 dev accuracy: 0.6814\n","\n"," --------- \n","Epoch: 17\n","\n","Epoch 17 train loss: 0.1677\n","Epoch 17 train accuracy: 0.9503\n","Epoch 17 dev loss: 1.7781\n","Epoch 17 dev accuracy: 0.6895\n","\n"," ######## \n","\n","lr:0.000399, alpha:6.1e-05 @ epoch 11.\n","TL:0.2202906826570009, TA:0.9371342759399622.\n","DL:1.593559372564102, DA:0.6925895276961629\n","4\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.8349\n","Epoch 1 train accuracy: 0.5535\n","Epoch 1 dev loss: 1.1942\n","Epoch 1 dev accuracy: 0.6354\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.7903\n","Epoch 2 train accuracy: 0.7923\n","Epoch 2 dev loss: 1.1584\n","Epoch 2 dev accuracy: 0.6530\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.5199\n","Epoch 3 train accuracy: 0.8667\n","Epoch 3 dev loss: 1.1562\n","Epoch 3 dev accuracy: 0.6761\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.4103\n","Epoch 4 train accuracy: 0.8938\n","Epoch 4 dev loss: 1.2395\n","Epoch 4 dev accuracy: 0.6811\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.3338\n","Epoch 5 train accuracy: 0.9139\n","Epoch 5 dev loss: 1.4488\n","Epoch 5 dev accuracy: 0.6591\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2889\n","Epoch 6 train accuracy: 0.9238\n","Epoch 6 dev loss: 1.3925\n","Epoch 6 dev accuracy: 0.6669\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2711\n","Epoch 7 train accuracy: 0.9288\n","Epoch 7 dev loss: 1.3608\n","Epoch 7 dev accuracy: 0.6762\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2399\n","Epoch 8 train accuracy: 0.9353\n","Epoch 8 dev loss: 1.4406\n","Epoch 8 dev accuracy: 0.6779\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2356\n","Epoch 9 train accuracy: 0.9368\n","Epoch 9 dev loss: 1.4924\n","Epoch 9 dev accuracy: 0.6655\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2145\n","Epoch 10 train accuracy: 0.9412\n","Epoch 10 dev loss: 1.4623\n","Epoch 10 dev accuracy: 0.6718\n","\n"," ######## \n","\n","lr:0.00011, alpha:3.9e-05 @ epoch 4.\n","TL:0.4102948072006305, TA:0.8938184680749073.\n","DL:1.2395232789302861, DA:0.6811011917857945\n","5\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 2.7073\n","Epoch 1 train accuracy: 0.3804\n","Epoch 1 dev loss: 1.6097\n","Epoch 1 dev accuracy: 0.5503\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 1.5062\n","Epoch 2 train accuracy: 0.6248\n","Epoch 2 dev loss: 1.2745\n","Epoch 2 dev accuracy: 0.6236\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 1.0918\n","Epoch 3 train accuracy: 0.7197\n","Epoch 3 dev loss: 1.1828\n","Epoch 3 dev accuracy: 0.6469\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.8370\n","Epoch 4 train accuracy: 0.7843\n","Epoch 4 dev loss: 1.1536\n","Epoch 4 dev accuracy: 0.6461\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.6890\n","Epoch 5 train accuracy: 0.8233\n","Epoch 5 dev loss: 1.1396\n","Epoch 5 dev accuracy: 0.6607\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.5745\n","Epoch 6 train accuracy: 0.8547\n","Epoch 6 dev loss: 1.1516\n","Epoch 6 dev accuracy: 0.6724\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.4814\n","Epoch 7 train accuracy: 0.8796\n","Epoch 7 dev loss: 1.1797\n","Epoch 7 dev accuracy: 0.6798\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.4317\n","Epoch 8 train accuracy: 0.8922\n","Epoch 8 dev loss: 1.1801\n","Epoch 8 dev accuracy: 0.6752\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.3798\n","Epoch 9 train accuracy: 0.9050\n","Epoch 9 dev loss: 1.2416\n","Epoch 9 dev accuracy: 0.6789\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.3445\n","Epoch 10 train accuracy: 0.9144\n","Epoch 10 dev loss: 1.2647\n","Epoch 10 dev accuracy: 0.6833\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.3089\n","Epoch 11 train accuracy: 0.9227\n","Epoch 11 dev loss: 1.2707\n","Epoch 11 dev accuracy: 0.6865\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.2950\n","Epoch 12 train accuracy: 0.9252\n","Epoch 12 dev loss: 1.2916\n","Epoch 12 dev accuracy: 0.6779\n","\n"," --------- \n","Epoch: 13\n","\n","Epoch 13 train loss: 0.2844\n","Epoch 13 train accuracy: 0.9275\n","Epoch 13 dev loss: 1.3037\n","Epoch 13 dev accuracy: 0.6883\n","\n"," --------- \n","Epoch: 14\n","\n","Epoch 14 train loss: 0.2604\n","Epoch 14 train accuracy: 0.9332\n","Epoch 14 dev loss: 1.2986\n","Epoch 14 dev accuracy: 0.6844\n","\n"," --------- \n","Epoch: 15\n","\n","Epoch 15 train loss: 0.2410\n","Epoch 15 train accuracy: 0.9371\n","Epoch 15 dev loss: 1.2910\n","Epoch 15 dev accuracy: 0.6834\n","\n"," --------- \n","Epoch: 16\n","\n","Epoch 16 train loss: 0.2359\n","Epoch 16 train accuracy: 0.9385\n","Epoch 16 dev loss: 1.3552\n","Epoch 16 dev accuracy: 0.6669\n","\n"," --------- \n","Epoch: 17\n","\n","Epoch 17 train loss: 0.2161\n","Epoch 17 train accuracy: 0.9429\n","Epoch 17 dev loss: 1.3814\n","Epoch 17 dev accuracy: 0.6810\n","\n"," --------- \n","Epoch: 18\n","\n","Epoch 18 train loss: 0.2015\n","Epoch 18 train accuracy: 0.9467\n","Epoch 18 dev loss: 1.3670\n","Epoch 18 dev accuracy: 0.6854\n","\n"," --------- \n","Epoch: 19\n","\n","Epoch 19 train loss: 0.2078\n","Epoch 19 train accuracy: 0.9457\n","Epoch 19 dev loss: 1.3814\n","Epoch 19 dev accuracy: 0.6804\n","\n"," ######## \n","\n","lr:3.7e-05, alpha:4.8e-05 @ epoch 13.\n","TL:0.2843890269945065, TA:0.9275243645661768.\n","DL:1.3036672693891933, DA:0.6882623951513193\n","6\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.1144\n","Epoch 1 train accuracy: 0.7116\n","Epoch 1 dev loss: 1.3024\n","Epoch 1 dev accuracy: 0.6552\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.4912\n","Epoch 2 train accuracy: 0.8675\n","Epoch 2 dev loss: 1.4575\n","Epoch 2 dev accuracy: 0.6469\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.3727\n","Epoch 3 train accuracy: 0.8978\n","Epoch 3 dev loss: 1.4910\n","Epoch 3 dev accuracy: 0.6589\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3243\n","Epoch 4 train accuracy: 0.9116\n","Epoch 4 dev loss: 1.4789\n","Epoch 4 dev accuracy: 0.6771\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.3135\n","Epoch 5 train accuracy: 0.9138\n","Epoch 5 dev loss: 1.5824\n","Epoch 5 dev accuracy: 0.6700\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2646\n","Epoch 6 train accuracy: 0.9267\n","Epoch 6 dev loss: 1.5204\n","Epoch 6 dev accuracy: 0.6803\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2565\n","Epoch 7 train accuracy: 0.9283\n","Epoch 7 dev loss: 1.6987\n","Epoch 7 dev accuracy: 0.6798\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2622\n","Epoch 8 train accuracy: 0.9283\n","Epoch 8 dev loss: 1.7119\n","Epoch 8 dev accuracy: 0.6617\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2173\n","Epoch 9 train accuracy: 0.9379\n","Epoch 9 dev loss: 1.6772\n","Epoch 9 dev accuracy: 0.6782\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2077\n","Epoch 10 train accuracy: 0.9414\n","Epoch 10 dev loss: 1.7192\n","Epoch 10 dev accuracy: 0.6911\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.2192\n","Epoch 11 train accuracy: 0.9371\n","Epoch 11 dev loss: 1.7044\n","Epoch 11 dev accuracy: 0.6877\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.2143\n","Epoch 12 train accuracy: 0.9402\n","Epoch 12 dev loss: 1.6984\n","Epoch 12 dev accuracy: 0.6842\n","\n"," --------- \n","Epoch: 13\n","\n","Epoch 13 train loss: 0.1813\n","Epoch 13 train accuracy: 0.9481\n","Epoch 13 dev loss: 2.0487\n","Epoch 13 dev accuracy: 0.6619\n","\n"," --------- \n","Epoch: 14\n","\n","Epoch 14 train loss: 0.1815\n","Epoch 14 train accuracy: 0.9468\n","Epoch 14 dev loss: 1.8211\n","Epoch 14 dev accuracy: 0.6685\n","\n"," --------- \n","Epoch: 15\n","\n","Epoch 15 train loss: 0.1816\n","Epoch 15 train accuracy: 0.9474\n","Epoch 15 dev loss: 1.9287\n","Epoch 15 dev accuracy: 0.7022\n","\n"," --------- \n","Epoch: 16\n","\n","Epoch 16 train loss: 0.1672\n","Epoch 16 train accuracy: 0.9497\n","Epoch 16 dev loss: 1.7998\n","Epoch 16 dev accuracy: 0.6731\n","\n"," --------- \n","Epoch: 17\n","\n","Epoch 17 train loss: 0.1776\n","Epoch 17 train accuracy: 0.9479\n","Epoch 17 dev loss: 1.9615\n","Epoch 17 dev accuracy: 0.6640\n","\n"," --------- \n","Epoch: 18\n","\n","Epoch 18 train loss: 0.1505\n","Epoch 18 train accuracy: 0.9560\n","Epoch 18 dev loss: 1.9240\n","Epoch 18 dev accuracy: 0.6684\n","\n"," --------- \n","Epoch: 19\n","\n","Epoch 19 train loss: 0.1477\n","Epoch 19 train accuracy: 0.9564\n","Epoch 19 dev loss: 1.8301\n","Epoch 19 dev accuracy: 0.6764\n","\n"," --------- \n","Epoch: 20\n","\n","Epoch 20 train loss: 0.1487\n","Epoch 20 train accuracy: 0.9548\n","Epoch 20 dev loss: 1.9489\n","Epoch 20 dev accuracy: 0.6800\n","\n"," --------- \n","Epoch: 21\n","\n","Epoch 21 train loss: 0.1503\n","Epoch 21 train accuracy: 0.9548\n","Epoch 21 dev loss: 2.0900\n","Epoch 21 dev accuracy: 0.6634\n","\n"," ######## \n","\n","lr:0.000458, alpha:0.000525 @ epoch 15.\n","TL:0.181590415709652, TA:0.9473572890572522.\n","DL:1.928712646967596, DA:0.7022279812094488\n","Stats(train_loss=0.181590415709652, train_accuracy=0.9473572890572522, dev_loss=1.928712646967596, dev_accuracy=0.7022279812094488, epoch=15, lr=0.000458, alpha=0.000525, max_accuracy=0.7022279812094488)\n","\n","################\n","\n","round: 1\n","max 0.7022279812094488\n","\n","################\n","\n","7\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.3194\n","Epoch 1 train accuracy: 0.6703\n","Epoch 1 dev loss: 1.1404\n","Epoch 1 dev accuracy: 0.6610\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.5399\n","Epoch 2 train accuracy: 0.8591\n","Epoch 2 dev loss: 1.3131\n","Epoch 2 dev accuracy: 0.6533\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.3890\n","Epoch 3 train accuracy: 0.8972\n","Epoch 3 dev loss: 1.3516\n","Epoch 3 dev accuracy: 0.6646\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3358\n","Epoch 4 train accuracy: 0.9094\n","Epoch 4 dev loss: 1.4531\n","Epoch 4 dev accuracy: 0.6692\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.2864\n","Epoch 5 train accuracy: 0.9218\n","Epoch 5 dev loss: 1.3942\n","Epoch 5 dev accuracy: 0.6784\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2764\n","Epoch 6 train accuracy: 0.9253\n","Epoch 6 dev loss: 1.4010\n","Epoch 6 dev accuracy: 0.6804\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2388\n","Epoch 7 train accuracy: 0.9343\n","Epoch 7 dev loss: 1.4924\n","Epoch 7 dev accuracy: 0.6818\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2246\n","Epoch 8 train accuracy: 0.9370\n","Epoch 8 dev loss: 1.5199\n","Epoch 8 dev accuracy: 0.6732\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2030\n","Epoch 9 train accuracy: 0.9425\n","Epoch 9 dev loss: 1.6010\n","Epoch 9 dev accuracy: 0.6465\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2153\n","Epoch 10 train accuracy: 0.9393\n","Epoch 10 dev loss: 1.4504\n","Epoch 10 dev accuracy: 0.7048\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.1962\n","Epoch 11 train accuracy: 0.9452\n","Epoch 11 dev loss: 1.5872\n","Epoch 11 dev accuracy: 0.6768\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.1941\n","Epoch 12 train accuracy: 0.9459\n","Epoch 12 dev loss: 1.5955\n","Epoch 12 dev accuracy: 0.6729\n","\n"," --------- \n","Epoch: 13\n","\n","Epoch 13 train loss: 0.1770\n","Epoch 13 train accuracy: 0.9482\n","Epoch 13 dev loss: 1.6817\n","Epoch 13 dev accuracy: 0.6952\n","\n"," --------- \n","Epoch: 14\n","\n","Epoch 14 train loss: 0.1747\n","Epoch 14 train accuracy: 0.9504\n","Epoch 14 dev loss: 1.6174\n","Epoch 14 dev accuracy: 0.6901\n","\n"," --------- \n","Epoch: 15\n","\n","Epoch 15 train loss: 0.1670\n","Epoch 15 train accuracy: 0.9517\n","Epoch 15 dev loss: 1.6261\n","Epoch 15 dev accuracy: 0.6860\n","\n"," --------- \n","Epoch: 16\n","\n","Epoch 16 train loss: 0.1590\n","Epoch 16 train accuracy: 0.9538\n","Epoch 16 dev loss: 1.6190\n","Epoch 16 dev accuracy: 0.6759\n","\n"," ######## \n","\n","lr:0.000241, alpha:0.000497 @ epoch 10.\n","TL:0.21530038304502766, TA:0.9392935180224953.\n","DL:1.4504363209052689, DA:0.7047604500393153\n","8\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.3773\n","Epoch 1 train accuracy: 0.6524\n","Epoch 1 dev loss: 1.2380\n","Epoch 1 dev accuracy: 0.6583\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.5338\n","Epoch 2 train accuracy: 0.8594\n","Epoch 2 dev loss: 1.2624\n","Epoch 2 dev accuracy: 0.6814\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.3988\n","Epoch 3 train accuracy: 0.8938\n","Epoch 3 dev loss: 1.4008\n","Epoch 3 dev accuracy: 0.6590\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3214\n","Epoch 4 train accuracy: 0.9138\n","Epoch 4 dev loss: 1.4002\n","Epoch 4 dev accuracy: 0.6838\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.3041\n","Epoch 5 train accuracy: 0.9174\n","Epoch 5 dev loss: 1.4876\n","Epoch 5 dev accuracy: 0.6823\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2720\n","Epoch 6 train accuracy: 0.9254\n","Epoch 6 dev loss: 1.5499\n","Epoch 6 dev accuracy: 0.6945\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2489\n","Epoch 7 train accuracy: 0.9312\n","Epoch 7 dev loss: 1.5029\n","Epoch 7 dev accuracy: 0.6771\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2285\n","Epoch 8 train accuracy: 0.9364\n","Epoch 8 dev loss: 1.5746\n","Epoch 8 dev accuracy: 0.6994\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2107\n","Epoch 9 train accuracy: 0.9415\n","Epoch 9 dev loss: 1.7036\n","Epoch 9 dev accuracy: 0.6693\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2039\n","Epoch 10 train accuracy: 0.9419\n","Epoch 10 dev loss: 1.7601\n","Epoch 10 dev accuracy: 0.6928\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.1906\n","Epoch 11 train accuracy: 0.9461\n","Epoch 11 dev loss: 1.6451\n","Epoch 11 dev accuracy: 0.6776\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.1865\n","Epoch 12 train accuracy: 0.9462\n","Epoch 12 dev loss: 1.6845\n","Epoch 12 dev accuracy: 0.6907\n","\n"," --------- \n","Epoch: 13\n","\n","Epoch 13 train loss: 0.1681\n","Epoch 13 train accuracy: 0.9519\n","Epoch 13 dev loss: 1.7456\n","Epoch 13 dev accuracy: 0.6803\n","\n"," --------- \n","Epoch: 14\n","\n","Epoch 14 train loss: 0.1630\n","Epoch 14 train accuracy: 0.9529\n","Epoch 14 dev loss: 1.6024\n","Epoch 14 dev accuracy: 0.6909\n","\n"," ######## \n","\n","lr:0.000242, alpha:0.000511 @ epoch 8.\n","TL:0.22849918086640536, TA:0.9363784649634366.\n","DL:1.574611223641139, DA:0.6993959324792892\n","9\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.0987\n","Epoch 1 train accuracy: 0.7115\n","Epoch 1 dev loss: 1.4228\n","Epoch 1 dev accuracy: 0.6351\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.5102\n","Epoch 2 train accuracy: 0.8602\n","Epoch 2 dev loss: 1.4855\n","Epoch 2 dev accuracy: 0.6479\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.3900\n","Epoch 3 train accuracy: 0.8917\n","Epoch 3 dev loss: 1.6482\n","Epoch 3 dev accuracy: 0.6391\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3582\n","Epoch 4 train accuracy: 0.9019\n","Epoch 4 dev loss: 1.4900\n","Epoch 4 dev accuracy: 0.6689\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.3156\n","Epoch 5 train accuracy: 0.9132\n","Epoch 5 dev loss: 1.4807\n","Epoch 5 dev accuracy: 0.6603\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2886\n","Epoch 6 train accuracy: 0.9188\n","Epoch 6 dev loss: 1.7683\n","Epoch 6 dev accuracy: 0.6770\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2829\n","Epoch 7 train accuracy: 0.9222\n","Epoch 7 dev loss: 1.5871\n","Epoch 7 dev accuracy: 0.6799\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2562\n","Epoch 8 train accuracy: 0.9289\n","Epoch 8 dev loss: 1.5253\n","Epoch 8 dev accuracy: 0.6922\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2547\n","Epoch 9 train accuracy: 0.9285\n","Epoch 9 dev loss: 1.7230\n","Epoch 9 dev accuracy: 0.6670\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2536\n","Epoch 10 train accuracy: 0.9305\n","Epoch 10 dev loss: 1.8831\n","Epoch 10 dev accuracy: 0.6573\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.2372\n","Epoch 11 train accuracy: 0.9337\n","Epoch 11 dev loss: 1.8957\n","Epoch 11 dev accuracy: 0.6504\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.2119\n","Epoch 12 train accuracy: 0.9384\n","Epoch 12 dev loss: 1.8931\n","Epoch 12 dev accuracy: 0.6480\n","\n"," --------- \n","Epoch: 13\n","\n","Epoch 13 train loss: 0.2103\n","Epoch 13 train accuracy: 0.9404\n","Epoch 13 dev loss: 1.9213\n","Epoch 13 dev accuracy: 0.6551\n","\n"," --------- \n","Epoch: 14\n","\n","Epoch 14 train loss: 0.1943\n","Epoch 14 train accuracy: 0.9439\n","Epoch 14 dev loss: 2.0676\n","Epoch 14 dev accuracy: 0.6676\n","\n"," ######## \n","\n","lr:0.000581, alpha:0.000746 @ epoch 8.\n","TL:0.2562340653607001, TA:0.9288926545330517.\n","DL:1.52531009637322, DA:0.6921797486856192\n","10\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.0797\n","Epoch 1 train accuracy: 0.7176\n","Epoch 1 dev loss: 1.4196\n","Epoch 1 dev accuracy: 0.6373\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.4996\n","Epoch 2 train accuracy: 0.8627\n","Epoch 2 dev loss: 1.4090\n","Epoch 2 dev accuracy: 0.6672\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.3996\n","Epoch 3 train accuracy: 0.8900\n","Epoch 3 dev loss: 1.8460\n","Epoch 3 dev accuracy: 0.6346\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3481\n","Epoch 4 train accuracy: 0.9043\n","Epoch 4 dev loss: 1.5762\n","Epoch 4 dev accuracy: 0.6510\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.3190\n","Epoch 5 train accuracy: 0.9117\n","Epoch 5 dev loss: 1.7280\n","Epoch 5 dev accuracy: 0.6483\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2726\n","Epoch 6 train accuracy: 0.9234\n","Epoch 6 dev loss: 1.7469\n","Epoch 6 dev accuracy: 0.6696\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2798\n","Epoch 7 train accuracy: 0.9223\n","Epoch 7 dev loss: 1.8130\n","Epoch 7 dev accuracy: 0.6467\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2822\n","Epoch 8 train accuracy: 0.9223\n","Epoch 8 dev loss: 1.8074\n","Epoch 8 dev accuracy: 0.6613\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2414\n","Epoch 9 train accuracy: 0.9311\n","Epoch 9 dev loss: 1.7058\n","Epoch 9 dev accuracy: 0.6584\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2596\n","Epoch 10 train accuracy: 0.9296\n","Epoch 10 dev loss: 1.8148\n","Epoch 10 dev accuracy: 0.6616\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.2453\n","Epoch 11 train accuracy: 0.9303\n","Epoch 11 dev loss: 1.8139\n","Epoch 11 dev accuracy: 0.6651\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.2096\n","Epoch 12 train accuracy: 0.9391\n","Epoch 12 dev loss: 2.0064\n","Epoch 12 dev accuracy: 0.6633\n","\n"," ######## \n","\n","lr:0.000675, alpha:0.000519 @ epoch 6.\n","TL:0.27264129792526365, TA:0.923388443098015.\n","DL:1.7469294788266292, DA:0.6695711696450555\n","11\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.0803\n","Epoch 1 train accuracy: 0.7157\n","Epoch 1 dev loss: 1.4087\n","Epoch 1 dev accuracy: 0.6388\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.5036\n","Epoch 2 train accuracy: 0.8630\n","Epoch 2 dev loss: 1.6143\n","Epoch 2 dev accuracy: 0.6422\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.3949\n","Epoch 3 train accuracy: 0.8904\n","Epoch 3 dev loss: 1.4839\n","Epoch 3 dev accuracy: 0.6730\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3626\n","Epoch 4 train accuracy: 0.9008\n","Epoch 4 dev loss: 1.6482\n","Epoch 4 dev accuracy: 0.6468\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.2859\n","Epoch 5 train accuracy: 0.9204\n","Epoch 5 dev loss: 1.5418\n","Epoch 5 dev accuracy: 0.6532\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2805\n","Epoch 6 train accuracy: 0.9211\n","Epoch 6 dev loss: 1.7478\n","Epoch 6 dev accuracy: 0.6627\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2693\n","Epoch 7 train accuracy: 0.9243\n","Epoch 7 dev loss: 1.7188\n","Epoch 7 dev accuracy: 0.6702\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2498\n","Epoch 8 train accuracy: 0.9303\n","Epoch 8 dev loss: 1.7454\n","Epoch 8 dev accuracy: 0.6694\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2355\n","Epoch 9 train accuracy: 0.9332\n","Epoch 9 dev loss: 1.7458\n","Epoch 9 dev accuracy: 0.6679\n","\n"," ######## \n","\n","lr:0.000579, alpha:0.000682 @ epoch 3.\n","TL:0.3948674385622144, TA:0.8903781196844693.\n","DL:1.4839360645265127, DA:0.6729963488172575\n","12\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.2047\n","Epoch 1 train accuracy: 0.6917\n","Epoch 1 dev loss: 1.2871\n","Epoch 1 dev accuracy: 0.6405\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.5017\n","Epoch 2 train accuracy: 0.8656\n","Epoch 2 dev loss: 1.3620\n","Epoch 2 dev accuracy: 0.6568\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.3929\n","Epoch 3 train accuracy: 0.8949\n","Epoch 3 dev loss: 1.3685\n","Epoch 3 dev accuracy: 0.6714\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3322\n","Epoch 4 train accuracy: 0.9097\n","Epoch 4 dev loss: 1.5247\n","Epoch 4 dev accuracy: 0.6779\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.2883\n","Epoch 5 train accuracy: 0.9205\n","Epoch 5 dev loss: 1.4510\n","Epoch 5 dev accuracy: 0.6953\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2661\n","Epoch 6 train accuracy: 0.9264\n","Epoch 6 dev loss: 1.6518\n","Epoch 6 dev accuracy: 0.6682\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2585\n","Epoch 7 train accuracy: 0.9293\n","Epoch 7 dev loss: 1.5676\n","Epoch 7 dev accuracy: 0.6749\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2250\n","Epoch 8 train accuracy: 0.9359\n","Epoch 8 dev loss: 1.5467\n","Epoch 8 dev accuracy: 0.6875\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2334\n","Epoch 9 train accuracy: 0.9345\n","Epoch 9 dev loss: 1.5872\n","Epoch 9 dev accuracy: 0.6815\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2094\n","Epoch 10 train accuracy: 0.9397\n","Epoch 10 dev loss: 1.8288\n","Epoch 10 dev accuracy: 0.6847\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.1950\n","Epoch 11 train accuracy: 0.9444\n","Epoch 11 dev loss: 1.6242\n","Epoch 11 dev accuracy: 0.6956\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.1867\n","Epoch 12 train accuracy: 0.9463\n","Epoch 12 dev loss: 1.6224\n","Epoch 12 dev accuracy: 0.7025\n","\n"," --------- \n","Epoch: 13\n","\n","Epoch 13 train loss: 0.1862\n","Epoch 13 train accuracy: 0.9469\n","Epoch 13 dev loss: 1.7309\n","Epoch 13 dev accuracy: 0.6847\n","\n"," --------- \n","Epoch: 14\n","\n","Epoch 14 train loss: 0.1681\n","Epoch 14 train accuracy: 0.9510\n","Epoch 14 dev loss: 1.8688\n","Epoch 14 dev accuracy: 0.6610\n","\n"," --------- \n","Epoch: 15\n","\n","Epoch 15 train loss: 0.1534\n","Epoch 15 train accuracy: 0.9544\n","Epoch 15 dev loss: 2.2765\n","Epoch 15 dev accuracy: 0.6263\n","\n"," --------- \n","Epoch: 16\n","\n","Epoch 16 train loss: 0.1453\n","Epoch 16 train accuracy: 0.9564\n","Epoch 16 dev loss: 1.8247\n","Epoch 16 dev accuracy: 0.6836\n","\n"," --------- \n","Epoch: 17\n","\n","Epoch 17 train loss: 0.1454\n","Epoch 17 train accuracy: 0.9564\n","Epoch 17 dev loss: 1.9049\n","Epoch 17 dev accuracy: 0.6720\n","\n"," --------- \n","Epoch: 18\n","\n","Epoch 18 train loss: 0.1443\n","Epoch 18 train accuracy: 0.9564\n","Epoch 18 dev loss: 1.8749\n","Epoch 18 dev accuracy: 0.6886\n","\n"," ######## \n","\n","lr:0.000341, alpha:0.00031 @ epoch 12.\n","TL:0.18667483444170405, TA:0.9462790566713399.\n","DL:1.6223895991692288, DA:0.7024951184736418\n","Stats(train_loss=0.21530038304502766, train_accuracy=0.9392935180224953, dev_loss=1.4504363209052689, dev_accuracy=0.7047604500393153, epoch=10, lr=0.000241, alpha=0.000497, max_accuracy=0.7047604500393153)\n","\n","################\n","\n","round: 2\n","max 0.7047604500393153\n","\n","################\n","\n","13\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.2476\n","Epoch 1 train accuracy: 0.6849\n","Epoch 1 dev loss: 1.2716\n","Epoch 1 dev accuracy: 0.6431\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.5055\n","Epoch 2 train accuracy: 0.8660\n","Epoch 2 dev loss: 1.4024\n","Epoch 2 dev accuracy: 0.6557\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.3933\n","Epoch 3 train accuracy: 0.8960\n","Epoch 3 dev loss: 1.4775\n","Epoch 3 dev accuracy: 0.6649\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3104\n","Epoch 4 train accuracy: 0.9143\n","Epoch 4 dev loss: 1.6665\n","Epoch 4 dev accuracy: 0.6741\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.2899\n","Epoch 5 train accuracy: 0.9206\n","Epoch 5 dev loss: 1.5992\n","Epoch 5 dev accuracy: 0.6751\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2668\n","Epoch 6 train accuracy: 0.9255\n","Epoch 6 dev loss: 1.6969\n","Epoch 6 dev accuracy: 0.6683\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2487\n","Epoch 7 train accuracy: 0.9297\n","Epoch 7 dev loss: 1.6640\n","Epoch 7 dev accuracy: 0.6799\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2452\n","Epoch 8 train accuracy: 0.9311\n","Epoch 8 dev loss: 1.5134\n","Epoch 8 dev accuracy: 0.6920\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2048\n","Epoch 9 train accuracy: 0.9421\n","Epoch 9 dev loss: 1.6374\n","Epoch 9 dev accuracy: 0.6713\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2137\n","Epoch 10 train accuracy: 0.9399\n","Epoch 10 dev loss: 1.6204\n","Epoch 10 dev accuracy: 0.6994\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.1976\n","Epoch 11 train accuracy: 0.9432\n","Epoch 11 dev loss: 1.7629\n","Epoch 11 dev accuracy: 0.6865\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.1941\n","Epoch 12 train accuracy: 0.9445\n","Epoch 12 dev loss: 1.6276\n","Epoch 12 dev accuracy: 0.7020\n","\n"," --------- \n","Epoch: 13\n","\n","Epoch 13 train loss: 0.1741\n","Epoch 13 train accuracy: 0.9497\n","Epoch 13 dev loss: 1.6920\n","Epoch 13 dev accuracy: 0.6819\n","\n"," --------- \n","Epoch: 14\n","\n","Epoch 14 train loss: 0.1816\n","Epoch 14 train accuracy: 0.9491\n","Epoch 14 dev loss: 1.9657\n","Epoch 14 dev accuracy: 0.6790\n","\n"," --------- \n","Epoch: 15\n","\n","Epoch 15 train loss: 0.1710\n","Epoch 15 train accuracy: 0.9502\n","Epoch 15 dev loss: 1.7468\n","Epoch 15 dev accuracy: 0.6784\n","\n"," --------- \n","Epoch: 16\n","\n","Epoch 16 train loss: 0.1527\n","Epoch 16 train accuracy: 0.9549\n","Epoch 16 dev loss: 1.7909\n","Epoch 16 dev accuracy: 0.6864\n","\n"," --------- \n","Epoch: 17\n","\n","Epoch 17 train loss: 0.1571\n","Epoch 17 train accuracy: 0.9537\n","Epoch 17 dev loss: 1.8040\n","Epoch 17 dev accuracy: 0.6836\n","\n"," --------- \n","Epoch: 18\n","\n","Epoch 18 train loss: 0.1358\n","Epoch 18 train accuracy: 0.9592\n","Epoch 18 dev loss: 1.6931\n","Epoch 18 dev accuracy: 0.6883\n","\n"," ######## \n","\n","lr:0.000298, alpha:0.000416 @ epoch 12.\n","TL:0.19410322182128828, TA:0.9444935309221778.\n","DL:1.6276241890820986, DA:0.7019626812237675\n","14\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.2400\n","Epoch 1 train accuracy: 0.6850\n","Epoch 1 dev loss: 1.2186\n","Epoch 1 dev accuracy: 0.6528\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.5021\n","Epoch 2 train accuracy: 0.8666\n","Epoch 2 dev loss: 1.3057\n","Epoch 2 dev accuracy: 0.6762\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.4005\n","Epoch 3 train accuracy: 0.8945\n","Epoch 3 dev loss: 1.3816\n","Epoch 3 dev accuracy: 0.6751\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3524\n","Epoch 4 train accuracy: 0.9034\n","Epoch 4 dev loss: 1.4453\n","Epoch 4 dev accuracy: 0.6530\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.2938\n","Epoch 5 train accuracy: 0.9206\n","Epoch 5 dev loss: 1.5686\n","Epoch 5 dev accuracy: 0.6447\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2642\n","Epoch 6 train accuracy: 0.9267\n","Epoch 6 dev loss: 1.4928\n","Epoch 6 dev accuracy: 0.6607\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2417\n","Epoch 7 train accuracy: 0.9331\n","Epoch 7 dev loss: 1.6662\n","Epoch 7 dev accuracy: 0.6803\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2322\n","Epoch 8 train accuracy: 0.9346\n","Epoch 8 dev loss: 1.4472\n","Epoch 8 dev accuracy: 0.6954\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2179\n","Epoch 9 train accuracy: 0.9393\n","Epoch 9 dev loss: 1.4935\n","Epoch 9 dev accuracy: 0.6878\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2190\n","Epoch 10 train accuracy: 0.9383\n","Epoch 10 dev loss: 1.5973\n","Epoch 10 dev accuracy: 0.6866\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.1898\n","Epoch 11 train accuracy: 0.9465\n","Epoch 11 dev loss: 1.5918\n","Epoch 11 dev accuracy: 0.6840\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.2046\n","Epoch 12 train accuracy: 0.9406\n","Epoch 12 dev loss: 1.4935\n","Epoch 12 dev accuracy: 0.6886\n","\n"," --------- \n","Epoch: 13\n","\n","Epoch 13 train loss: 0.1787\n","Epoch 13 train accuracy: 0.9480\n","Epoch 13 dev loss: 1.5555\n","Epoch 13 dev accuracy: 0.6930\n","\n"," --------- \n","Epoch: 14\n","\n","Epoch 14 train loss: 0.1693\n","Epoch 14 train accuracy: 0.9508\n","Epoch 14 dev loss: 1.6734\n","Epoch 14 dev accuracy: 0.6867\n","\n"," ######## \n","\n","lr:0.00029, alpha:0.000571 @ epoch 8.\n","TL:0.23222963602996122, TA:0.9346071051809349.\n","DL:1.447165093780132, DA:0.6953584497245129\n","15\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.3824\n","Epoch 1 train accuracy: 0.6526\n","Epoch 1 dev loss: 1.2245\n","Epoch 1 dev accuracy: 0.6408\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.5565\n","Epoch 2 train accuracy: 0.8543\n","Epoch 2 dev loss: 1.2678\n","Epoch 2 dev accuracy: 0.6568\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.4023\n","Epoch 3 train accuracy: 0.8954\n","Epoch 3 dev loss: 1.3512\n","Epoch 3 dev accuracy: 0.6627\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3328\n","Epoch 4 train accuracy: 0.9115\n","Epoch 4 dev loss: 1.3721\n","Epoch 4 dev accuracy: 0.6592\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.2977\n","Epoch 5 train accuracy: 0.9207\n","Epoch 5 dev loss: 1.4278\n","Epoch 5 dev accuracy: 0.6709\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2746\n","Epoch 6 train accuracy: 0.9261\n","Epoch 6 dev loss: 1.4794\n","Epoch 6 dev accuracy: 0.6725\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2527\n","Epoch 7 train accuracy: 0.9301\n","Epoch 7 dev loss: 1.5057\n","Epoch 7 dev accuracy: 0.6685\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2221\n","Epoch 8 train accuracy: 0.9371\n","Epoch 8 dev loss: 1.5645\n","Epoch 8 dev accuracy: 0.6563\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2067\n","Epoch 9 train accuracy: 0.9423\n","Epoch 9 dev loss: 1.5791\n","Epoch 9 dev accuracy: 0.6693\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2008\n","Epoch 10 train accuracy: 0.9433\n","Epoch 10 dev loss: 1.5218\n","Epoch 10 dev accuracy: 0.6825\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.1904\n","Epoch 11 train accuracy: 0.9461\n","Epoch 11 dev loss: 1.6820\n","Epoch 11 dev accuracy: 0.6729\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.1917\n","Epoch 12 train accuracy: 0.9451\n","Epoch 12 dev loss: 1.6609\n","Epoch 12 dev accuracy: 0.6664\n","\n"," --------- \n","Epoch: 13\n","\n","Epoch 13 train loss: 0.1815\n","Epoch 13 train accuracy: 0.9478\n","Epoch 13 dev loss: 1.5587\n","Epoch 13 dev accuracy: 0.6894\n","\n"," --------- \n","Epoch: 14\n","\n","Epoch 14 train loss: 0.1511\n","Epoch 14 train accuracy: 0.9545\n","Epoch 14 dev loss: 1.6663\n","Epoch 14 dev accuracy: 0.6686\n","\n"," --------- \n","Epoch: 15\n","\n","Epoch 15 train loss: 0.1637\n","Epoch 15 train accuracy: 0.9532\n","Epoch 15 dev loss: 1.6308\n","Epoch 15 dev accuracy: 0.6821\n","\n"," --------- \n","Epoch: 16\n","\n","Epoch 16 train loss: 0.1513\n","Epoch 16 train accuracy: 0.9561\n","Epoch 16 dev loss: 1.5976\n","Epoch 16 dev accuracy: 0.6751\n","\n"," --------- \n","Epoch: 17\n","\n","Epoch 17 train loss: 0.1408\n","Epoch 17 train accuracy: 0.9567\n","Epoch 17 dev loss: 1.5069\n","Epoch 17 dev accuracy: 0.6970\n","\n"," --------- \n","Epoch: 18\n","\n","Epoch 18 train loss: 0.1452\n","Epoch 18 train accuracy: 0.9578\n","Epoch 18 dev loss: 1.6990\n","Epoch 18 dev accuracy: 0.6845\n","\n"," --------- \n","Epoch: 19\n","\n","Epoch 19 train loss: 0.1288\n","Epoch 19 train accuracy: 0.9619\n","Epoch 19 dev loss: 1.6622\n","Epoch 19 dev accuracy: 0.6946\n","\n"," --------- \n","Epoch: 20\n","\n","Epoch 20 train loss: 0.1262\n","Epoch 20 train accuracy: 0.9621\n","Epoch 20 dev loss: 1.8066\n","Epoch 20 dev accuracy: 0.6838\n","\n"," --------- \n","Epoch: 21\n","\n","Epoch 21 train loss: 0.1305\n","Epoch 21 train accuracy: 0.9610\n","Epoch 21 dev loss: 1.6364\n","Epoch 21 dev accuracy: 0.7094\n","\n"," --------- \n","Epoch: 22\n","\n","Epoch 22 train loss: 0.1177\n","Epoch 22 train accuracy: 0.9644\n","Epoch 22 dev loss: 1.7064\n","Epoch 22 dev accuracy: 0.6849\n","\n"," --------- \n","Epoch: 23\n","\n","Epoch 23 train loss: 0.1209\n","Epoch 23 train accuracy: 0.9641\n","Epoch 23 dev loss: 1.7053\n","Epoch 23 dev accuracy: 0.6798\n","\n"," --------- \n","Epoch: 24\n","\n","Epoch 24 train loss: 0.1072\n","Epoch 24 train accuracy: 0.9672\n","Epoch 24 dev loss: 1.8026\n","Epoch 24 dev accuracy: 0.7025\n","\n"," --------- \n","Epoch: 25\n","\n","Epoch 25 train loss: 0.0987\n","Epoch 25 train accuracy: 0.9703\n","Epoch 25 dev loss: 1.6377\n","Epoch 25 dev accuracy: 0.7045\n","\n"," --------- \n","Epoch: 26\n","\n","Epoch 26 train loss: 0.0895\n","Epoch 26 train accuracy: 0.9723\n","Epoch 26 dev loss: 1.8252\n","Epoch 26 dev accuracy: 0.6881\n","\n"," --------- \n","Epoch: 27\n","\n","Epoch 27 train loss: 0.1049\n","Epoch 27 train accuracy: 0.9687\n","Epoch 27 dev loss: 1.9238\n","Epoch 27 dev accuracy: 0.6833\n","\n"," ######## \n","\n","lr:0.00022, alpha:0.000519 @ epoch 21.\n","TL:0.13050361182043949, TA:0.9609997436623077.\n","DL:1.6364245035638487, DA:0.7093572360587586\n","16\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.2892\n","Epoch 1 train accuracy: 0.6736\n","Epoch 1 dev loss: 1.2501\n","Epoch 1 dev accuracy: 0.6420\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.5191\n","Epoch 2 train accuracy: 0.8617\n","Epoch 2 dev loss: 1.3912\n","Epoch 2 dev accuracy: 0.6580\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.3619\n","Epoch 3 train accuracy: 0.9026\n","Epoch 3 dev loss: 1.3195\n","Epoch 3 dev accuracy: 0.6755\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3301\n","Epoch 4 train accuracy: 0.9107\n","Epoch 4 dev loss: 1.4381\n","Epoch 4 dev accuracy: 0.6751\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.2831\n","Epoch 5 train accuracy: 0.9226\n","Epoch 5 dev loss: 1.4919\n","Epoch 5 dev accuracy: 0.6754\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2561\n","Epoch 6 train accuracy: 0.9280\n","Epoch 6 dev loss: 1.4022\n","Epoch 6 dev accuracy: 0.6899\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2478\n","Epoch 7 train accuracy: 0.9307\n","Epoch 7 dev loss: 1.5449\n","Epoch 7 dev accuracy: 0.6790\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2281\n","Epoch 8 train accuracy: 0.9353\n","Epoch 8 dev loss: 1.5738\n","Epoch 8 dev accuracy: 0.6795\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2152\n","Epoch 9 train accuracy: 0.9382\n","Epoch 9 dev loss: 1.4291\n","Epoch 9 dev accuracy: 0.6893\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2139\n","Epoch 10 train accuracy: 0.9408\n","Epoch 10 dev loss: 1.5828\n","Epoch 10 dev accuracy: 0.6878\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.2137\n","Epoch 11 train accuracy: 0.9389\n","Epoch 11 dev loss: 1.6207\n","Epoch 11 dev accuracy: 0.6762\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.1825\n","Epoch 12 train accuracy: 0.9472\n","Epoch 12 dev loss: 1.7150\n","Epoch 12 dev accuracy: 0.6816\n","\n"," ######## \n","\n","lr:0.000288, alpha:0.000431 @ epoch 6.\n","TL:0.25609488533840824, TA:0.9279931454275563.\n","DL:1.4022422830534165, DA:0.68994707223318\n","17\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.2625\n","Epoch 1 train accuracy: 0.6803\n","Epoch 1 dev loss: 1.1577\n","Epoch 1 dev accuracy: 0.6555\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.5022\n","Epoch 2 train accuracy: 0.8663\n","Epoch 2 dev loss: 1.3219\n","Epoch 2 dev accuracy: 0.6751\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.3880\n","Epoch 3 train accuracy: 0.8966\n","Epoch 3 dev loss: 1.4741\n","Epoch 3 dev accuracy: 0.6622\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3481\n","Epoch 4 train accuracy: 0.9061\n","Epoch 4 dev loss: 1.3969\n","Epoch 4 dev accuracy: 0.6950\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.2858\n","Epoch 5 train accuracy: 0.9232\n","Epoch 5 dev loss: 1.4903\n","Epoch 5 dev accuracy: 0.6760\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2626\n","Epoch 6 train accuracy: 0.9272\n","Epoch 6 dev loss: 1.5106\n","Epoch 6 dev accuracy: 0.6846\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2459\n","Epoch 7 train accuracy: 0.9322\n","Epoch 7 dev loss: 1.5883\n","Epoch 7 dev accuracy: 0.6688\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2276\n","Epoch 8 train accuracy: 0.9362\n","Epoch 8 dev loss: 1.6187\n","Epoch 8 dev accuracy: 0.6694\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2240\n","Epoch 9 train accuracy: 0.9374\n","Epoch 9 dev loss: 1.5253\n","Epoch 9 dev accuracy: 0.6680\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2037\n","Epoch 10 train accuracy: 0.9414\n","Epoch 10 dev loss: 1.4348\n","Epoch 10 dev accuracy: 0.7006\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.1972\n","Epoch 11 train accuracy: 0.9440\n","Epoch 11 dev loss: 1.6074\n","Epoch 11 dev accuracy: 0.6918\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.1774\n","Epoch 12 train accuracy: 0.9483\n","Epoch 12 dev loss: 1.4645\n","Epoch 12 dev accuracy: 0.6962\n","\n"," --------- \n","Epoch: 13\n","\n","Epoch 13 train loss: 0.1656\n","Epoch 13 train accuracy: 0.9517\n","Epoch 13 dev loss: 1.6005\n","Epoch 13 dev accuracy: 0.6961\n","\n"," --------- \n","Epoch: 14\n","\n","Epoch 14 train loss: 0.1769\n","Epoch 14 train accuracy: 0.9486\n","Epoch 14 dev loss: 1.5873\n","Epoch 14 dev accuracy: 0.6873\n","\n"," --------- \n","Epoch: 15\n","\n","Epoch 15 train loss: 0.1527\n","Epoch 15 train accuracy: 0.9555\n","Epoch 15 dev loss: 1.6870\n","Epoch 15 dev accuracy: 0.6935\n","\n"," --------- \n","Epoch: 16\n","\n","Epoch 16 train loss: 0.1488\n","Epoch 16 train accuracy: 0.9566\n","Epoch 16 dev loss: 1.6680\n","Epoch 16 dev accuracy: 0.6851\n","\n"," ######## \n","\n","lr:0.000287, alpha:0.000566 @ epoch 10.\n","TL:0.20374596196909744, TA:0.9413920388782553.\n","DL:1.4348027333184454, DA:0.7005554538000853\n","18\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 1.4171\n","Epoch 1 train accuracy: 0.6457\n","Epoch 1 dev loss: 1.2413\n","Epoch 1 dev accuracy: 0.6486\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 0.5714\n","Epoch 2 train accuracy: 0.8498\n","Epoch 2 dev loss: 1.3950\n","Epoch 2 dev accuracy: 0.6617\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 0.3966\n","Epoch 3 train accuracy: 0.8949\n","Epoch 3 dev loss: 1.3427\n","Epoch 3 dev accuracy: 0.6745\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 0.3373\n","Epoch 4 train accuracy: 0.9092\n","Epoch 4 dev loss: 1.3860\n","Epoch 4 dev accuracy: 0.6783\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 0.2824\n","Epoch 5 train accuracy: 0.9236\n","Epoch 5 dev loss: 1.4404\n","Epoch 5 dev accuracy: 0.6715\n","\n"," --------- \n","Epoch: 6\n","\n","Epoch 6 train loss: 0.2562\n","Epoch 6 train accuracy: 0.9293\n","Epoch 6 dev loss: 1.4955\n","Epoch 6 dev accuracy: 0.6893\n","\n"," --------- \n","Epoch: 7\n","\n","Epoch 7 train loss: 0.2449\n","Epoch 7 train accuracy: 0.9322\n","Epoch 7 dev loss: 1.5023\n","Epoch 7 dev accuracy: 0.7043\n","\n"," --------- \n","Epoch: 8\n","\n","Epoch 8 train loss: 0.2371\n","Epoch 8 train accuracy: 0.9358\n","Epoch 8 dev loss: 1.5038\n","Epoch 8 dev accuracy: 0.6850\n","\n"," --------- \n","Epoch: 9\n","\n","Epoch 9 train loss: 0.2055\n","Epoch 9 train accuracy: 0.9416\n","Epoch 9 dev loss: 1.6173\n","Epoch 9 dev accuracy: 0.6921\n","\n"," --------- \n","Epoch: 10\n","\n","Epoch 10 train loss: 0.2057\n","Epoch 10 train accuracy: 0.9418\n","Epoch 10 dev loss: 1.7149\n","Epoch 10 dev accuracy: 0.6604\n","\n"," --------- \n","Epoch: 11\n","\n","Epoch 11 train loss: 0.1992\n","Epoch 11 train accuracy: 0.9429\n","Epoch 11 dev loss: 1.6888\n","Epoch 11 dev accuracy: 0.6767\n","\n"," --------- \n","Epoch: 12\n","\n","Epoch 12 train loss: 0.1709\n","Epoch 12 train accuracy: 0.9512\n","Epoch 12 dev loss: 1.7368\n","Epoch 12 dev accuracy: 0.6853\n","\n"," --------- \n","Epoch: 13\n","\n","Epoch 13 train loss: 0.1623\n","Epoch 13 train accuracy: 0.9526\n","Epoch 13 dev loss: 1.6206\n","Epoch 13 dev accuracy: 0.6773\n","\n"," ######## \n","\n","lr:0.000214, alpha:0.000475 @ epoch 7.\n","TL:0.244851594692717, TA:0.9321955278850781.\n","DL:1.502331281029848, DA:0.7043465018187623\n","Stats(train_loss=0.13050361182043949, train_accuracy=0.9609997436623077, dev_loss=1.6364245035638487, dev_accuracy=0.7093572360587586, epoch=21, lr=0.00022, alpha=0.000519, max_accuracy=0.7093572360587586)\n"]}]}]}