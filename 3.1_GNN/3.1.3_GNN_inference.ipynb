{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyMp/Z19vviqi5XiunQ/cEdW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Text Classification - GNN Inference\n"],"metadata":{"id":"X_8ti75bDLxt"}},{"cell_type":"markdown","source":["## $\\color{blue}{Sections:}$\n","\n","* Preamble\n","1.   Admin\n","2.   Dataset\n","3.   Model\n","4.   Validate\n","5.   Post Treatment"],"metadata":{"id":"wZIpe_waDL3M"}},{"cell_type":"markdown","source":["## $\\color{blue}{Preamble:}$\n","\n","We now look at inference on our GNN, returning the predictions from the GNN for further analysis."],"metadata":{"id":"ePT4vq35DL5n"}},{"cell_type":"markdown","source":["## $\\color{blue}{Admin}$\n","* Install relevant Libraries\n","* Import relevant Libraries"],"metadata":{"id":"qYbJFtOuDL0a"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"mDOQmYMSCdTJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742827215870,"user_tz":-60,"elapsed":4878,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"87c647a6-363c-4831-8868-3724885f21cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}],"source":["import torch\n","import pandas as pd\n","from google.colab import drive\n","import numpy as np\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","source":["drive.mount(\"/content/drive\")\n","%cd '/content/drive/MyDrive'"],"metadata":{"id":"IHgEq1_zEIOw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742827240550,"user_tz":-60,"elapsed":24676,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"1cd70896-9c67-4657-a672-38232a429597"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive\n"]}]},{"cell_type":"markdown","source":["## $\\color{blue}{Data}$\n","\n","* Connect to Drive\n","* Load the data\n","* Load adjacency matrices"],"metadata":{"id":"XNUYb7PmENbA"}},{"cell_type":"code","source":["path = 'class/datasets/'\n","df_train = pd.read_pickle(path + 'df_train_augmentation_ft')\n","df_dev = pd.read_pickle(path + 'df_dev_augmentation_ft')\n","df_test = pd.read_pickle(path + 'df_test_augmentation_ft')\n","df1 = df_train[['direct_ft_augmented_embedding','chapter_idx']]\n","df2 = df_dev[['direct_ft_augmented_embedding','chapter_idx']]\n","df_val = pd.concat([df2, df1])"],"metadata":{"id":"VjESklMTEKkP","executionInfo":{"status":"ok","timestamp":1742827281715,"user_tz":-60,"elapsed":12185,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["path = 'class/tensors/adj_{}.pt'\n","\n","# train\n","# train_people = torch.load(path.format('train_people'))\n","# train_locations = torch.load(path.format('train_locations'))\n","train_entities = torch.load(path.format('train_augmented_entities'))\n","\n","# dev\n","# dev_people = torch.load(path.format('dev_people'))\n","# dev_locations = torch.load(path.format('dev_locations'))\n","dev_entities = torch.load(path.format('dev_augmented_entities'))\n","\n","# val (contains the adjacency matrix for both the training and the development set)\n","# val_people = torch.load(path.format('val_people'))\n","# val_locations = torch.load(path.format('val_locations'))\n","val_entities = torch.load(path.format('val_augmented_entities'))"],"metadata":{"id":"gelR5lmHEa5J","executionInfo":{"status":"ok","timestamp":1742827345324,"user_tz":-60,"elapsed":40515,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## $\\color{blue}{Dataset:}$"],"metadata":{"id":"x8mdsEBrEqzB"}},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","from copy import deepcopy\n","\n","def sample_neighborhood(A, inds, neighbor_max, branch_max, seed=None):\n","    # Set the random seed for deterministic responses\n","    if seed is not None:\n","        np.random.seed(seed)\n","\n","    np.random.shuffle(A)  # Shuffle the list of adjacency matrices in place\n","    sampled_indices = set(inds)  # Initialize the set of sampled indices\n","\n","    for ind in inds:  # Iterate through node in mini-batch\n","        break_to_outer = False\n","        neighbors = set()\n","\n","        for adj in A:  # Iterate through all adjacency matrices\n","            if break_to_outer:\n","              break\n","\n","            # Get the indices of all neighbors that idx links to\n","            disclude = set([ind]) | sampled_indices\n","            new_neighbors = [neighbor.item() for neighbor in (adj[ind] > 0).nonzero(as_tuple=True)[0] if neighbor.item() not in disclude]\n","            neighbors.update(new_neighbors)\n","\n","\n","            if len(neighbors) >= neighbor_max:  # Check if we have too many neighbors\n","                # Take a random subset using np.random.choice\n","                neighbors = set(np.random.choice(list(neighbors), neighbor_max, replace=False))\n","\n","\n","            copy_neighbors = deepcopy(neighbors)\n","            for idx in copy_neighbors:\n","                if break_to_outer:\n","                  break\n","\n","                neighbors_neighbors = set()\n","                for adj in A:\n","                    disclude = set([ind,idx]) | sampled_indices | neighbors\n","                    new_neighbors_neighbors = [neighbor.item() for neighbor in (adj[idx] > 0).nonzero(as_tuple=True)[0] if neighbor.item() not in disclude]\n","                    if len(new_neighbors_neighbors) > neighbor_max:\n","                      new_neighbors_neighbors = set(np.random.choice(list(new_neighbors_neighbors), neighbor_max, replace = False))\n","                    neighbors_neighbors.update(new_neighbors_neighbors)\n","\n","                    if len(neighbors) + len(neighbors_neighbors) >= branch_max:\n","\n","                      neighbors_neighbors = set(np.random.choice(list(neighbors_neighbors), branch_max - len(neighbors), replace=False))\n","\n","                      neighbors.update(neighbors_neighbors)\n","\n","                      break_to_outer = True\n","                      break\n","\n","                    neighbors.update(neighbors_neighbors)\n","\n","        sampled_indices.update(neighbors)  # Add new neighbors\n","\n","    return list(sampled_indices)"],"metadata":{"id":"iHrpWiauKaGR","executionInfo":{"status":"ok","timestamp":1742827345341,"user_tz":-60,"elapsed":2,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class GNNDataset(Dataset):\n","  def __init__(self, H, A, labels, meta_indices, neighbor_max=4, branch_max=16, seed=None):\n","    \"\"\"Custom dataset with neighborhood sampling\n","\n","    Args:\n","      H : torch.tensor\n","        input embeddings (n x d)\n","\n","      A : list[torch.tensor]\n","        list of (n x n)\n","\n","      labels : torch.LongTensor\n","        y\n","\n","      meta_indices : torch.LongTensor\n","        index of datapoint to filter validation score\n","\n","      neighbor_max : int\n","        max neighbors for each node in mini-batch\n","\n","      batch_max : int\n","        max size of batch\n","\n","    \"\"\"\n","    # All inits must be tensors\n","    self.H = H.to(device)\n","    self.A = [a.to(device) for a in A]\n","    self.labels = labels.to(device)\n","    self.meta_indices = meta_indices\n","    self.neighbor_max = neighbor_max\n","    self.branch_max = branch_max\n","    self.seed = seed\n","\n","  def __len__(self):\n","    return len(self.labels)\n","\n","  def __getitem__(self, inds):\n","    # print('\\n####################\\n')\n","    # print('GET ITEM CALLED', 'INDS:', inds)\n","    # Sample neighborhood\n","\n","    # get inds in list\n","    inds = inds.tolist() if torch.is_tensor(inds) else (inds if isinstance(inds,list) else [inds])\n","\n","    # return the required inds\n","    sampled_indices = sample_neighborhood(self.A, inds, self.neighbor_max, self.branch_max,seed=self.seed)\n","\n","    # get the input for the required inds\n","    H_batch = self.H[sampled_indices]\n","\n","    # get the adjacency matrix for the required inds\n","    A_batch = [self.A[k][sampled_indices][:, sampled_indices] for k in range(len(self.A))]\n","\n","    # get the labels for the required inds\n","    labels_batch = self.labels[sampled_indices]\n","\n","    # get meta indices\n","    index_batch = self.meta_indices[sampled_indices]\n","\n","    return H_batch, A_batch, labels_batch, index_batch"],"metadata":{"id":"jU03yW3dKeK7","executionInfo":{"status":"ok","timestamp":1742827345409,"user_tz":-60,"elapsed":3,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["H_val = torch.Tensor(np.stack(df_val['direct_ft_augmented_embedding'].to_list()))\n","labels_val = torch.LongTensor(list(df_val['chapter_idx']))\n","A_val = []\n","A_val.append(val_entities)\n","val_indices = torch.LongTensor(list(range(df_val.shape[0])))\n","\n","validation_dataset = GNNDataset(H_val, A_val, labels_val, val_indices, neighbor_max=4, branch_max=16, seed=42)\n","\n","# Prevent dataloader from calling a single index at a time\n","custom_validation_sampler = torch.utils.data.sampler.BatchSampler(\n","    torch.utils.data.sampler.SequentialSampler(validation_dataset),\n","    batch_size=32,\n","    drop_last=False)\n","\n","\n","dev_loader = DataLoader(validation_dataset, sampler = custom_validation_sampler)"],"metadata":{"id":"D4T-LfSijgKY","executionInfo":{"status":"ok","timestamp":1742828164714,"user_tz":-60,"elapsed":484,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["## $\\color{blue}{Model:}$"],"metadata":{"id":"S3rKXf1DOXJr"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class GNNLayer(nn.Module):\n","    def __init__(self, in_features, out_features, num_relations=1, dropout=0.3):\n","        super(GNNLayer, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.num_relations = num_relations\n","        self.dropout = dropout\n","\n","        self.T = nn.ParameterList([nn.Parameter(torch.Tensor(in_features, out_features)) for _ in range(num_relations)])\n","        self.E = nn.ParameterList([nn.Parameter(torch.Tensor(in_features, out_features)) for _ in range(num_relations)])\n","\n","        # Batch normalization\n","        self.batch_norm = nn.BatchNorm1d(out_features)\n","\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        for t in self.T:\n","            nn.init.xavier_uniform_(t)\n","        for e in self.E:\n","            nn.init.xavier_uniform_(e)\n","\n","    def forward(self, H, A):\n","        H_out = torch.zeros_like(H)\n","        for k in range(self.num_relations):\n","            messages_projection = A[k].T @ H @ self.E[k]\n","            degrees = A[k].sum(dim=1, keepdim=True)\n","            degrees[degrees == 0] = 1.0\n","            messages_projection /= degrees\n","\n","            self_projection = H @ self.T[k]\n","\n","            # Include skip connection\n","            H_out += F.leaky_relu(self_projection + messages_projection) + H\n","\n","        # Apply batch normalization\n","        H_out = self.batch_norm(H_out)\n","\n","        # Apply dropout\n","        H_out = F.dropout(H_out, p=self.dropout, training=self.training)\n","\n","        return H_out\n","\n","class GNNModel(nn.Module):\n","   def __init__(self, d, h, c, num_relations=1, num_layers=2):\n","      super(GNNModel, self).__init__()\n","      self.num_layers = num_layers\n","      self.gnn_layers = nn.ModuleList([GNNLayer(d, d) for _ in range(num_layers)])\n","      self.fc1 = nn.Linear(d, h)\n","      self.fc2 = nn.Linear(h, c)\n","\n","   def forward(self, H, A):\n","      for layer in self.gnn_layers:\n","         H = layer(H, A)\n","      # Classification\n","      H = F.relu(self.fc1(H))\n","      Output = self.fc2(H)\n","      return Output\n"],"metadata":{"id":"ePZHtFFH19J9","executionInfo":{"status":"ok","timestamp":1742827405501,"user_tz":-60,"elapsed":18,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["d = 768\n","h = 400\n","c = 70\n","num_relations = 1\n","path = \"class/models/GNN_augmented_ft.pt\"\n"],"metadata":{"id":"A0WxOwAE-Sxv","executionInfo":{"status":"ok","timestamp":1742828244389,"user_tz":-60,"elapsed":3,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["model = GNNModel(d, h, c, num_relations)\n","criterion = nn.CrossEntropyLoss()\n","model.load_state_dict(torch.load(path))\n","model = model.to(device)"],"metadata":{"collapsed":true,"id":"WwhOVb_R-NUL","executionInfo":{"status":"ok","timestamp":1742828245160,"user_tz":-60,"elapsed":55,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["## $\\color{blue}{Validate:}$"],"metadata":{"id":"XjE8r0jqkwUW"}},{"cell_type":"code","source":["def accuracy(outputs, labels):\n","    # argmax to get predicted classes\n","    _, predicted = torch.max(outputs, 1)\n","\n","    # count correct\n","    correct = (predicted == labels).sum().item()\n","\n","    # get average\n","    acc = correct / labels.size(0)  # Total number of samples\n","    return acc"],"metadata":{"id":"nbJc7_uMAGy-","executionInfo":{"status":"ok","timestamp":1742828246525,"user_tz":-60,"elapsed":6,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["df_dev.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W8z_AB9Ep_JW","executionInfo":{"status":"ok","timestamp":1742828210649,"user_tz":-60,"elapsed":24,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"e670362f-dfc6-448e-d2c3-f21942ea9888"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(746, 7)"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["def validate(model, dev_loader, criterion, threshold=746):\n","    \"\"\"return predictions, ground truths and indices, as val results includes\n","    data points sampled numerous times per epoch, reported results are for\n","    sampled points and so many points are included many times in the result\n","    post treatment is required to get a single score for each validation point.\n","    \"\"\"\n","    model.eval()\n","    epoch_dev_losses = []\n","    epoch_dev_accuracy = []\n","    pred_holder = []\n","    real_holder = []\n","    index_holder = []\n","\n","    with torch.no_grad():\n","        for batch_idx, (H, A, y, indices) in enumerate(dev_loader):\n","            H = H.squeeze(0)\n","            A = [a.squeeze(0) for a in A]\n","            y = y.squeeze(0)\n","            indices = indices.squeeze(0)\n","\n","            out = model(H, A)\n","\n","            # Filter out training points\n","            mask = indices < threshold\n","            filtered_out = out[mask]\n","            filtered_y = y[mask]\n","            filtered_indices = indices[mask]\n","\n","\n","\n","            # Calculate loss and accuracy only on filtered outputs\n","            if filtered_out.size(0) > 0:  # Ensure there are samples to evaluate\n","                dev_loss = criterion(filtered_out, filtered_y)\n","                dev_accuracy = accuracy(filtered_out, filtered_y)\n","\n","                epoch_dev_losses.append(dev_loss.item())\n","                epoch_dev_accuracy.append(dev_accuracy)\n","\n","                _, predicted = torch.max(filtered_out, 1)\n","\n","                preds = [item.item() for item in predicted]\n","                pred_holder += preds\n","                reals = [item.item() for item in filtered_y]\n","                real_holder += reals\n","                inds = [item.item() for item in filtered_indices]\n","                index_holder += inds\n","\n","    return np.mean(epoch_dev_losses), np.mean(epoch_dev_accuracy), pred_holder, real_holder, index_holder"],"metadata":{"id":"w8gapaRnAM4Q","executionInfo":{"status":"ok","timestamp":1742828249919,"user_tz":-60,"elapsed":16,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["loss, accuracy, preds, reals, inds = validate(model, dev_loader, criterion)"],"metadata":{"id":"uJCQxVxvBA0g","executionInfo":{"status":"ok","timestamp":1742828631275,"user_tz":-60,"elapsed":378572,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DzTSirU7FgGq","executionInfo":{"status":"ok","timestamp":1742828631282,"user_tz":-60,"elapsed":19,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"26ca5838-b3cd-4abd-eaf1-5f879e3d1a22"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["np.float64(2.389428412983462)"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["accuracy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JMkD9r_sFiDR","executionInfo":{"status":"ok","timestamp":1742828631291,"user_tz":-60,"elapsed":10,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"275196c2-f110-417a-bc4c-3ae5443f7a06"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["np.float64(0.7167972938184882)"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["## $\\color{blue}{Post-Treatment:}$"],"metadata":{"id":"GYC7TBZYAeXP"}},{"cell_type":"code","source":["df_dev.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9wysRwXg5xar","executionInfo":{"status":"ok","timestamp":1742827805585,"user_tz":-60,"elapsed":12,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"ea4c0e19-ade4-406c-c57c-0340e7a74cc3"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['master', 'book_idx', 'chapter_idx', 'content', 'vanilla_embedding.1',\n","       'direct_ft_augmented_embedding', 'ner_responses'],\n","      dtype='object')"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["inds[200:220]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1H2HhkXvsATm","executionInfo":{"status":"ok","timestamp":1742828890814,"user_tz":-60,"elapsed":26,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"a6979f83-25ec-4f21-8246-55937a977d16"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[212,\n"," 230,\n"," 231,\n"," 284,\n"," 41,\n"," 46,\n"," 51,\n"," 85,\n"," 160,\n"," 161,\n"," 162,\n"," 163,\n"," 164,\n"," 165,\n"," 166,\n"," 167,\n"," 168,\n"," 169,\n"," 170,\n"," 171]"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["from collections import namedtuple\n","\n","# store results for every validation point in a named tuple\n","\n","Check = namedtuple(\"Check\", ['id', 'df_label','model_label', 'predicted_label'])\n","\n","res = []\n","for i in range(len(inds)):\n","  point_ind = inds[i]\n","  df_label = df_dev.loc[point_ind]['chapter_idx']\n","  res.append(Check(point_ind,df_label,reals[i], preds[i]))"],"metadata":{"id":"yh4dKrXq40py","executionInfo":{"status":"ok","timestamp":1742828970209,"user_tz":-60,"elapsed":211,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["# note proportion of data points with NER neighbors\n","\n","A_val[0].size()\n","adj = A_val[0]\n","neighbors = torch.sum(adj,dim=1)!=0\n","torch.sum(neighbors)/len(neighbors)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k3qcqgXhEM88","executionInfo":{"status":"ok","timestamp":1742828975476,"user_tz":-60,"elapsed":119,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"6ed54225-8c2d-4115-d318-34b750920eec"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.5457)"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["df_dev.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W2HgEcJRtYkR","executionInfo":{"status":"ok","timestamp":1742829101493,"user_tz":-60,"elapsed":12,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"f2e1ea1d-e9b5-4a3d-92fe-0c1422e84922"},"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(746, 7)"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["# bool if node has neighbors\n","\n","neighbors_valid = neighbors[:746]\n","neighbors_valid[:20]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3DuiXtiEIaMQ","executionInfo":{"status":"ok","timestamp":1742829112217,"user_tz":-60,"elapsed":21,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"2cb9005a-5393-4e0d-a273-239ec1019094"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ True,  True,  True, False,  True,  True,  True, False,  True,  True,\n","         True,  True,  True,  True,  True, False, False, False, False,  True])"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["# list of results just for connected points\n","\n","neighbors_res = []\n","neighbors_ids = []\n","for check in res:\n","  identity = check.id\n","  if neighbors_valid[identity]:\n","    neighbors_res.append(check)\n","    neighbors_ids.append(identity)\n","neighbors_ids = list(set(neighbors_ids))"],"metadata":{"id":"pro4Gb6GGOXR","executionInfo":{"status":"ok","timestamp":1742829131103,"user_tz":-60,"elapsed":24,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["# Get a list of predictions for each index (inds is index recorded in model output)\n","\n","check = [None] * df_dev.shape[0]\n","for i in range(df_dev.shape[0]):\n","  hold = []\n","  results_ind = i\n","  for j in range(len(inds)):\n","    if (inds[j] == results_ind):\n","      hold.append(preds[j])\n","  check[i] = hold"],"metadata":{"id":"T2TNcKsRBpWl","executionInfo":{"status":"ok","timestamp":1742829159656,"user_tz":-60,"elapsed":320,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["# Use mode as central tendancy for multiple predictions of the same data point\n","\n","def mode(lstr):\n","  unique, counts = np.unique(lstr, return_counts=True)\n","  max_idx = np.argmax(counts)\n","  mode_val = unique[max_idx]\n","  return mode_val, lstr.index(mode_val)\n","\n","predictions = [mode(el)[0] for el in check]"],"metadata":{"id":"YFTKH48gCSJS","executionInfo":{"status":"ok","timestamp":1742829163400,"user_tz":-60,"elapsed":18,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["gnn_count_n = 0\n","gnn_number_n = 0\n","gnn_count_i = 0\n","gnn_number_i = 0\n","\n","van_count_n = 0\n","van_number_n = 0\n","van_count_i = 0\n","van_number_i = 0\n","\n","for i in range(df_dev.shape[0]):\n","  if i in neighbors_ids:\n","    gnn_number_n += 1\n","    van_number_n += 1\n","    if df_dev.loc[i][\"chapter_idx\"] == predictions[i]:\n","      gnn_count_n += 1\n","    # if df_dev.loc[i][\"chapter_idx\"] == df_dev.loc[i]['vanilla_preds']:\n","    #   van_count_n += 1\n","  else:\n","    van_number_i += 1\n","    gnn_number_i += 1\n","    if df_dev.loc[i][\"chapter_idx\"] == predictions[i]:\n","      gnn_count_i += 1\n","    # if df_dev.loc[i][\"chapter_idx\"] == df_dev.loc[i]['vanilla_preds']:\n","    #   van_count_i += 1\n","\n","print(f'GNN Neighbors: {gnn_count_n/gnn_number_n}')\n","# print(f'Van Neighbors: {van_count_n/van_number_n}')\n","\n","print('----')\n","\n","print(f'GNN Loners: {gnn_count_i/gnn_number_i}')\n","# print(f'Van Loners: {van_count_i/van_number_i}')\n"],"metadata":{"id":"SC4372ZNEft3","executionInfo":{"status":"ok","timestamp":1742829209000,"user_tz":-60,"elapsed":36,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c26749f5-0445-4aad-d8cc-183fed5520ec"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["GNN Neighbors: 0.6820652173913043\n","----\n","GNN Loners: 0.6428571428571429\n"]}]},{"cell_type":"code","source":["# Update DataFrame\n","\n","df_dev['gcn_preds'] = predictions\n","df_dev['connected'] = list(neighbors_valid)"],"metadata":{"id":"EFwMaPFhGBnB","executionInfo":{"status":"ok","timestamp":1742829249149,"user_tz":-60,"elapsed":5,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["path = \"class/datasets/\"\n","df_dev.to_pickle(path + \"df_dev_augmentation_ft\")"],"metadata":{"id":"cCpOXPPHGP4F","executionInfo":{"status":"ok","timestamp":1742829270245,"user_tz":-60,"elapsed":172,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":48,"outputs":[]}]}