{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Q8SwOiR2bxtadXJeoC2c-bHDAG6P_HtL","timestamp":1736506739079}],"machine_shape":"hm","gpuType":"L4","collapsed_sections":["q4Mn5bkiJDE8","Tml4hS4iKIGX","WrZ8xkIsSnvw","5so7JQvHdAvF","dyzhkkmcR29K","1C-wVpsPUasU","ZBHJn5V8mIDu"],"authorship_tag":"ABX9TyM0+4oqcEW5f7lIFYv7E1vj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Text Classification - Training a GAT\n"],"metadata":{"id":"FBjat8lrIEsf"}},{"cell_type":"markdown","source":["## $\\color{blue}{Sections:}$\n","\n","* Preamble\n","1.   Admin\n","2.   Dataset\n","3.   Model\n","4.   Train - Validate\n","5.   Training Loop"],"metadata":{"id":"O5wHwOnkIKvx"}},{"cell_type":"markdown","source":["## $\\color{blue}{Preamble:}$\n","\n","We now train a GAT in basic PyTorch. The model will replace GCN node udates with attention based updates. Note that the GAT solution is immplemented as intended, but the results are sub-standard. There is no learning in the graph attention mechanism."],"metadata":{"id":"2LYAHLRYIY_C"}},{"cell_type":"markdown","source":["## $\\color{blue}{Admin}$\n","* Install relevant Libraries\n","* Import relevant Libraries"],"metadata":{"id":"udtGvzIPItVT"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vfqeSzgDH285","executionInfo":{"status":"ok","timestamp":1738852804314,"user_tz":-60,"elapsed":4406,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"5b940d13-10db-4a62-d421-4d7f585553f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}],"source":["import torch\n","import pandas as pd\n","from google.colab import drive\n","import numpy as np\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","source":["drive.mount(\"/content/drive\")\n","%cd '/content/drive/MyDrive'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FSXQ2wAfPWoF","executionInfo":{"status":"ok","timestamp":1738852821664,"user_tz":-60,"elapsed":17353,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"06619ff9-7b40-41af-808b-77b23d27b8f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive\n"]}]},{"cell_type":"markdown","source":["## $\\color{blue}{Data}$\n","\n","* Connect to Drive\n","* Load the data\n","* Load adjacency matrices"],"metadata":{"id":"q4Mn5bkiJDE8"}},{"cell_type":"code","source":["path = 'class/datasets/'\n","df_train = pd.read_pickle(path + 'df_train')\n","df_dev = pd.read_pickle(path + 'df_dev')\n","df_test = pd.read_pickle(path + 'df_test')"],"metadata":{"id":"gPwH-5O5JHeM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = 'class/tensors/adj_{}.pt'\n","\n","# train\n","train_people = torch.load(path.format('train_people'))\n","train_locations = torch.load(path.format('train_locations'))\n","train_entities = torch.load(path.format('train_entities'))\n","\n","# dev\n","dev_people = torch.load(path.format('dev_people'))\n","dev_locations = torch.load(path.format('dev_locations'))\n","dev_entities = torch.load(path.format('dev_entities'))\n","\n","# val (contains the adjacency matrix for both the training and the development set)\n","val_people = torch.load(path.format('val_people'))\n","val_locations = torch.load(path.format('val_locations'))\n","val_entities = torch.load(path.format('val_entities'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DXpjrdiiJSO0","executionInfo":{"status":"ok","timestamp":1738852881979,"user_tz":-60,"elapsed":45047,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"7244c723-a854-4f17-aecb-b8ee728f267b","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-4-fd49eb201090>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  train_people = torch.load(path.format('train_people'))\n","<ipython-input-4-fd49eb201090>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  train_locations = torch.load(path.format('train_locations'))\n","<ipython-input-4-fd49eb201090>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  train_entities = torch.load(path.format('train_entities'))\n","<ipython-input-4-fd49eb201090>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  dev_people = torch.load(path.format('dev_people'))\n","<ipython-input-4-fd49eb201090>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  dev_locations = torch.load(path.format('dev_locations'))\n","<ipython-input-4-fd49eb201090>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  dev_entities = torch.load(path.format('dev_entities'))\n","<ipython-input-4-fd49eb201090>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  val_people = torch.load(path.format('val_people'))\n","<ipython-input-4-fd49eb201090>:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  val_locations = torch.load(path.format('val_locations'))\n","<ipython-input-4-fd49eb201090>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  val_entities = torch.load(path.format('val_entities'))\n"]}]},{"cell_type":"markdown","source":["## $\\color{blue}{Dataset:}$"],"metadata":{"id":"Tml4hS4iKIGX"}},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","from copy import deepcopy\n","\n","def sample_neighborhood(A, inds, neighbor_max, branch_max, seed=None):\n","    # Set the random seed for deterministic responses\n","    if seed is not None:\n","        np.random.seed(seed)\n","\n","    sampled_indices = set(inds)  # Initialize the set of sampled indices\n","\n","    for ind in inds:  # Iterate through node in mini-batch\n","        break_to_outer = False\n","        neighbors = set()\n","\n","        # Get the indices of all neighbors that idx links to\n","        disclude = set([ind]) | sampled_indices\n","        new_neighbors = [neighbor.item() for neighbor in (A[ind] > 0).nonzero(as_tuple=True)[0] if neighbor.item() not in disclude]\n","        neighbors.update(new_neighbors)\n","\n","        if len(neighbors) >= neighbor_max:  # Check if we have too many neighbors\n","            # Take a random subset using np.random.choice\n","            neighbors = set(np.random.choice(list(neighbors), neighbor_max, replace=False))\n","\n","        copy_neighbors = deepcopy(neighbors)\n","        for idx in copy_neighbors:\n","            if break_to_outer:\n","              break\n","\n","            neighbors_neighbors = set()\n","            disclude = set([ind,idx]) | sampled_indices | neighbors\n","            new_neighbors_neighbors = [neighbor.item() for neighbor in (A[idx] > 0).nonzero(as_tuple=True)[0] if neighbor.item() not in disclude]\n","            if len(new_neighbors_neighbors) > neighbor_max:\n","              new_neighbors_neighbors = set(np.random.choice(list(new_neighbors_neighbors), neighbor_max, replace = False))\n","            neighbors_neighbors.update(new_neighbors_neighbors)\n","\n","\n","            if len(neighbors) + len(neighbors_neighbors) >= branch_max:\n","\n","              neighbors_neighbors = set(np.random.choice(list(neighbors_neighbors), branch_max - len(neighbors), replace=False))\n","\n","              neighbors.update(neighbors_neighbors)\n","\n","              break_to_outer = True\n","              break\n","\n","            neighbors.update(neighbors_neighbors)\n","\n","        sampled_indices.update(neighbors)  # Add new neighbors\n","\n","    return list(sampled_indices)"],"metadata":{"id":"wvPKQX4B_k9J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GNNDataset(Dataset):\n","  def __init__(self, H, A, labels, meta_indices, neighbor_max=4, branch_max=16, seed=None):\n","    \"\"\"Custom dataset with neighborhood sampling\n","\n","    Args:\n","      H : torch.tensor\n","        input embeddings (n x d)\n","\n","      A : torch.tensor\n","        adjacency matrix (n x n)\n","\n","      labels : torch.LongTensor\n","        y\n","\n","      meta_indices : torch.LongTensor\n","        index of datapoint to filter validation score\n","\n","      neighbor_max : int\n","        max neighbors for each node in mini-batch\n","\n","      batch_max : int\n","        max size of batch\n","\n","    \"\"\"\n","    # All inits must be tensors\n","    self.H = H.to(device)\n","    self.A = A.to(device)\n","    self.labels = labels.to(device)\n","    self.meta_indices = meta_indices\n","    self.neighbor_max = neighbor_max\n","    self.branch_max = branch_max\n","    self.seed = seed\n","\n","  def __len__(self):\n","    return len(self.labels)\n","\n","  def __getitem__(self, inds):\n","    # Sample neighborhood\n","\n","    # get inds in list\n","    inds = inds.tolist() if torch.is_tensor(inds) else (inds if isinstance(inds,list) else [inds])\n","\n","    # return the required inds\n","    sampled_indices = sample_neighborhood(self.A, inds, self.neighbor_max, self.branch_max,seed=self.seed)\n","\n","    # get the input for the required inds\n","    H_batch = self.H[sampled_indices]\n","\n","    # get the adjacency matrix for the required inds\n","    A_batch = self.A[sampled_indices][:, sampled_indices]\n","\n","    # get the labels for the required inds\n","    labels_batch = self.labels[sampled_indices]\n","\n","    # get meta indices\n","    index_batch = self.meta_indices[sampled_indices]\n","\n","    return H_batch, A_batch, labels_batch, index_batch"],"metadata":{"id":"9Rag8OCtKo1r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Sampling Verificiation"],"metadata":{"id":"zE_tQG50xpCd"}},{"cell_type":"code","source":["H = torch.stack(list(df_train['vanilla_embedding.1']))\n","labels = torch.LongTensor(list(df_train['chapter_idx']))\n","A = train_entities\n","meta_indices = torch.LongTensor(list(range(df_train.shape[0])))"],"metadata":{"id":"NWXfTAo_PxGe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = GNNDataset(H, A, labels, meta_indices, neighbor_max=4, branch_max=10)\n","\n","# Prevent dataloader from calling a single index at a time\n","custom_sampler = torch.utils.data.sampler.BatchSampler(\n","    torch.utils.data.sampler.RandomSampler(train_dataset),\n","    batch_size=6,\n","    drop_last=False)\n","\n","\n","train_loader = DataLoader(train_dataset, sampler = custom_sampler)\n"],"metadata":{"id":"AxzIHGzKRr2V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check batches\n","\n","# Number of batches to inspect\n","num_batches_to_check = 2\n","\n","for batch_idx, (inputs, adjacency, labels, indices) in enumerate(train_loader):\n","    print('\\n##########################\\n')\n","    print(f\"Batch {batch_idx + 1}/{num_batches_to_check}:\")\n","    print('-' * 10)\n","    print(\"Inputs:\")\n","    print(f\"  Type: {type(inputs)}\")\n","    print(f\"  Shape: {inputs.size()}\")\n","    print('-' * 10)\n","    print(\"Adjacency:\")\n","    print(f\"  Type: {type(adjacency)}\")\n","    print(f\"  Shape: {adjacency[0].size()}\")\n","    print('-' * 10)\n","    print(\"Indices:\")\n","    print(f\"  Type: {type(indices)}\")\n","    print(f\"  Shape: {indices.size()}\")\n","    print(indices)\n","    print('-' * 10)\n","    print(\"Labels:\")\n","    print(f\"  Type: {type(labels)}\")\n","    print(f\"  Shape: {labels.size()}\")\n","    print(labels)\n","\n","    # Stop after inspecting the desired number of batches\n","    if batch_idx + 1 >= num_batches_to_check:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2MW9c2A1WE08","executionInfo":{"status":"ok","timestamp":1738852883672,"user_tz":-60,"elapsed":319,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"840b7596-bcf2-4010-e311-a065bf84cef7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","##########################\n","\n","Batch 1/2:\n","----------\n","Inputs:\n","  Type: <class 'torch.Tensor'>\n","  Shape: torch.Size([1, 21, 768])\n","----------\n","Adjacency:\n","  Type: <class 'torch.Tensor'>\n","  Shape: torch.Size([21, 21])\n","----------\n","Indices:\n","  Type: <class 'torch.Tensor'>\n","  Shape: torch.Size([1, 21])\n","tensor([[ 1473, 11203, 11972,  4997,   390, 11975, 11917,  1936,  1746,  9875,\n","         10770,  2196,  8665,  1562,  2735,  7536,  8112,  6517, 11765,  1019,\n","          8508]])\n","----------\n","Labels:\n","  Type: <class 'torch.Tensor'>\n","  Shape: torch.Size([1, 21])\n","tensor([[14, 16, 14, 61,  6,  7,  9, 22, 67, 13,  6,  4,  0, 14, 14,  2, 14,  5,\n","         14, 14, 51]], device='cuda:0')\n","\n","##########################\n","\n","Batch 2/2:\n","----------\n","Inputs:\n","  Type: <class 'torch.Tensor'>\n","  Shape: torch.Size([1, 22, 768])\n","----------\n","Adjacency:\n","  Type: <class 'torch.Tensor'>\n","  Shape: torch.Size([22, 22])\n","----------\n","Indices:\n","  Type: <class 'torch.Tensor'>\n","  Shape: torch.Size([1, 22])\n","tensor([[ 2366,  4354,  5699,  9284, 10309,  7880,  5961,  9674,  9996,  5389,\n","         11341,  5200,  5585, 10131,  8726,  3547, 11356,  1182, 10527,  9385,\n","          8693,  1598]])\n","----------\n","Labels:\n","  Type: <class 'torch.Tensor'>\n","  Shape: torch.Size([1, 22])\n","tensor([[50, 45, 48, 42, 15, 11, 17, 23,  7,  3, 37, 35, 45, 43, 42,  7, 52, 37,\n","         47, 12, 55, 12]], device='cuda:0')\n"]}]},{"cell_type":"code","source":["H = torch.stack(list(df_train['vanilla_embedding.1']))\n","labels = torch.LongTensor(list(df_train['chapter_idx']))\n","A = train_entities\n","meta_indices = torch.LongTensor(list(range(df_train.shape[0])))"],"metadata":{"id":"k5YldoFcXCHa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The dataloader seems to be working correctly, the implementation of custom sampling on all indices at once leads to DataLoaders collate function inserting a new dimension that it will stach against. Because all indices are dealt with at once, there is no stacking.\n","\n","The simple solution will be to simply squeeze the tensors in the training loop. The validation loader eradicates randomness from the process."],"metadata":{"id":"6H8779aHR9wL"}},{"cell_type":"markdown","source":["## $\\color{blue}{Model:}$"],"metadata":{"id":"WrZ8xkIsSnvw"}},{"cell_type":"code","source":["class GATVaswaniLayer(nn.Module):\n","    def __init__(self, in_features, out_features, dropout_rate):\n","        super(GATVaswaniLayer, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","        # define self projections\n","        self.Wq = nn.Parameter(torch.Tensor(in_features, out_features))\n","        self.Wk = nn.Parameter(torch.Tensor(in_features, out_features))\n","        self.Wv = nn.Parameter(torch.Tensor(in_features, out_features))\n","        nn.init.xavier_uniform_(self.Wq)\n","        nn.init.xavier_uniform_(self.Wk)\n","        nn.init.xavier_uniform_(self.Wv)\n","\n","        # boost self\n","        self.self_bias = nn.Parameter(torch.Tensor(1))  # Bias term for self-loops\n","        nn.init.constant_(self.self_bias, 1.0)\n","        self.leakyrelu = nn.LeakyReLU()\n","\n","        self.batch_norm = nn.BatchNorm1d(out_features)\n","\n","    def forward(self, H, A):\n","        # Apply transformation of the input\n","        H_q = torch.matmul(H, self.Wq)  # n x d\n","        H_k = torch.matmul(H, self.Wk)  # n x d\n","        H_v = torch.matmul(H, self.Wv)  # n x d\n","\n","        # Compute attention scores\n","        E = torch.matmul(H_q, H_k.transpose(0, 1))  # n x n\n","\n","        # Scale identity matrix and add self-loops\n","        n = H.size()[0]\n","        I = self.self_bias.item() * torch.eye(n, device=H.device)\n","\n","        # Apply the adjacency matrix A in sparse format\n","        A_sparse = A.to_sparse()\n","        attention = E + I\n","\n","        # Apply the sparse mask from the adjacency matrix\n","        attention = attention * A_sparse.to_dense()\n","\n","        # Create a large negative value for non-existing edges\n","        N = -9e15 * torch.ones_like(attention)\n","\n","        # Apply the non-existing edges to negative values\n","        attention = torch.where(A_sparse.to_dense() > 0, attention, N)\n","\n","        # Perform softmax over the rows\n","        attention = F.softmax(attention, dim=1)\n","\n","        # Weight the projected input\n","        H_out = self.leakyrelu(torch.matmul(attention, H_v))\n","        H_out = self.dropout(H_out)\n","        H_out = self.batch_norm(H_out)\n","\n","        return H_out\n","\n","class GATModel(nn.Module):\n","    def __init__(self, d, h, c, num_layers=2, dropout_rate=0.3):\n","        super(GATModel, self).__init__()\n","        self.num_layers = num_layers\n","        self.gnn_layers = nn.ModuleList([GATVaswaniLayer(d, d, dropout_rate) for _ in range(num_layers)])\n","        self.fc1 = nn.Linear(d, h)\n","        self.batch_norm_fc1 = nn.BatchNorm1d(h)\n","        self.fc2 = nn.Linear(h, c)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, H, A):\n","        for layer in self.gnn_layers:\n","            H = layer(H, A)\n","\n","        H = self.relu(self.batch_norm_fc1(self.dropout(self.fc1(H))))\n","        Output = self.fc2(H)\n","        return Output\n","\n","    def forward_layer(self, H, A, layer_idx):\n","        \"\"\"Forward pass for a specific layer.\"\"\"\n","        H = self.gnn_layers[layer_idx](H, A)\n","        return H"],"metadata":{"id":"i1A6kop8ulgA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.optim as optim\n","\n","d = 768\n","h = 400   # hidden dimension of fully connected layer\n","c = 70   # number of classes\n","num_relations = 1   # number of relationship types\n","\n","# Model, Loss, Optimizer\n","model = GATModel(d, h, c)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.AdamW(model.parameters(), lr=0.001)\n","\n"],"metadata":{"id":"NP8Q4qpiVp-1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def count_parameters_per_module(model):\n","    print(\"Module and parameter counts:\")\n","\n","    for name, module in model.named_modules():\n","        # Skip the top-level module (the model itself)\n","        if not isinstance(module, nn.Module) or name == \"\":\n","            continue\n","\n","        param_count = sum(p.numel() for p in module.parameters() if p.requires_grad)\n","\n","        if param_count > 0:  # Only print modules that have parameters\n","            print(f\"{name}: {param_count} parameters\")"],"metadata":{"id":"2zm1UjLWZUBB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count_parameters_per_module(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5LxQpA1fYB5e","executionInfo":{"status":"ok","timestamp":1738852899344,"user_tz":-60,"elapsed":301,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"228f35ad-4707-4309-a463-4fa86fffc8fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Module and parameter counts:\n","gnn_layers: 3542018 parameters\n","gnn_layers.0: 1771009 parameters\n","gnn_layers.0.batch_norm: 1536 parameters\n","gnn_layers.1: 1771009 parameters\n","gnn_layers.1.batch_norm: 1536 parameters\n","fc1: 307600 parameters\n","batch_norm_fc1: 800 parameters\n","fc2: 28070 parameters\n"]}]},{"cell_type":"markdown","source":["## $\\color{blue}{Train-Validate:}$"],"metadata":{"id":"5so7JQvHdAvF"}},{"cell_type":"code","source":["def accuracy(outputs, labels):\n","    # argmax to get predicted classes\n","    _, predicted = torch.max(outputs, 1)\n","\n","    # count correct\n","    correct = (predicted == labels).sum().item()\n","\n","    # get average\n","    acc = correct / labels.size(0)  # Total number of samples\n","    return acc"],"metadata":{"id":"EZhnvYLtWbqk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","def train(model, train_loader, criterion, optimizer):\n","    model.train()\n","    epoch_train_losses = []\n","    epoch_train_accuracy = []\n","\n","    for batch_idx, (H, A, y, indices) in enumerate(train_loader):\n","        print('TTT',batch_idx)\n","        optimizer.zero_grad()\n","\n","        H = H.squeeze(0)\n","        A = A.squeeze(0)\n","        y = y.squeeze(0)\n","\n","        out = model(H,A)\n","        train_loss = criterion(out, y)\n","        train_accuracy = accuracy(out, y)\n","\n","\n","        epoch_train_losses.append(train_loss.item())\n","        epoch_train_accuracy.append(train_accuracy)\n","\n","        # Backpropagation and optimization\n","        train_loss.backward()\n","        optimizer.step()\n","\n","    return np.mean(epoch_train_losses), np.mean(epoch_train_accuracy)"],"metadata":{"id":"EftvJs1pdDlQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def validate(model, dev_loader, criterion, threshold=12000):\n","    model.eval()\n","    epoch_dev_losses = []\n","    epoch_dev_accuracy = []\n","    pred_holder = []\n","    real_holder = []\n","\n","    with torch.no_grad():\n","        for batch_idx, (H, A, y, indices) in enumerate(dev_loader):\n","            print('VVV', batch_idx)\n","            H = H.squeeze(0)\n","            A = A.squeeze(0)\n","            y = y.squeeze(0)\n","            indices = indices.squeeze(0)\n","\n","            out = model(H, A)\n","\n","            # Filter out training points\n","            mask = indices >= threshold\n","            filtered_out = out[mask]\n","            filtered_y = y[mask]\n","\n","            # Calculate loss and accuracy only on filtered outputs\n","            if filtered_out.size(0) > 0:  # Ensure there are samples to evaluate\n","                dev_loss = criterion(filtered_out, filtered_y)\n","                dev_accuracy = accuracy(filtered_out, filtered_y)\n","\n","                epoch_dev_losses.append(dev_loss.item())\n","                epoch_dev_accuracy.append(dev_accuracy)\n","\n","    # Avoid division by zero if no validation points were processed\n","    return np.mean(epoch_dev_losses), np.mean(epoch_dev_accuracy)"],"metadata":{"id":"FEfSsAmimuEU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## $\\color{blue}{Training-Loop:}$"],"metadata":{"id":"dyzhkkmcR29K"}},{"cell_type":"code","source":["from collections import namedtuple\n","Stats = namedtuple('Stats', [\n","    'train_loss',\n","    'train_accuracy',\n","    'dev_loss',\n","    'dev_accuracy',\n","    'epoch',\n","    'lr',\n","    'alpha',\n","    'max_accuracy'\n","])"],"metadata":{"id":"8gM37WLCNRJ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def gen_config(lr_low, lr_high, alpha_low, alpha_high):\n","  np.random.seed()\n","  lr = round(10**float(np.random.uniform(lr_low,lr_high)),6)\n","  alpha = round(10**float(np.random.uniform(alpha_low,alpha_high)),6)\n","  return lr, alpha"],"metadata":{"id":"E_DaEjRlNXl7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def gen_ranges( lr, lr_range, alpha, alpha_range):\n","\n","  lr_center = lr\n","  lr_low = lr_center - lr_range/2\n","  lr_high = lr_center + lr_range/2\n","  lr_diff = lr_high - lr_low\n","\n","  alpha_center = alpha\n","  alpha_low = alpha_center - alpha_range/2\n","  alpha_high = alpha_center + alpha_range/2\n","  alpha_diff = alpha_high - alpha_low\n","\n","  return (lr_low, lr_high, alpha_low, alpha_high)"],"metadata":{"id":"4V9JC3PUNbpf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def search_stats(results):\n","  best_stats = None\n","  max_dev_accuracy = 0\n","  for i in range(len(results)):\n","    acc = results[i].dev_accuracy\n","    if acc > max_dev_accuracy:\n","      best_stats = results[i]\n","      max_dev_accuracy = acc\n","  return best_stats"],"metadata":{"id":"-sNDLKonNj_Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tv_run(epochs, model, lr, alpha, max_accuracy, path, verbose = 0):\n","  \"\"\"\n","  Runs a training setup\n","  verbose == 1 - print model results\n","  verbose == 2 -> print epoch and model results\n","  \"\"\"\n","  print(f'lr {lr}, alpha {alpha}')\n","  model = model.to(device)\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=alpha)\n","\n","  # Prepare data loaders\n","  train_loader = DataLoader(train_dataset, sampler = custom_train_sampler)\n","  dev_loader = DataLoader(validation_dataset, sampler = custom_validation_sampler)\n","\n","  # Hold epoch stats\n","  train_losses = []\n","  train_accuracy = []\n","  dev_losses = []\n","  dev_accuracy = []\n","  epoch_holder = []\n","\n","  # Break if no improvement\n","  current_best = 0\n","  no_improvement = 0\n","\n","\n","  # Run epochs\n","  for epoch in range(epochs):\n","\n","    # break out of epochs\n","    if no_improvement >= 3:\n","      break\n","\n","    # call training and validation functions\n","    train_loss, train_acc = train(model, train_loader, criterion, optimizer)\n","    dev_loss, dev_acc = validate(model, dev_loader, criterion)\n","\n","    # Store epoch stats\n","    train_losses.append(train_loss)\n","    train_accuracy.append(train_acc)\n","    dev_losses.append(dev_loss)\n","    dev_accuracy.append(dev_acc)\n","    epoch_holder.append(epoch + 1)\n","\n","    # check for improvement\n","    if dev_acc > current_best:\n","      current_best = dev_acc\n","      no_improvement = 0\n","    else:\n","      no_improvement += 1\n","\n","    # save best model\n","    if dev_acc > max_accuracy:\n","      torch.save(model.state_dict(), path)\n","      max_accuracy = dev_acc\n","\n","\n","    # optionally print epoch results\n","    if verbose == 2:\n","      print(f'\\n --------- \\nEpoch: {epoch + 1}\\n')\n","      print(f'Epoch {epoch + 1} train loss: {train_loss:.4f}')\n","      print(f'Epoch {epoch + 1} train accuracy: {train_acc:.4f}')\n","      print(f'Epoch {epoch + 1} dev loss: {dev_loss:.4f}')\n","      print(f'Epoch {epoch + 1} dev accuracy: {dev_acc:.4f}')\n","\n","      # save best results\n","  max_ind = np.argmax(dev_accuracy)\n","\n","  stats = Stats(\n","      train_losses[max_ind],\n","      train_accuracy[max_ind],\n","      dev_losses[max_ind],\n","      dev_accuracy[max_ind],\n","      epoch_holder[max_ind],\n","      lr, alpha,\n","      max_accuracy\n","  )\n","\n","  # optionally print model results\n","  if verbose in [1,2]:\n","    print('\\n ######## \\n')\n","    print(f'lr:{stats.lr}, alpha:{stats.alpha} @ epoch {stats.epoch}.')\n","    print(f'TL:{stats.train_loss}, TA:{stats.train_accuracy}.')\n","    print(f'DL:{stats.dev_loss}, DA:{stats.dev_accuracy}')\n","\n","  return stats"],"metadata":{"id":"J3PQBpDIYnib"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### $\\color{red}{Sanity-check:}$"],"metadata":{"id":"1C-wVpsPUasU"}},{"cell_type":"code","source":["# model\n","model = GATModel(d, h, c)"],"metadata":{"id":"h9dVcM1VaN6F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# training loader\n","H_train = torch.stack(list(df_train['vanilla_embedding.1']))\n","labels_train = torch.LongTensor(list(df_train['chapter_idx']))\n","A_train = train_entities\n","train_indices = torch.LongTensor(list(range(df_train.shape[0])))\n","\n","train_dataset = GNNDataset(H_train, A_train, labels_train, train_indices, neighbor_max=4, branch_max=16)\n","\n","# Prevent dataloader from calling a single index at a time\n","custom_train_sampler = torch.utils.data.sampler.BatchSampler(\n","    torch.utils.data.sampler.RandomSampler(train_dataset),\n","    batch_size=2048,\n","    drop_last=False)\n","\n","\n","train_loader = DataLoader(train_dataset, sampler = custom_train_sampler)\n","\n","# training loader\n","df1 = df_train[['vanilla_embedding.1', 'chapter_idx']]\n","df2 = df_dev[['vanilla_embedding.1', 'chapter_idx']]\n","df_val = pd.concat([df1, df2])\n","H_val = torch.stack(list(df_val['vanilla_embedding.1']))\n","labels_val = torch.LongTensor(list(df_val['chapter_idx']))\n","A_val = val_entities\n","val_indices = torch.LongTensor(list(range(df_val.shape[0])))\n","\n","\n","\n","validation_dataset = GNNDataset(H_val, A_val, labels_val, val_indices, neighbor_max=4, branch_max=16, seed=42)\n","\n","# Prevent dataloader from calling a single index at a time\n","custom_validation_sampler = torch.utils.data.sampler.BatchSampler(\n","    torch.utils.data.sampler.SequentialSampler(validation_dataset),\n","    batch_size=2048,\n","    drop_last=False)\n","\n","\n","dev_loader = DataLoader(validation_dataset, sampler = custom_validation_sampler)\n","\n"],"metadata":{"id":"9pxw5ELFaN8t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 10\n","lr = 0.0005\n","alpha = 0.0001\n","path = \"class/models/GAT.pt\"\n","max_accuracy = 0"],"metadata":{"id":"Qf0cxrsteYzM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tv_run(epochs, model, lr, alpha, max_accuracy, path, verbose = 2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RPYCNxFAUZPu","executionInfo":{"status":"ok","timestamp":1738853874816,"user_tz":-60,"elapsed":303943,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}},"outputId":"267d0af8-54aa-4c7e-b393-0cd9a3bad7f4","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["lr 0.0005, alpha 0.0001\n","TTT 0\n","TTT 1\n","TTT 2\n","TTT 3\n","TTT 4\n","TTT 5\n","VVV 0\n","VVV 1\n","VVV 2\n","VVV 3\n","VVV 4\n","VVV 5\n","VVV 6\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 2.7746\n","Epoch 1 train accuracy: 0.3085\n","Epoch 1 dev loss: 4.0973\n","Epoch 1 dev accuracy: 0.0380\n","TTT 0\n","TTT 1\n","TTT 2\n","TTT 3\n","TTT 4\n","TTT 5\n","VVV 0\n","VVV 1\n","VVV 2\n","VVV 3\n","VVV 4\n","VVV 5\n","VVV 6\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 2.5677\n","Epoch 2 train accuracy: 0.3516\n","Epoch 2 dev loss: 4.1256\n","Epoch 2 dev accuracy: 0.0380\n","TTT 0\n","TTT 1\n","TTT 2\n","TTT 3\n","TTT 4\n","TTT 5\n","VVV 0\n","VVV 1\n","VVV 2\n","VVV 3\n","VVV 4\n","VVV 5\n","VVV 6\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 2.4397\n","Epoch 3 train accuracy: 0.3750\n","Epoch 3 dev loss: 4.1322\n","Epoch 3 dev accuracy: 0.0380\n","TTT 0\n","TTT 1\n","TTT 2\n","TTT 3\n","TTT 4\n","TTT 5\n","VVV 0\n","VVV 1\n","VVV 2\n","VVV 3\n","VVV 4\n","VVV 5\n","VVV 6\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 2.3502\n","Epoch 4 train accuracy: 0.3854\n","Epoch 4 dev loss: 4.1128\n","Epoch 4 dev accuracy: 0.0380\n","\n"," ######## \n","\n","lr:0.0005, alpha:0.0001 @ epoch 1.\n","TL:2.7745629151662192, TA:0.3085403757639949.\n","DL:4.0972602026803155, DA:0.03804008961005582\n"]},{"output_type":"execute_result","data":{"text/plain":["Stats(train_loss=2.7745629151662192, train_accuracy=0.3085403757639949, dev_loss=4.0972602026803155, dev_accuracy=0.03804008961005582, epoch=1, lr=0.0005, alpha=0.0001, max_accuracy=0.03804008961005582)"]},"metadata":{},"execution_count":41}]},{"cell_type":"markdown","source":["#### $\\color{red}{Run:}$"],"metadata":{"id":"ZBHJn5V8mIDu"}},{"cell_type":"code","source":["\"\"\"\n","Main Admin\n","\"\"\"\n","epochs = 22\n","max_accuracy = 0\n","path = \"class/models/GAT.pt\"\n","results = []\n","\n","\"\"\"\n","init random search\n","lr [10^-5 - 10^-1]\n","alpha [10^-5 - 10^-1]\n","bs [8, 32, 128]\n","\"\"\"\n","lr_low = -2\n","lr_high = -1\n","lr_range = lr_high - lr_low\n","\n","alpha_low = -5\n","alpha_high = -3\n","alpha_range = alpha_high - alpha_low\n","\n","d = 768\n","h = 400\n","c = 70\n","num_relations = 1\n","\n","count = 0\n","\n","\"\"\"\n","Hyperparameter Search\n","\"\"\"\n","\n","for i in range(3):\n","  # debug\n","  print(\"\\n################\\n\")\n","  print(f'round: {i}')\n","  # print(f'lr_low{lr_low}, lr_high{lr_high}, lr_range{lr_range}')\n","  # print(f'alpha_low{alpha_low}, lr_high{alpha_high}, lr_range{alpha_range}')\n","  print('max', max_accuracy)\n","  print(\"\\n################\\n\")\n","\n","\n","  for j in range(3):\n","    count += 1\n","    print(count)\n","\n","    # get config\n","    lr, alpha = gen_config(lr_low, lr_high, alpha_low, alpha_high)\n","    # define model\n","    model = GATModel(d, h, c)\n","    model = model.to(device)\n","\n","    # run training\n","    res = tv_run(epochs, model, lr, alpha, max_accuracy, path, verbose = 2)\n","    max_accuracy = res.max_accuracy\n","    results.append(res)\n","\n","  # get best result of the round or even so far\n","  stats = search_stats(results)\n","\n","\n","  print(stats) # debug\n","\n","  # reconfigure the new hypers\n","  lr = np.log10(stats.lr)\n","  lr_range = lr_range / 3\n","\n","  alpha = np.log10(stats.alpha)\n","  alpha_range = alpha_range / 3\n","\n","  config = gen_ranges(lr, lr_range, alpha, alpha_range)\n","  lr_low, lr_high, alpha_low, alpha_high = config\n","  lr_range = lr_high - lr_low\n","  alpha_range = alpha_high - alpha_low\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"fSA1uTMSaN_g","outputId":"919e2ecf-125c-4737-ff70-108eb57724b0","executionInfo":{"status":"error","timestamp":1736858035264,"user_tz":-60,"elapsed":3609077,"user":{"displayName":"Clarke Ricahrd","userId":"13372369852905387831"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","################\n","\n","round: 0\n","max 0\n","\n","################\n","\n","1\n","lr 0.013422, alpha 0.000411\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 3.6948\n","Epoch 1 train accuracy: 0.1359\n","Epoch 1 dev loss: 3.4371\n","Epoch 1 dev accuracy: 0.1587\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 3.5486\n","Epoch 2 train accuracy: 0.1550\n","Epoch 2 dev loss: 3.4074\n","Epoch 2 dev accuracy: 0.1777\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 3.5000\n","Epoch 3 train accuracy: 0.1602\n","Epoch 3 dev loss: 3.6082\n","Epoch 3 dev accuracy: 0.1561\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 3.5645\n","Epoch 4 train accuracy: 0.1534\n","Epoch 4 dev loss: 3.5864\n","Epoch 4 dev accuracy: 0.1516\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 3.4625\n","Epoch 5 train accuracy: 0.1655\n","Epoch 5 dev loss: 3.6374\n","Epoch 5 dev accuracy: 0.1399\n","\n"," ######## \n","\n","lr:0.013422, alpha:0.000411 @ epoch 2.\n","TL:3.548618005434672, TA:0.15499645821744848.\n","DL:3.4073648167777733, DA:0.1777405110706763\n","2\n","lr 0.015027, alpha 3.2e-05\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 3.6556\n","Epoch 1 train accuracy: 0.1408\n","Epoch 1 dev loss: 3.4323\n","Epoch 1 dev accuracy: 0.1494\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 3.5125\n","Epoch 2 train accuracy: 0.1558\n","Epoch 2 dev loss: 3.4110\n","Epoch 2 dev accuracy: 0.1569\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 3.4259\n","Epoch 3 train accuracy: 0.1677\n","Epoch 3 dev loss: 3.6038\n","Epoch 3 dev accuracy: 0.1454\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 3.4320\n","Epoch 4 train accuracy: 0.1633\n","Epoch 4 dev loss: 3.8329\n","Epoch 4 dev accuracy: 0.1376\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 3.4365\n","Epoch 5 train accuracy: 0.1630\n","Epoch 5 dev loss: 3.6393\n","Epoch 5 dev accuracy: 0.1556\n","\n"," ######## \n","\n","lr:0.015027, alpha:3.2e-05 @ epoch 2.\n","TL:3.512484696706136, TA:0.1557511271631382.\n","DL:3.410986867743641, DA:0.15686763812141707\n","3\n","lr 0.011704, alpha 1.9e-05\n","\n"," --------- \n","Epoch: 1\n","\n","Epoch 1 train loss: 3.6288\n","Epoch 1 train accuracy: 0.1435\n","Epoch 1 dev loss: 3.6768\n","Epoch 1 dev accuracy: 0.1151\n","\n"," --------- \n","Epoch: 2\n","\n","Epoch 2 train loss: 3.5133\n","Epoch 2 train accuracy: 0.1591\n","Epoch 2 dev loss: 3.5110\n","Epoch 2 dev accuracy: 0.1737\n","\n"," --------- \n","Epoch: 3\n","\n","Epoch 3 train loss: 3.4609\n","Epoch 3 train accuracy: 0.1688\n","Epoch 3 dev loss: 3.5644\n","Epoch 3 dev accuracy: 0.1623\n","\n"," --------- \n","Epoch: 4\n","\n","Epoch 4 train loss: 3.3479\n","Epoch 4 train accuracy: 0.1800\n","Epoch 4 dev loss: 3.5913\n","Epoch 4 dev accuracy: 0.1700\n","\n"," --------- \n","Epoch: 5\n","\n","Epoch 5 train loss: 3.3279\n","Epoch 5 train accuracy: 0.1796\n","Epoch 5 dev loss: 3.7824\n","Epoch 5 dev accuracy: 0.1381\n","\n"," ######## \n","\n","lr:0.011704, alpha:1.9e-05 @ epoch 2.\n","TL:3.5133010136286416, TA:0.1590863765665724.\n","DL:3.510972771591645, DA:0.17366740480215254\n","Stats(train_loss=3.548618005434672, train_accuracy=0.15499645821744848, dev_loss=3.4073648167777733, dev_accuracy=0.1777405110706763, epoch=2, lr=0.013422, alpha=0.000411, max_accuracy=0.1777405110706763)\n","\n","################\n","\n","round: 1\n","max 0.1777405110706763\n","\n","################\n","\n","4\n","lr 0.015091, alpha 0.000246\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-699bda62bb10>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# run training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtv_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mmax_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-dc6194764475>\u001b[0m in \u001b[0;36mtv_run\u001b[0;34m(epochs, model, lr, alpha, max_accuracy, path, verbose)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# call training and validation functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mdev_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-fa7368504d1a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepoch_train_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-cf72608a33f8>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, inds)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# return the required inds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0msampled_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_neighborhood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbor_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch_max\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# get the input for the required inds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-f2f26c895757>\u001b[0m in \u001b[0;36msample_neighborhood\u001b[0;34m(A, inds, neighbor_max, branch_max, seed)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mneighbors_neighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mdisclude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0msampled_indices\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mnew_neighbors_neighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mneighbor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mneighbor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mneighbor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdisclude\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_neighbors_neighbors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mneighbor_max\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m               \u001b[0mnew_neighbors_neighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_neighbors_neighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbor_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-f2f26c895757>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mneighbors_neighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mdisclude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0msampled_indices\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mnew_neighbors_neighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mneighbor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mneighbor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mneighbor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdisclude\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_neighbors_neighbors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mneighbor_max\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m               \u001b[0mnew_neighbors_neighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_neighbors_neighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbor_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}